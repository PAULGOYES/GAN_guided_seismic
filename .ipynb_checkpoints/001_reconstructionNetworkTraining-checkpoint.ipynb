{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "tSPICA3hW5r-"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, ReLU, Input, Activation, BatchNormalization, MaxPooling2D, Cropping2D, UpSampling2D, Concatenate,Flatten, Conv2D, Conv2DTranspose, LeakyReLU,PReLU, add, ReLU, concatenate\n",
    "from tensorflow.keras.layers import Flatten, Reshape, Add, GroupNormalization\n",
    "\n",
    "\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import backend as K\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "\n",
    "#visualizaci√≥n de imagenes\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "\n",
    "from network import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XnJLEdAVpJ-x"
   },
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simpleinter(img, indx):\n",
    "    out = np.zeros_like(img)\n",
    "    out[:,:]=img.copy()\n",
    "    \n",
    "    indx.sort()\n",
    "    \n",
    "    for i in range(len(indx)):\n",
    "        print('interpolatiing: ',indx[i])\n",
    "        if (indx[i]-1==0) and (indx[i]+1==0):\n",
    "            out[:,indx[i]]=img[:,indx[i]-2]\n",
    "        else:\n",
    "            out[:,indx[i]]=0.5*(img[:,indx[i]-1]+img[:,indx[i]+1])\n",
    "\n",
    "    return out\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "hRG6zGJaXww-"
   },
   "outputs": [],
   "source": [
    "def randomsamling(N=28,cr=0.1):\n",
    "  '''\n",
    "  cr = compression ratio\n",
    "  N  = full element number\n",
    "  '''\n",
    "\n",
    "  x0 = np.ones((N,))\n",
    "  ss = np.random.permutation(list(range(1, N - 1)))\n",
    "  x0[ss[0:int(cr * N)]] = 0\n",
    "\n",
    "  return x0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "NZiKDZfen6ER"
   },
   "outputs": [],
   "source": [
    "def subsampling(x,cr=[0.7]):\n",
    "  '''\n",
    "  x : full data\n",
    "  H : random vector 1xN zeros-ones from binomial distribution\n",
    "  '''\n",
    "  batch,M,N,L = x.shape\n",
    "\n",
    "  out = []\n",
    "  train = []\n",
    "\n",
    "  for j in cr:\n",
    "    new = np.zeros_like(x)\n",
    "\n",
    "    for i in range(batch):\n",
    "       H = randomsamling(N=N,cr=j)\n",
    "       new[i,:,:,0] = (x[i,:,:,0]+ (0.5+j)*np.random.normal(0, 0.008, size=(28,28))) * np.tile(H.reshape(1,-1),(x.shape[1],1))\n",
    "    out.append(new)\n",
    "    train.append(x)\n",
    "\n",
    "  return np.concatenate(out,axis=0),np.concatenate(train,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "v90y5oyZgSpd"
   },
   "outputs": [],
   "source": [
    "def log10(x):\n",
    "  numerator = K.log(x)\n",
    "  denominator = K.log(K.constant(10, dtype=numerator.dtype))\n",
    "  return numerator / denominator\n",
    "\n",
    "def PSNR(y_true, y_pred):\n",
    "  msel = tf.keras.losses.MeanSquaredError()\n",
    "  max_pixel=1\n",
    "  return 10.0 * log10((max_pixel) / msel(y_true,y_pred))\n",
    "\n",
    "def ssimm(y_true, y_pred):\n",
    "  return tf.reduce_mean(tf.image.ssim(y_true, y_pred, max_val=1, filter_size=2,\n",
    "                          filter_sigma=1.5, k1=0.01, k2=0.03))\n",
    "\n",
    "def l6(y_true, y_pred):\n",
    "  return tf.keras.losses.MeanSquaredError(y_true, y_pred)*ssimm(y_true, y_pred)\n",
    "\n",
    "def l7(y_true, y_pred):\n",
    "  return 1-ssimm(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9zQx6GFxrzMi"
   },
   "source": [
    "### Load Generator/Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rM33n_CqVSDg",
    "outputId": "4e2c2af1-71b4-417d-91f5-83f4a5cfcf75"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "generator = keras.models.load_model('models/generator_GAN_1000.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q2HMq_NTnm-z"
   },
   "source": [
    "### Augmentation Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 68ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<matplotlib.image.AxesImage at 0x7f5edc45d580>, None)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 700x700 with 16 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def imggan(sigma=1):\n",
    "    noise = np.random.normal(0, sigma, size=(1, 128))\n",
    "    return generator.predict(noise)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(4,4,figsize=(7, 7))\n",
    "ax[0,0].imshow(imggan(sigma=1).squeeze(),cmap='gray'), ax[0,0].set_axis_off()\n",
    "ax[0,1].imshow(imggan(sigma=1).squeeze(),cmap='gray'), ax[0,1].set_axis_off()\n",
    "ax[0,2].imshow(imggan(sigma=1).squeeze(),cmap='gray'), ax[0,2].set_axis_off()\n",
    "ax[0,3].imshow(imggan(sigma=1).squeeze(),cmap='gray'), ax[0,3].set_axis_off()\n",
    "\n",
    "ax[1,0].imshow(imggan(sigma=2).squeeze(),cmap='gray'), ax[1,0].set_axis_off()\n",
    "ax[1,1].imshow(imggan(sigma=2).squeeze(),cmap='gray'), ax[1,1].set_axis_off()\n",
    "ax[1,2].imshow(imggan(sigma=2).squeeze(),cmap='gray'), ax[1,2].set_axis_off()\n",
    "ax[1,3].imshow(imggan(sigma=2).squeeze(),cmap='gray'), ax[1,3].set_axis_off()\n",
    "\n",
    "ax[2,0].imshow(imggan(sigma=4).squeeze(),cmap='gray'), ax[2,0].set_axis_off()\n",
    "ax[2,1].imshow(imggan(sigma=4).squeeze(),cmap='gray'), ax[2,1].set_axis_off()\n",
    "ax[2,2].imshow(imggan(sigma=4).squeeze(),cmap='gray'), ax[2,2].set_axis_off()\n",
    "ax[2,3].imshow(imggan(sigma=4).squeeze(),cmap='gray'), ax[2,3].set_axis_off()\n",
    "\n",
    "ax[3,0].imshow(imggan(sigma=6).squeeze(),cmap='gray'), ax[3,0].set_axis_off()\n",
    "ax[3,1].imshow(imggan(sigma=6).squeeze(),cmap='gray'), ax[3,1].set_axis_off()\n",
    "ax[3,2].imshow(imggan(sigma=6).squeeze(),cmap='gray'), ax[3,2].set_axis_off()\n",
    "ax[3,3].imshow(imggan(sigma=6).squeeze(),cmap='gray'), ax[3,3].set_axis_off()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cHoc8lhQne7h",
    "tags": []
   },
   "source": [
    "### Resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Obua89wydpCV",
    "outputId": "616f9c06-cd79-4846-866f-d337288968a5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_4 (InputLayer)           [(None, 28, 28, 1)]  0           []                               \n",
      "                                                                                                  \n",
      " conv2d_10 (Conv2D)             (None, 14, 14, 64)   1600        ['input_4[0][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization_14 (BatchN  (None, 14, 14, 64)  256         ['conv2d_10[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_14 (LeakyReLU)     (None, 14, 14, 64)   0           ['batch_normalization_14[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_11 (Conv2D)             (None, 7, 7, 128)    204800      ['leaky_re_lu_14[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_15 (BatchN  (None, 7, 7, 128)   512         ['conv2d_11[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_15 (LeakyReLU)     (None, 7, 7, 128)    0           ['batch_normalization_15[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_12 (Conv2D)             (None, 7, 7, 256)    819200      ['leaky_re_lu_15[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_16 (BatchN  (None, 7, 7, 256)   1024        ['conv2d_12[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_16 (LeakyReLU)     (None, 7, 7, 256)    0           ['batch_normalization_16[0][0]'] \n",
      "                                                                                                  \n",
      " flatten_1 (Flatten)            (None, 12544)        0           ['leaky_re_lu_16[0][0]']         \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 10)           125450      ['flatten_1[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_17 (BatchN  (None, 10)          40          ['dense_2[0][0]']                \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_17 (LeakyReLU)     (None, 10)           0           ['batch_normalization_17[0][0]'] \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 12544)        137984      ['leaky_re_lu_17[0][0]']         \n",
      "                                                                                                  \n",
      " reshape_1 (Reshape)            (None, 7, 7, 256)    0           ['dense_3[0][0]']                \n",
      "                                                                                                  \n",
      " concatenate_4 (Concatenate)    (None, 7, 7, 512)    0           ['reshape_1[0][0]',              \n",
      "                                                                  'leaky_re_lu_16[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_18 (BatchN  (None, 7, 7, 512)   2048        ['concatenate_4[0][0]']          \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_18 (LeakyReLU)     (None, 7, 7, 512)    0           ['batch_normalization_18[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_transpose_2 (Conv2DTran  (None, 7, 7, 128)   1638400     ['leaky_re_lu_18[0][0]']         \n",
      " spose)                                                                                           \n",
      "                                                                                                  \n",
      " concatenate_5 (Concatenate)    (None, 7, 7, 256)    0           ['conv2d_transpose_2[0][0]',     \n",
      "                                                                  'leaky_re_lu_15[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_19 (BatchN  (None, 7, 7, 256)   1024        ['concatenate_5[0][0]']          \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_19 (LeakyReLU)     (None, 7, 7, 256)    0           ['batch_normalization_19[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_transpose_3 (Conv2DTran  (None, 14, 14, 64)  409600      ['leaky_re_lu_19[0][0]']         \n",
      " spose)                                                                                           \n",
      "                                                                                                  \n",
      " concatenate_6 (Concatenate)    (None, 14, 14, 128)  0           ['conv2d_transpose_3[0][0]',     \n",
      "                                                                  'leaky_re_lu_14[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_20 (BatchN  (None, 14, 14, 128)  512        ['concatenate_6[0][0]']          \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_20 (LeakyReLU)     (None, 14, 14, 128)  0           ['batch_normalization_20[0][0]'] \n",
      "                                                                                                  \n",
      " outUnet (Conv2DTranspose)      (None, 28, 28, 63)   201600      ['leaky_re_lu_20[0][0]']         \n",
      "                                                                                                  \n",
      " concatenate_7 (Concatenate)    (None, 28, 28, 64)   0           ['input_4[0][0]',                \n",
      "                                                                  'outUnet[0][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization_21 (BatchN  (None, 28, 28, 64)  256         ['concatenate_7[0][0]']          \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " leaky_re_lu_21 (LeakyReLU)     (None, 28, 28, 64)   0           ['batch_normalization_21[0][0]'] \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3,544,306\n",
      "Trainable params: 3,541,470\n",
      "Non-trainable params: 2,836\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "unet = Unet()\n",
    "resnet = ResNet()\n",
    "\n",
    "unet.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x9XkN9PffrU3",
    "outputId": "155878f4-9423-4d90-8809-aafdab9fe800"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_6 (InputLayer)        [(None, 28, 28, 1)]       0         \n",
      "                                                                 \n",
      " model_3 (Functional)        (None, 28, 28, 64)        3544306   \n",
      "                                                                 \n",
      " model_4 (Functional)        (None, 28, 28, 1)         617921    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,162,227\n",
      "Trainable params: 4,158,623\n",
      "Non-trainable params: 3,604\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_y = Input(shape=(28, 28, 1))\n",
    "output_u = unet(input_y)\n",
    "output_x = resnet(output_u)\n",
    "modelcombinado = Model(inputs=input_y, outputs=output_x)\n",
    "modelcombinado.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AdvaJF9HoDYF"
   },
   "source": [
    "# TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 340
    },
    "id": "oCF4T5raaN7G",
    "outputId": "416b6c82-696c-4b51-a37d-c2d89543baad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 16ms/step\n",
      "(2, 28, 28, 1)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 700x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "noise = np.random.normal(0, 1, size=(2, 128))\n",
    "generated_image = generator.predict(noise)\n",
    "print(generated_image.shape)\n",
    "n = subsampling(generated_image,cr=[0.1, 0.2, 0.3])\n",
    "\n",
    "fig, ax = plt.subplots(1, 2)\n",
    "fig.set_figheight(3)\n",
    "fig.set_figwidth(7)\n",
    "fig.tight_layout()\n",
    "\n",
    "ax[0].imshow(n[0][5],cmap='gray')\n",
    "ax[0].set_title(\"Corrupt seismic shot [0]\")\n",
    "ax[1].imshow(n[1][5], cmap=\"gray\")\n",
    "ax[1].set_title(\"Noncorrupt seismic shot [1]\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "N-GTIoV1AkAx",
    "outputId": "e2e92035-4230-45e3-8da4-59e146fae54b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f5edc3d4d30>]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def decayed_learning_rate(step,decay_rate,decay_steps,initial_learning_rate):\n",
    "  return initial_learning_rate * decay_rate**(step // decay_steps)\n",
    "\n",
    "plt.semilogy(decayed_learning_rate(np.arange(4000),0.6,450,1e-3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Y7uMOGBMbOGF",
    "outputId": "243a93a2-10b2-494d-d3ef-ca6b8efc3254"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " epoch 1 of 2000\n",
      "5/5 [==============================] - 0s 11ms/step\n",
      "11/11 [==============================] - 0s 6ms/step\n",
      "(for 1 minibatch) Training loss 0.9376677 | PSNR training 16.3298817\n",
      "\n",
      " epoch 2 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 1ms/step\n",
      "(for 1 minibatch) Training loss 0.7524222 | PSNR training 16.0523624\n",
      "\n",
      " epoch 3 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 876us/step\n",
      "(for 1 minibatch) Training loss 0.4745135 | PSNR training 18.5402699\n",
      "\n",
      " epoch 4 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 907us/step\n",
      "(for 1 minibatch) Training loss 0.3660829 | PSNR training 19.0624771\n",
      "\n",
      " epoch 5 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 903us/step\n",
      "(for 1 minibatch) Training loss 0.3565437 | PSNR training 19.1677532\n",
      "\n",
      " epoch 6 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 891us/step\n",
      "(for 1 minibatch) Training loss 0.2972085 | PSNR training 20.2292328\n",
      "\n",
      " epoch 7 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 871us/step\n",
      "(for 1 minibatch) Training loss 0.2934339 | PSNR training 20.2384987\n",
      "\n",
      " epoch 8 of 2000\n",
      "5/5 [==============================] - 0s 981us/step\n",
      "11/11 [==============================] - 0s 874us/step\n",
      "(for 1 minibatch) Training loss 0.2568691 | PSNR training 20.7996941\n",
      "\n",
      " epoch 9 of 2000\n",
      "5/5 [==============================] - 0s 961us/step\n",
      "11/11 [==============================] - 0s 908us/step\n",
      "(for 1 minibatch) Training loss 0.2358302 | PSNR training 21.5960083\n",
      "\n",
      " epoch 10 of 2000\n",
      "5/5 [==============================] - 0s 911us/step\n",
      "11/11 [==============================] - 0s 801us/step\n",
      "(for 1 minibatch) Training loss 0.2233572 | PSNR training 21.6331139\n",
      "\n",
      " epoch 11 of 2000\n",
      "5/5 [==============================] - 0s 974us/step\n",
      "11/11 [==============================] - 0s 1ms/step\n",
      "(for 1 minibatch) Training loss 0.1995086 | PSNR training 21.8598213\n",
      "\n",
      " epoch 12 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 854us/step\n",
      "(for 1 minibatch) Training loss 0.1899697 | PSNR training 21.9175873\n",
      "\n",
      " epoch 13 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 904us/step\n",
      "(for 1 minibatch) Training loss 0.2067273 | PSNR training 21.0259857\n",
      "\n",
      " epoch 14 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 874us/step\n",
      "(for 1 minibatch) Training loss 0.1634373 | PSNR training 21.9293880\n",
      "\n",
      " epoch 15 of 2000\n",
      "5/5 [==============================] - 0s 996us/step\n",
      "11/11 [==============================] - 0s 910us/step\n",
      "(for 1 minibatch) Training loss 0.1720009 | PSNR training 22.1031380\n",
      "\n",
      " epoch 16 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 808us/step\n",
      "(for 1 minibatch) Training loss 0.1530341 | PSNR training 23.0944824\n",
      "\n",
      " epoch 17 of 2000\n",
      "5/5 [==============================] - 0s 950us/step\n",
      "11/11 [==============================] - 0s 891us/step\n",
      "(for 1 minibatch) Training loss 0.1496356 | PSNR training 22.7887745\n",
      "\n",
      " epoch 18 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 869us/step\n",
      "(for 1 minibatch) Training loss 0.1340114 | PSNR training 23.1847153\n",
      "\n",
      " epoch 19 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 865us/step\n",
      "(for 1 minibatch) Training loss 0.1280814 | PSNR training 23.0684395\n",
      "\n",
      " epoch 20 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 967us/step\n",
      "(for 1 minibatch) Training loss 0.1193212 | PSNR training 23.1861649\n",
      "\n",
      " epoch 21 of 2000\n",
      "5/5 [==============================] - 0s 911us/step\n",
      "11/11 [==============================] - 0s 969us/step\n",
      "(for 1 minibatch) Training loss 0.1142862 | PSNR training 23.5859699\n",
      "\n",
      " epoch 22 of 2000\n",
      "5/5 [==============================] - 0s 966us/step\n",
      "11/11 [==============================] - 0s 795us/step\n",
      "(for 1 minibatch) Training loss 0.1083863 | PSNR training 23.7769299\n",
      "\n",
      " epoch 23 of 2000\n",
      "5/5 [==============================] - 0s 982us/step\n",
      "11/11 [==============================] - 0s 897us/step\n",
      "(for 1 minibatch) Training loss 0.1083761 | PSNR training 24.0784645\n",
      "\n",
      " epoch 24 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 815us/step\n",
      "(for 1 minibatch) Training loss 0.0995452 | PSNR training 24.1530647\n",
      "\n",
      " epoch 25 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 827us/step\n",
      "(for 1 minibatch) Training loss 0.0923311 | PSNR training 24.5156651\n",
      "\n",
      " epoch 26 of 2000\n",
      "5/5 [==============================] - 0s 952us/step\n",
      "11/11 [==============================] - 0s 818us/step\n",
      "(for 1 minibatch) Training loss 0.0944905 | PSNR training 24.4424648\n",
      "\n",
      " epoch 27 of 2000\n",
      "5/5 [==============================] - 0s 987us/step\n",
      "11/11 [==============================] - 0s 829us/step\n",
      "(for 1 minibatch) Training loss 0.0936564 | PSNR training 24.8478165\n",
      "\n",
      " epoch 28 of 2000\n",
      "5/5 [==============================] - 0s 938us/step\n",
      "11/11 [==============================] - 0s 878us/step\n",
      "(for 1 minibatch) Training loss 0.0956873 | PSNR training 24.5085220\n",
      "\n",
      " epoch 29 of 2000\n",
      "5/5 [==============================] - 0s 945us/step\n",
      "11/11 [==============================] - 0s 834us/step\n",
      "(for 1 minibatch) Training loss 0.1006387 | PSNR training 23.9370117\n",
      "\n",
      " epoch 30 of 2000\n",
      "5/5 [==============================] - 0s 948us/step\n",
      "11/11 [==============================] - 0s 842us/step\n",
      "(for 1 minibatch) Training loss 0.0864006 | PSNR training 24.6821499\n",
      "\n",
      " epoch 31 of 2000\n",
      "5/5 [==============================] - 0s 949us/step\n",
      "11/11 [==============================] - 0s 1ms/step\n",
      "(for 1 minibatch) Training loss 0.0800124 | PSNR training 24.7394333\n",
      "\n",
      " epoch 32 of 2000\n",
      "5/5 [==============================] - 0s 990us/step\n",
      "11/11 [==============================] - 0s 841us/step\n",
      "(for 1 minibatch) Training loss 0.0746695 | PSNR training 25.0390034\n",
      "\n",
      " epoch 33 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 816us/step\n",
      "(for 1 minibatch) Training loss 0.0758188 | PSNR training 25.3827286\n",
      "\n",
      " epoch 34 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 902us/step\n",
      "(for 1 minibatch) Training loss 0.0848476 | PSNR training 24.5885963\n",
      "\n",
      " epoch 35 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 930us/step\n",
      "(for 1 minibatch) Training loss 0.0764931 | PSNR training 24.8028450\n",
      "\n",
      " epoch 36 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 1ms/step\n",
      "(for 1 minibatch) Training loss 0.0652909 | PSNR training 25.6022625\n",
      "\n",
      " epoch 37 of 2000\n",
      "5/5 [==============================] - 0s 993us/step\n",
      "11/11 [==============================] - 0s 823us/step\n",
      "(for 1 minibatch) Training loss 0.0734130 | PSNR training 25.4204636\n",
      "\n",
      " epoch 38 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 997us/step\n",
      "(for 1 minibatch) Training loss 0.0698162 | PSNR training 25.5159264\n",
      "\n",
      " epoch 39 of 2000\n",
      "5/5 [==============================] - 0s 992us/step\n",
      "11/11 [==============================] - 0s 802us/step\n",
      "(for 1 minibatch) Training loss 0.0669618 | PSNR training 25.6787643\n",
      "\n",
      " epoch 40 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 835us/step\n",
      "(for 1 minibatch) Training loss 0.0694041 | PSNR training 25.8359413\n",
      "\n",
      " epoch 41 of 2000\n",
      "5/5 [==============================] - 0s 959us/step\n",
      "11/11 [==============================] - 0s 794us/step\n",
      "(for 1 minibatch) Training loss 0.0856563 | PSNR training 25.3032990\n",
      "\n",
      " epoch 42 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 832us/step\n",
      "(for 1 minibatch) Training loss 0.0681444 | PSNR training 25.3623104\n",
      "\n",
      " epoch 43 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 812us/step\n",
      "(for 1 minibatch) Training loss 0.0884688 | PSNR training 24.6479301\n",
      "\n",
      " epoch 44 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 832us/step\n",
      "(for 1 minibatch) Training loss 0.0653781 | PSNR training 25.6973248\n",
      "\n",
      " epoch 45 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 854us/step\n",
      "(for 1 minibatch) Training loss 0.0864735 | PSNR training 24.4753342\n",
      "\n",
      " epoch 46 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 903us/step\n",
      "(for 1 minibatch) Training loss 0.0638406 | PSNR training 25.5696850\n",
      "\n",
      " epoch 47 of 2000\n",
      "5/5 [==============================] - 0s 945us/step\n",
      "11/11 [==============================] - 0s 892us/step\n",
      "(for 1 minibatch) Training loss 0.0724130 | PSNR training 25.7041378\n",
      "\n",
      " epoch 48 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 814us/step\n",
      "(for 1 minibatch) Training loss 0.0634128 | PSNR training 25.6632996\n",
      "\n",
      " epoch 49 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 826us/step\n",
      "(for 1 minibatch) Training loss 0.0701699 | PSNR training 25.9557819\n",
      "\n",
      " epoch 50 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 872us/step\n",
      "(for 1 minibatch) Training loss 0.0550144 | PSNR training 26.4167881\n",
      "\n",
      " epoch 51 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 827us/step\n",
      "(for 1 minibatch) Training loss 0.0690911 | PSNR training 25.8574848\n",
      "\n",
      " epoch 52 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 846us/step\n",
      "(for 1 minibatch) Training loss 0.0580454 | PSNR training 26.2460556\n",
      "\n",
      " epoch 53 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 877us/step\n",
      "(for 1 minibatch) Training loss 0.0583632 | PSNR training 26.3793793\n",
      "\n",
      " epoch 54 of 2000\n",
      "5/5 [==============================] - 0s 951us/step\n",
      "11/11 [==============================] - 0s 897us/step\n",
      "(for 1 minibatch) Training loss 0.0581529 | PSNR training 26.1555901\n",
      "\n",
      " epoch 55 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 844us/step\n",
      "(for 1 minibatch) Training loss 0.0574835 | PSNR training 26.3861446\n",
      "\n",
      " epoch 56 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 832us/step\n",
      "(for 1 minibatch) Training loss 0.0532396 | PSNR training 26.5584183\n",
      "\n",
      " epoch 57 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 816us/step\n",
      "(for 1 minibatch) Training loss 0.0531704 | PSNR training 26.6200981\n",
      "\n",
      " epoch 58 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 863us/step\n",
      "(for 1 minibatch) Training loss 0.0536321 | PSNR training 26.2501888\n",
      "\n",
      " epoch 59 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 817us/step\n",
      "(for 1 minibatch) Training loss 0.0562173 | PSNR training 26.3141899\n",
      "\n",
      " epoch 60 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 1ms/step\n",
      "(for 1 minibatch) Training loss 0.0561849 | PSNR training 26.4190521\n",
      "\n",
      " epoch 61 of 2000\n",
      "5/5 [==============================] - 0s 987us/step\n",
      "11/11 [==============================] - 0s 865us/step\n",
      "(for 1 minibatch) Training loss 0.0565564 | PSNR training 26.3178234\n",
      "\n",
      " epoch 62 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 926us/step\n",
      "(for 1 minibatch) Training loss 0.0536369 | PSNR training 26.3650341\n",
      "\n",
      " epoch 63 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 834us/step\n",
      "(for 1 minibatch) Training loss 0.0526096 | PSNR training 26.5058517\n",
      "\n",
      " epoch 64 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 815us/step\n",
      "(for 1 minibatch) Training loss 0.0484987 | PSNR training 27.0134182\n",
      "\n",
      " epoch 65 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 858us/step\n",
      "(for 1 minibatch) Training loss 0.0497119 | PSNR training 27.0893021\n",
      "\n",
      " epoch 66 of 2000\n",
      "5/5 [==============================] - 0s 941us/step\n",
      "11/11 [==============================] - 0s 895us/step\n",
      "(for 1 minibatch) Training loss 0.0503005 | PSNR training 26.8101120\n",
      "\n",
      " epoch 67 of 2000\n",
      "5/5 [==============================] - 0s 973us/step\n",
      "11/11 [==============================] - 0s 882us/step\n",
      "(for 1 minibatch) Training loss 0.0480750 | PSNR training 26.8498192\n",
      "\n",
      " epoch 68 of 2000\n",
      "5/5 [==============================] - 0s 969us/step\n",
      "11/11 [==============================] - 0s 837us/step\n",
      "(for 1 minibatch) Training loss 0.0496535 | PSNR training 26.5719833\n",
      "\n",
      " epoch 69 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 842us/step\n",
      "(for 1 minibatch) Training loss 0.0540100 | PSNR training 26.3783283\n",
      "\n",
      " epoch 70 of 2000\n",
      "5/5 [==============================] - 0s 986us/step\n",
      "11/11 [==============================] - 0s 839us/step\n",
      "(for 1 minibatch) Training loss 0.0554717 | PSNR training 26.2139854\n",
      "\n",
      " epoch 71 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 840us/step\n",
      "(for 1 minibatch) Training loss 0.0454816 | PSNR training 27.1582317\n",
      "\n",
      " epoch 72 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 997us/step\n",
      "(for 1 minibatch) Training loss 0.0469264 | PSNR training 26.8259850\n",
      "\n",
      " epoch 73 of 2000\n",
      "5/5 [==============================] - 0s 977us/step\n",
      "11/11 [==============================] - 0s 872us/step\n",
      "(for 1 minibatch) Training loss 0.0454981 | PSNR training 27.3929234\n",
      "\n",
      " epoch 74 of 2000\n",
      "5/5 [==============================] - 0s 972us/step\n",
      "11/11 [==============================] - 0s 848us/step\n",
      "(for 1 minibatch) Training loss 0.0459679 | PSNR training 26.9719658\n",
      "\n",
      " epoch 75 of 2000\n",
      "5/5 [==============================] - 0s 992us/step\n",
      "11/11 [==============================] - 0s 935us/step\n",
      "(for 1 minibatch) Training loss 0.0447142 | PSNR training 27.4079437\n",
      "\n",
      " epoch 76 of 2000\n",
      "5/5 [==============================] - 0s 940us/step\n",
      "11/11 [==============================] - 0s 869us/step\n",
      "(for 1 minibatch) Training loss 0.0511154 | PSNR training 26.8529320\n",
      "\n",
      " epoch 77 of 2000\n",
      "5/5 [==============================] - 0s 958us/step\n",
      "11/11 [==============================] - 0s 898us/step\n",
      "(for 1 minibatch) Training loss 0.0552133 | PSNR training 26.3108463\n",
      "\n",
      " epoch 78 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 944us/step\n",
      "(for 1 minibatch) Training loss 0.0521885 | PSNR training 26.7175827\n",
      "\n",
      " epoch 79 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 941us/step\n",
      "(for 1 minibatch) Training loss 0.0436088 | PSNR training 27.5075836\n",
      "\n",
      " epoch 80 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 857us/step\n",
      "(for 1 minibatch) Training loss 0.0435697 | PSNR training 27.2148552\n",
      "\n",
      " epoch 81 of 2000\n",
      "5/5 [==============================] - 0s 1000us/step\n",
      "11/11 [==============================] - 0s 847us/step\n",
      "(for 1 minibatch) Training loss 0.0464566 | PSNR training 27.0121403\n",
      "\n",
      " epoch 82 of 2000\n",
      "5/5 [==============================] - 0s 977us/step\n",
      "11/11 [==============================] - 0s 833us/step\n",
      "(for 1 minibatch) Training loss 0.0456865 | PSNR training 27.2238998\n",
      "\n",
      " epoch 83 of 2000\n",
      "5/5 [==============================] - 0s 945us/step\n",
      "11/11 [==============================] - 0s 822us/step\n",
      "(for 1 minibatch) Training loss 0.0461562 | PSNR training 27.2382908\n",
      "\n",
      " epoch 84 of 2000\n",
      "5/5 [==============================] - 0s 976us/step\n",
      "11/11 [==============================] - 0s 852us/step\n",
      "(for 1 minibatch) Training loss 0.0448494 | PSNR training 27.3144951\n",
      "\n",
      " epoch 85 of 2000\n",
      "5/5 [==============================] - 0s 988us/step\n",
      "11/11 [==============================] - 0s 821us/step\n",
      "(for 1 minibatch) Training loss 0.0420568 | PSNR training 27.4790401\n",
      "\n",
      " epoch 86 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 786us/step\n",
      "(for 1 minibatch) Training loss 0.0401492 | PSNR training 27.6898041\n",
      "\n",
      " epoch 87 of 2000\n",
      "5/5 [==============================] - 0s 999us/step\n",
      "11/11 [==============================] - 0s 790us/step\n",
      "(for 1 minibatch) Training loss 0.0401773 | PSNR training 28.0092926\n",
      "\n",
      " epoch 88 of 2000\n",
      "5/5 [==============================] - 0s 957us/step\n",
      "11/11 [==============================] - 0s 875us/step\n",
      "(for 1 minibatch) Training loss 0.0426538 | PSNR training 27.3956661\n",
      "\n",
      " epoch 89 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 913us/step\n",
      "(for 1 minibatch) Training loss 0.0444170 | PSNR training 27.4122791\n",
      "\n",
      " epoch 90 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 883us/step\n",
      "(for 1 minibatch) Training loss 0.0509431 | PSNR training 26.7033119\n",
      "\n",
      " epoch 91 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 965us/step\n",
      "(for 1 minibatch) Training loss 0.0463583 | PSNR training 27.4741936\n",
      "\n",
      " epoch 92 of 2000\n",
      "5/5 [==============================] - 0s 955us/step\n",
      "11/11 [==============================] - 0s 806us/step\n",
      "(for 1 minibatch) Training loss 0.0388595 | PSNR training 28.0478745\n",
      "\n",
      " epoch 93 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 912us/step\n",
      "(for 1 minibatch) Training loss 0.0414911 | PSNR training 27.9377174\n",
      "\n",
      " epoch 94 of 2000\n",
      "5/5 [==============================] - 0s 976us/step\n",
      "11/11 [==============================] - 0s 809us/step\n",
      "(for 1 minibatch) Training loss 0.0502970 | PSNR training 27.1817322\n",
      "\n",
      " epoch 95 of 2000\n",
      "5/5 [==============================] - 0s 959us/step\n",
      "11/11 [==============================] - 0s 806us/step\n",
      "(for 1 minibatch) Training loss 0.0417392 | PSNR training 27.9585667\n",
      "\n",
      " epoch 96 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 926us/step\n",
      "(for 1 minibatch) Training loss 0.0411930 | PSNR training 27.6466599\n",
      "\n",
      " epoch 97 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 956us/step\n",
      "(for 1 minibatch) Training loss 0.0446420 | PSNR training 27.8568172\n",
      "\n",
      " epoch 98 of 2000\n",
      "5/5 [==============================] - 0s 992us/step\n",
      "11/11 [==============================] - 0s 844us/step\n",
      "(for 1 minibatch) Training loss 0.0426947 | PSNR training 27.5723190\n",
      "\n",
      " epoch 99 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 1ms/step\n",
      "(for 1 minibatch) Training loss 0.0371863 | PSNR training 28.2149200\n",
      "\n",
      " epoch 100 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 886us/step\n",
      "(for 1 minibatch) Training loss 0.0369493 | PSNR training 28.1283913\n",
      "\n",
      " epoch 101 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 851us/step\n",
      "(for 1 minibatch) Training loss 0.0393125 | PSNR training 28.3924637\n",
      "\n",
      " epoch 102 of 2000\n",
      "5/5 [==============================] - 0s 960us/step\n",
      "11/11 [==============================] - 0s 790us/step\n",
      "(for 1 minibatch) Training loss 0.0448162 | PSNR training 27.6677666\n",
      "\n",
      " epoch 103 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 810us/step\n",
      "(for 1 minibatch) Training loss 0.0410015 | PSNR training 28.2105675\n",
      "\n",
      " epoch 104 of 2000\n",
      "5/5 [==============================] - 0s 977us/step\n",
      "11/11 [==============================] - 0s 851us/step\n",
      "(for 1 minibatch) Training loss 0.0397580 | PSNR training 28.3796692\n",
      "\n",
      " epoch 105 of 2000\n",
      "5/5 [==============================] - 0s 957us/step\n",
      "11/11 [==============================] - 0s 816us/step\n",
      "(for 1 minibatch) Training loss 0.0426664 | PSNR training 28.4886723\n",
      "\n",
      " epoch 106 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 850us/step\n",
      "(for 1 minibatch) Training loss 0.0433335 | PSNR training 28.1315403\n",
      "\n",
      " epoch 107 of 2000\n",
      "5/5 [==============================] - 0s 942us/step\n",
      "11/11 [==============================] - 0s 821us/step\n",
      "(for 1 minibatch) Training loss 0.0392300 | PSNR training 28.1371765\n",
      "\n",
      " epoch 108 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 865us/step\n",
      "(for 1 minibatch) Training loss 0.0398017 | PSNR training 27.9226532\n",
      "\n",
      " epoch 109 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 832us/step\n",
      "(for 1 minibatch) Training loss 0.0469459 | PSNR training 27.8382969\n",
      "\n",
      " epoch 110 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 900us/step\n",
      "(for 1 minibatch) Training loss 0.0420340 | PSNR training 28.2052708\n",
      "\n",
      " epoch 111 of 2000\n",
      "5/5 [==============================] - 0s 928us/step\n",
      "11/11 [==============================] - 0s 870us/step\n",
      "(for 1 minibatch) Training loss 0.0352197 | PSNR training 28.6931629\n",
      "\n",
      " epoch 112 of 2000\n",
      "5/5 [==============================] - 0s 987us/step\n",
      "11/11 [==============================] - 0s 825us/step\n",
      "(for 1 minibatch) Training loss 0.0368367 | PSNR training 28.7804909\n",
      "\n",
      " epoch 113 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 808us/step\n",
      "(for 1 minibatch) Training loss 0.0424716 | PSNR training 27.9219131\n",
      "\n",
      " epoch 114 of 2000\n",
      "5/5 [==============================] - 0s 983us/step\n",
      "11/11 [==============================] - 0s 822us/step\n",
      "(for 1 minibatch) Training loss 0.0390583 | PSNR training 28.2999115\n",
      "\n",
      " epoch 115 of 2000\n",
      "5/5 [==============================] - 0s 936us/step\n",
      "11/11 [==============================] - 0s 815us/step\n",
      "(for 1 minibatch) Training loss 0.0393376 | PSNR training 28.0759716\n",
      "\n",
      " epoch 116 of 2000\n",
      "5/5 [==============================] - 0s 981us/step\n",
      "11/11 [==============================] - 0s 857us/step\n",
      "(for 1 minibatch) Training loss 0.0404410 | PSNR training 28.3383713\n",
      "\n",
      " epoch 117 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 834us/step\n",
      "(for 1 minibatch) Training loss 0.0363188 | PSNR training 28.5548439\n",
      "\n",
      " epoch 118 of 2000\n",
      "5/5 [==============================] - 0s 986us/step\n",
      "11/11 [==============================] - 0s 885us/step\n",
      "(for 1 minibatch) Training loss 0.0344351 | PSNR training 28.5582886\n",
      "\n",
      " epoch 119 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 833us/step\n",
      "(for 1 minibatch) Training loss 0.0353438 | PSNR training 28.6676807\n",
      "\n",
      " epoch 120 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 903us/step\n",
      "(for 1 minibatch) Training loss 0.0353137 | PSNR training 29.0312805\n",
      "\n",
      " epoch 121 of 2000\n",
      "5/5 [==============================] - 0s 987us/step\n",
      "11/11 [==============================] - 0s 849us/step\n",
      "(for 1 minibatch) Training loss 0.0355509 | PSNR training 28.7672329\n",
      "\n",
      " epoch 122 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 834us/step\n",
      "(for 1 minibatch) Training loss 0.0353433 | PSNR training 28.8321533\n",
      "\n",
      " epoch 123 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 861us/step\n",
      "(for 1 minibatch) Training loss 0.0364787 | PSNR training 28.6853561\n",
      "\n",
      " epoch 124 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 867us/step\n",
      "(for 1 minibatch) Training loss 0.0418078 | PSNR training 28.1262245\n",
      "\n",
      " epoch 125 of 2000\n",
      "5/5 [==============================] - 0s 973us/step\n",
      "11/11 [==============================] - 0s 848us/step\n",
      "(for 1 minibatch) Training loss 0.0387201 | PSNR training 28.5101299\n",
      "\n",
      " epoch 126 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 831us/step\n",
      "(for 1 minibatch) Training loss 0.0375361 | PSNR training 28.9687271\n",
      "\n",
      " epoch 127 of 2000\n",
      "5/5 [==============================] - 0s 972us/step\n",
      "11/11 [==============================] - 0s 826us/step\n",
      "(for 1 minibatch) Training loss 0.0379610 | PSNR training 28.6771755\n",
      "\n",
      " epoch 128 of 2000\n",
      "5/5 [==============================] - 0s 957us/step\n",
      "11/11 [==============================] - 0s 916us/step\n",
      "(for 1 minibatch) Training loss 0.0370958 | PSNR training 29.1236324\n",
      "\n",
      " epoch 129 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 886us/step\n",
      "(for 1 minibatch) Training loss 0.0349591 | PSNR training 29.0985126\n",
      "\n",
      " epoch 130 of 2000\n",
      "5/5 [==============================] - 0s 1000us/step\n",
      "11/11 [==============================] - 0s 878us/step\n",
      "(for 1 minibatch) Training loss 0.0360803 | PSNR training 28.7787094\n",
      "\n",
      " epoch 131 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 812us/step\n",
      "(for 1 minibatch) Training loss 0.0375281 | PSNR training 28.6946774\n",
      "\n",
      " epoch 132 of 2000\n",
      "5/5 [==============================] - 0s 989us/step\n",
      "11/11 [==============================] - 0s 841us/step\n",
      "(for 1 minibatch) Training loss 0.0375986 | PSNR training 28.6423569\n",
      "\n",
      " epoch 133 of 2000\n",
      "5/5 [==============================] - 0s 977us/step\n",
      "11/11 [==============================] - 0s 806us/step\n",
      "(for 1 minibatch) Training loss 0.0345278 | PSNR training 29.0069618\n",
      "\n",
      " epoch 134 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 794us/step\n",
      "(for 1 minibatch) Training loss 0.0341145 | PSNR training 28.9536934\n",
      "\n",
      " epoch 135 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 832us/step\n",
      "(for 1 minibatch) Training loss 0.0326074 | PSNR training 29.3922234\n",
      "\n",
      " epoch 136 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 815us/step\n",
      "(for 1 minibatch) Training loss 0.0330982 | PSNR training 29.1118259\n",
      "\n",
      " epoch 137 of 2000\n",
      "5/5 [==============================] - 0s 931us/step\n",
      "11/11 [==============================] - 0s 823us/step\n",
      "(for 1 minibatch) Training loss 0.0337217 | PSNR training 28.9464073\n",
      "\n",
      " epoch 138 of 2000\n",
      "5/5 [==============================] - 0s 983us/step\n",
      "11/11 [==============================] - 0s 874us/step\n",
      "(for 1 minibatch) Training loss 0.0351587 | PSNR training 28.9539547\n",
      "\n",
      " epoch 139 of 2000\n",
      "5/5 [==============================] - 0s 975us/step\n",
      "11/11 [==============================] - 0s 883us/step\n",
      "(for 1 minibatch) Training loss 0.0392544 | PSNR training 28.7436104\n",
      "\n",
      " epoch 140 of 2000\n",
      "5/5 [==============================] - 0s 969us/step\n",
      "11/11 [==============================] - 0s 842us/step\n",
      "(for 1 minibatch) Training loss 0.0387332 | PSNR training 28.6042805\n",
      "\n",
      " epoch 141 of 2000\n",
      "5/5 [==============================] - 0s 979us/step\n",
      "11/11 [==============================] - 0s 901us/step\n",
      "(for 1 minibatch) Training loss 0.0344999 | PSNR training 29.4919167\n",
      "\n",
      " epoch 142 of 2000\n",
      "5/5 [==============================] - 0s 959us/step\n",
      "11/11 [==============================] - 0s 821us/step\n",
      "(for 1 minibatch) Training loss 0.0409647 | PSNR training 28.6670952\n",
      "\n",
      " epoch 143 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 867us/step\n",
      "(for 1 minibatch) Training loss 0.0466709 | PSNR training 28.7989922\n",
      "\n",
      " epoch 144 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 877us/step\n",
      "(for 1 minibatch) Training loss 0.0410575 | PSNR training 28.4742336\n",
      "\n",
      " epoch 145 of 2000\n",
      "5/5 [==============================] - 0s 954us/step\n",
      "11/11 [==============================] - 0s 894us/step\n",
      "(for 1 minibatch) Training loss 0.0333776 | PSNR training 29.5157013\n",
      "\n",
      " epoch 146 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 862us/step\n",
      "(for 1 minibatch) Training loss 0.0358975 | PSNR training 29.5425777\n",
      "\n",
      " epoch 147 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 857us/step\n",
      "(for 1 minibatch) Training loss 0.0338122 | PSNR training 29.1913319\n",
      "\n",
      " epoch 148 of 2000\n",
      "5/5 [==============================] - 0s 971us/step\n",
      "11/11 [==============================] - 0s 859us/step\n",
      "(for 1 minibatch) Training loss 0.0365273 | PSNR training 29.4869022\n",
      "\n",
      " epoch 149 of 2000\n",
      "5/5 [==============================] - 0s 939us/step\n",
      "11/11 [==============================] - 0s 923us/step\n",
      "(for 1 minibatch) Training loss 0.0376092 | PSNR training 29.4246578\n",
      "\n",
      " epoch 150 of 2000\n",
      "5/5 [==============================] - 0s 928us/step\n",
      "11/11 [==============================] - 0s 837us/step\n",
      "(for 1 minibatch) Training loss 0.0329824 | PSNR training 29.6432209\n",
      "\n",
      " epoch 151 of 2000\n",
      "5/5 [==============================] - 0s 959us/step\n",
      "11/11 [==============================] - 0s 871us/step\n",
      "(for 1 minibatch) Training loss 0.0324360 | PSNR training 29.4938354\n",
      "\n",
      " epoch 152 of 2000\n",
      "5/5 [==============================] - 0s 981us/step\n",
      "11/11 [==============================] - 0s 937us/step\n",
      "(for 1 minibatch) Training loss 0.0329851 | PSNR training 29.4654636\n",
      "\n",
      " epoch 153 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 821us/step\n",
      "(for 1 minibatch) Training loss 0.0358738 | PSNR training 29.2239780\n",
      "\n",
      " epoch 154 of 2000\n",
      "5/5 [==============================] - 0s 949us/step\n",
      "11/11 [==============================] - 0s 819us/step\n",
      "(for 1 minibatch) Training loss 0.0333167 | PSNR training 29.7793541\n",
      "\n",
      " epoch 155 of 2000\n",
      "5/5 [==============================] - 0s 980us/step\n",
      "11/11 [==============================] - 0s 845us/step\n",
      "(for 1 minibatch) Training loss 0.0319966 | PSNR training 29.7109432\n",
      "\n",
      " epoch 156 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 842us/step\n",
      "(for 1 minibatch) Training loss 0.0315384 | PSNR training 29.4869499\n",
      "\n",
      " epoch 157 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 833us/step\n",
      "(for 1 minibatch) Training loss 0.0295975 | PSNR training 30.2379627\n",
      "\n",
      " epoch 158 of 2000\n",
      "5/5 [==============================] - 0s 977us/step\n",
      "11/11 [==============================] - 0s 830us/step\n",
      "(for 1 minibatch) Training loss 0.0295408 | PSNR training 30.1998444\n",
      "\n",
      " epoch 159 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 828us/step\n",
      "(for 1 minibatch) Training loss 0.0301726 | PSNR training 30.1062698\n",
      "\n",
      " epoch 160 of 2000\n",
      "5/5 [==============================] - 0s 984us/step\n",
      "11/11 [==============================] - 0s 843us/step\n",
      "(for 1 minibatch) Training loss 0.0320967 | PSNR training 29.9310131\n",
      "\n",
      " epoch 161 of 2000\n",
      "5/5 [==============================] - 0s 929us/step\n",
      "11/11 [==============================] - 0s 835us/step\n",
      "(for 1 minibatch) Training loss 0.0344089 | PSNR training 29.4980907\n",
      "\n",
      " epoch 162 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 883us/step\n",
      "(for 1 minibatch) Training loss 0.0337082 | PSNR training 29.8250294\n",
      "\n",
      " epoch 163 of 2000\n",
      "5/5 [==============================] - 0s 973us/step\n",
      "11/11 [==============================] - 0s 906us/step\n",
      "(for 1 minibatch) Training loss 0.0300312 | PSNR training 30.0153942\n",
      "\n",
      " epoch 164 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 899us/step\n",
      "(for 1 minibatch) Training loss 0.0297728 | PSNR training 30.2271271\n",
      "\n",
      " epoch 165 of 2000\n",
      "5/5 [==============================] - 0s 974us/step\n",
      "11/11 [==============================] - 0s 863us/step\n",
      "(for 1 minibatch) Training loss 0.0310346 | PSNR training 30.3573494\n",
      "\n",
      " epoch 166 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 843us/step\n",
      "(for 1 minibatch) Training loss 0.0345916 | PSNR training 30.0262089\n",
      "\n",
      " epoch 167 of 2000\n",
      "5/5 [==============================] - 0s 992us/step\n",
      "11/11 [==============================] - 0s 907us/step\n",
      "(for 1 minibatch) Training loss 0.0447286 | PSNR training 29.1136169\n",
      "\n",
      " epoch 168 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 877us/step\n",
      "(for 1 minibatch) Training loss 0.0398108 | PSNR training 28.9065571\n",
      "\n",
      " epoch 169 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 863us/step\n",
      "(for 1 minibatch) Training loss 0.0327917 | PSNR training 29.8334332\n",
      "\n",
      " epoch 170 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 848us/step\n",
      "(for 1 minibatch) Training loss 0.0348617 | PSNR training 30.1750259\n",
      "\n",
      " epoch 171 of 2000\n",
      "5/5 [==============================] - 0s 931us/step\n",
      "11/11 [==============================] - 0s 835us/step\n",
      "(for 1 minibatch) Training loss 0.0327082 | PSNR training 29.9054317\n",
      "\n",
      " epoch 172 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 818us/step\n",
      "(for 1 minibatch) Training loss 0.0292036 | PSNR training 30.6412029\n",
      "\n",
      " epoch 173 of 2000\n",
      "5/5 [==============================] - 0s 970us/step\n",
      "11/11 [==============================] - 0s 866us/step\n",
      "(for 1 minibatch) Training loss 0.0307330 | PSNR training 30.4844837\n",
      "\n",
      " epoch 174 of 2000\n",
      "5/5 [==============================] - 0s 1000us/step\n",
      "11/11 [==============================] - 0s 834us/step\n",
      "(for 1 minibatch) Training loss 0.0335550 | PSNR training 30.0634232\n",
      "\n",
      " epoch 175 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 868us/step\n",
      "(for 1 minibatch) Training loss 0.0314864 | PSNR training 30.4760551\n",
      "\n",
      " epoch 176 of 2000\n",
      "5/5 [==============================] - 0s 960us/step\n",
      "11/11 [==============================] - 0s 839us/step\n",
      "(for 1 minibatch) Training loss 0.0292535 | PSNR training 30.6067352\n",
      "\n",
      " epoch 177 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 936us/step\n",
      "(for 1 minibatch) Training loss 0.0323481 | PSNR training 30.3717003\n",
      "\n",
      " epoch 178 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 950us/step\n",
      "(for 1 minibatch) Training loss 0.0346749 | PSNR training 30.1536160\n",
      "\n",
      " epoch 179 of 2000\n",
      "5/5 [==============================] - 0s 978us/step\n",
      "11/11 [==============================] - 0s 952us/step\n",
      "(for 1 minibatch) Training loss 0.0337999 | PSNR training 30.0731373\n",
      "\n",
      " epoch 180 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 882us/step\n",
      "(for 1 minibatch) Training loss 0.0388356 | PSNR training 30.3520679\n",
      "\n",
      " epoch 181 of 2000\n",
      "5/5 [==============================] - 0s 983us/step\n",
      "11/11 [==============================] - 0s 854us/step\n",
      "(for 1 minibatch) Training loss 0.0333441 | PSNR training 30.1231575\n",
      "\n",
      " epoch 182 of 2000\n",
      "5/5 [==============================] - 0s 978us/step\n",
      "11/11 [==============================] - 0s 896us/step\n",
      "(for 1 minibatch) Training loss 0.0296335 | PSNR training 30.5270786\n",
      "\n",
      " epoch 183 of 2000\n",
      "5/5 [==============================] - 0s 949us/step\n",
      "11/11 [==============================] - 0s 837us/step\n",
      "(for 1 minibatch) Training loss 0.0336598 | PSNR training 30.4097538\n",
      "\n",
      " epoch 184 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 927us/step\n",
      "(for 1 minibatch) Training loss 0.0301202 | PSNR training 30.4589844\n",
      "\n",
      " epoch 185 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 871us/step\n",
      "(for 1 minibatch) Training loss 0.0301384 | PSNR training 30.3596478\n",
      "\n",
      " epoch 186 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 824us/step\n",
      "(for 1 minibatch) Training loss 0.0335723 | PSNR training 29.9512863\n",
      "\n",
      " epoch 187 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 913us/step\n",
      "(for 1 minibatch) Training loss 0.0326189 | PSNR training 30.3374329\n",
      "\n",
      " epoch 188 of 2000\n",
      "5/5 [==============================] - 0s 995us/step\n",
      "11/11 [==============================] - 0s 838us/step\n",
      "(for 1 minibatch) Training loss 0.0301484 | PSNR training 30.7698078\n",
      "\n",
      " epoch 189 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 920us/step\n",
      "(for 1 minibatch) Training loss 0.0292634 | PSNR training 30.4377384\n",
      "\n",
      " epoch 190 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 864us/step\n",
      "(for 1 minibatch) Training loss 0.0282989 | PSNR training 30.4923325\n",
      "\n",
      " epoch 191 of 2000\n",
      "5/5 [==============================] - 0s 988us/step\n",
      "11/11 [==============================] - 0s 875us/step\n",
      "(for 1 minibatch) Training loss 0.0292635 | PSNR training 31.0895195\n",
      "\n",
      " epoch 192 of 2000\n",
      "5/5 [==============================] - 0s 2ms/step\n",
      "11/11 [==============================] - 0s 1ms/step\n",
      "(for 1 minibatch) Training loss 0.0306228 | PSNR training 30.6689072\n",
      "\n",
      " epoch 193 of 2000\n",
      "5/5 [==============================] - 0s 997us/step\n",
      "11/11 [==============================] - 0s 911us/step\n",
      "(for 1 minibatch) Training loss 0.0303668 | PSNR training 30.6377506\n",
      "\n",
      " epoch 194 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 816us/step\n",
      "(for 1 minibatch) Training loss 0.0306607 | PSNR training 30.5184650\n",
      "\n",
      " epoch 195 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 930us/step\n",
      "(for 1 minibatch) Training loss 0.0275294 | PSNR training 30.9825401\n",
      "\n",
      " epoch 196 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 839us/step\n",
      "(for 1 minibatch) Training loss 0.0280096 | PSNR training 30.9247417\n",
      "\n",
      " epoch 197 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 849us/step\n",
      "(for 1 minibatch) Training loss 0.0276034 | PSNR training 31.5999908\n",
      "\n",
      " epoch 198 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 872us/step\n",
      "(for 1 minibatch) Training loss 0.0290498 | PSNR training 30.7885284\n",
      "\n",
      " epoch 199 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 946us/step\n",
      "(for 1 minibatch) Training loss 0.0286992 | PSNR training 30.7858372\n",
      "\n",
      " epoch 200 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 964us/step\n",
      "(for 1 minibatch) Training loss 0.0304126 | PSNR training 30.7581997\n",
      "\n",
      " epoch 201 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 1ms/step\n",
      "(for 1 minibatch) Training loss 0.0302973 | PSNR training 30.8490257\n",
      "\n",
      " epoch 202 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 932us/step\n",
      "(for 1 minibatch) Training loss 0.0285487 | PSNR training 30.8720360\n",
      "\n",
      " epoch 203 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 852us/step\n",
      "(for 1 minibatch) Training loss 0.0274477 | PSNR training 31.1986008\n",
      "\n",
      " epoch 204 of 2000\n",
      "5/5 [==============================] - 0s 960us/step\n",
      "11/11 [==============================] - 0s 870us/step\n",
      "(for 1 minibatch) Training loss 0.0276712 | PSNR training 31.0247688\n",
      "\n",
      " epoch 205 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 872us/step\n",
      "(for 1 minibatch) Training loss 0.0281320 | PSNR training 30.9452095\n",
      "\n",
      " epoch 206 of 2000\n",
      "5/5 [==============================] - 0s 972us/step\n",
      "11/11 [==============================] - 0s 962us/step\n",
      "(for 1 minibatch) Training loss 0.0271397 | PSNR training 31.1297703\n",
      "\n",
      " epoch 207 of 2000\n",
      "5/5 [==============================] - 0s 997us/step\n",
      "11/11 [==============================] - 0s 875us/step\n",
      "(for 1 minibatch) Training loss 0.0276809 | PSNR training 31.1928196\n",
      "\n",
      " epoch 208 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 876us/step\n",
      "(for 1 minibatch) Training loss 0.0300960 | PSNR training 31.1154938\n",
      "\n",
      " epoch 209 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 866us/step\n",
      "(for 1 minibatch) Training loss 0.0293143 | PSNR training 30.6501560\n",
      "\n",
      " epoch 210 of 2000\n",
      "5/5 [==============================] - 0s 988us/step\n",
      "11/11 [==============================] - 0s 888us/step\n",
      "(for 1 minibatch) Training loss 0.0280615 | PSNR training 31.4231834\n",
      "\n",
      " epoch 211 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 844us/step\n",
      "(for 1 minibatch) Training loss 0.0256034 | PSNR training 31.9970779\n",
      "\n",
      " epoch 212 of 2000\n",
      "5/5 [==============================] - 0s 979us/step\n",
      "11/11 [==============================] - 0s 809us/step\n",
      "(for 1 minibatch) Training loss 0.0248367 | PSNR training 31.8639965\n",
      "\n",
      " epoch 213 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 841us/step\n",
      "(for 1 minibatch) Training loss 0.0274761 | PSNR training 31.3133030\n",
      "\n",
      " epoch 214 of 2000\n",
      "5/5 [==============================] - 0s 977us/step\n",
      "11/11 [==============================] - 0s 825us/step\n",
      "(for 1 minibatch) Training loss 0.0293136 | PSNR training 31.0298996\n",
      "\n",
      " epoch 215 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 837us/step\n",
      "(for 1 minibatch) Training loss 0.0293682 | PSNR training 31.2626648\n",
      "\n",
      " epoch 216 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 834us/step\n",
      "(for 1 minibatch) Training loss 0.0276647 | PSNR training 31.6518116\n",
      "\n",
      " epoch 217 of 2000\n",
      "5/5 [==============================] - 0s 995us/step\n",
      "11/11 [==============================] - 0s 826us/step\n",
      "(for 1 minibatch) Training loss 0.0271404 | PSNR training 31.6209259\n",
      "\n",
      " epoch 218 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 978us/step\n",
      "(for 1 minibatch) Training loss 0.0256918 | PSNR training 31.8059444\n",
      "\n",
      " epoch 219 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 917us/step\n",
      "(for 1 minibatch) Training loss 0.0258059 | PSNR training 31.5181713\n",
      "\n",
      " epoch 220 of 2000\n",
      "5/5 [==============================] - 0s 997us/step\n",
      "11/11 [==============================] - 0s 827us/step\n",
      "(for 1 minibatch) Training loss 0.0238593 | PSNR training 32.1950951\n",
      "\n",
      " epoch 221 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 843us/step\n",
      "(for 1 minibatch) Training loss 0.0258222 | PSNR training 31.9438839\n",
      "\n",
      " epoch 222 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 891us/step\n",
      "(for 1 minibatch) Training loss 0.0291938 | PSNR training 31.8438072\n",
      "\n",
      " epoch 223 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 987us/step\n",
      "(for 1 minibatch) Training loss 0.0319462 | PSNR training 31.2532616\n",
      "\n",
      " epoch 224 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 936us/step\n",
      "(for 1 minibatch) Training loss 0.0258772 | PSNR training 32.0234528\n",
      "\n",
      " epoch 225 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 863us/step\n",
      "(for 1 minibatch) Training loss 0.0249407 | PSNR training 32.1071587\n",
      "\n",
      " epoch 226 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 841us/step\n",
      "(for 1 minibatch) Training loss 0.0274454 | PSNR training 31.9252968\n",
      "\n",
      " epoch 227 of 2000\n",
      "5/5 [==============================] - 0s 959us/step\n",
      "11/11 [==============================] - 0s 869us/step\n",
      "(for 1 minibatch) Training loss 0.0318313 | PSNR training 31.4717140\n",
      "\n",
      " epoch 228 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 918us/step\n",
      "(for 1 minibatch) Training loss 0.0284988 | PSNR training 31.8416672\n",
      "\n",
      " epoch 229 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 1ms/step\n",
      "(for 1 minibatch) Training loss 0.0251415 | PSNR training 32.0500641\n",
      "\n",
      " epoch 230 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 878us/step\n",
      "(for 1 minibatch) Training loss 0.0252739 | PSNR training 32.1622925\n",
      "\n",
      " epoch 231 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 892us/step\n",
      "(for 1 minibatch) Training loss 0.0265045 | PSNR training 31.6772995\n",
      "\n",
      " epoch 232 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 1ms/step\n",
      "(for 1 minibatch) Training loss 0.0248236 | PSNR training 32.2438507\n",
      "\n",
      " epoch 233 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 856us/step\n",
      "(for 1 minibatch) Training loss 0.0268452 | PSNR training 32.0428658\n",
      "\n",
      " epoch 234 of 2000\n",
      "5/5 [==============================] - 0s 947us/step\n",
      "11/11 [==============================] - 0s 924us/step\n",
      "(for 1 minibatch) Training loss 0.0293836 | PSNR training 31.5586472\n",
      "\n",
      " epoch 235 of 2000\n",
      "5/5 [==============================] - 0s 983us/step\n",
      "11/11 [==============================] - 0s 859us/step\n",
      "(for 1 minibatch) Training loss 0.0255482 | PSNR training 32.0740547\n",
      "\n",
      " epoch 236 of 2000\n",
      "5/5 [==============================] - 0s 990us/step\n",
      "11/11 [==============================] - 0s 885us/step\n",
      "(for 1 minibatch) Training loss 0.0240841 | PSNR training 32.5137596\n",
      "\n",
      " epoch 237 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 1ms/step\n",
      "(for 1 minibatch) Training loss 0.0253581 | PSNR training 32.0965271\n",
      "\n",
      " epoch 238 of 2000\n",
      "5/5 [==============================] - 0s 997us/step\n",
      "11/11 [==============================] - 0s 959us/step\n",
      "(for 1 minibatch) Training loss 0.0272292 | PSNR training 31.7873211\n",
      "\n",
      " epoch 239 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 973us/step\n",
      "(for 1 minibatch) Training loss 0.0304826 | PSNR training 31.4278965\n",
      "\n",
      " epoch 240 of 2000\n",
      "5/5 [==============================] - 0s 969us/step\n",
      "11/11 [==============================] - 0s 928us/step\n",
      "(for 1 minibatch) Training loss 0.0277724 | PSNR training 31.9444828\n",
      "\n",
      " epoch 241 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 804us/step\n",
      "(for 1 minibatch) Training loss 0.0279554 | PSNR training 32.0880775\n",
      "\n",
      " epoch 242 of 2000\n",
      "5/5 [==============================] - 0s 928us/step\n",
      "11/11 [==============================] - 0s 791us/step\n",
      "(for 1 minibatch) Training loss 0.0254412 | PSNR training 32.4391632\n",
      "\n",
      " epoch 243 of 2000\n",
      "5/5 [==============================] - 0s 937us/step\n",
      "11/11 [==============================] - 0s 820us/step\n",
      "(for 1 minibatch) Training loss 0.0238646 | PSNR training 32.6143417\n",
      "\n",
      " epoch 244 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 913us/step\n",
      "(for 1 minibatch) Training loss 0.0248815 | PSNR training 32.2781601\n",
      "\n",
      " epoch 245 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 898us/step\n",
      "(for 1 minibatch) Training loss 0.0266583 | PSNR training 32.1289139\n",
      "\n",
      " epoch 246 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 837us/step\n",
      "(for 1 minibatch) Training loss 0.0306672 | PSNR training 31.4742851\n",
      "\n",
      " epoch 247 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 1ms/step\n",
      "(for 1 minibatch) Training loss 0.0279718 | PSNR training 32.0638123\n",
      "\n",
      " epoch 248 of 2000\n",
      "5/5 [==============================] - 0s 973us/step\n",
      "11/11 [==============================] - 0s 879us/step\n",
      "(for 1 minibatch) Training loss 0.0249316 | PSNR training 32.3430481\n",
      "\n",
      " epoch 249 of 2000\n",
      "5/5 [==============================] - 0s 986us/step\n",
      "11/11 [==============================] - 0s 830us/step\n",
      "(for 1 minibatch) Training loss 0.0257817 | PSNR training 32.2420807\n",
      "\n",
      " epoch 250 of 2000\n",
      "5/5 [==============================] - 0s 972us/step\n",
      "11/11 [==============================] - 0s 906us/step\n",
      "(for 1 minibatch) Training loss 0.0248870 | PSNR training 32.4084854\n",
      "\n",
      " epoch 251 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 1ms/step\n",
      "(for 1 minibatch) Training loss 0.0247897 | PSNR training 32.4684029\n",
      "\n",
      " epoch 252 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 923us/step\n",
      "(for 1 minibatch) Training loss 0.0244924 | PSNR training 32.7397194\n",
      "\n",
      " epoch 253 of 2000\n",
      "5/5 [==============================] - 0s 964us/step\n",
      "11/11 [==============================] - 0s 918us/step\n",
      "(for 1 minibatch) Training loss 0.0278211 | PSNR training 32.3933754\n",
      "\n",
      " epoch 254 of 2000\n",
      "5/5 [==============================] - 0s 966us/step\n",
      "11/11 [==============================] - 0s 845us/step\n",
      "(for 1 minibatch) Training loss 0.0260789 | PSNR training 32.4039040\n",
      "\n",
      " epoch 255 of 2000\n",
      "5/5 [==============================] - 0s 984us/step\n",
      "11/11 [==============================] - 0s 901us/step\n",
      "(for 1 minibatch) Training loss 0.0231996 | PSNR training 32.7108345\n",
      "\n",
      " epoch 256 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 1ms/step\n",
      "(for 1 minibatch) Training loss 0.0228224 | PSNR training 32.9401245\n",
      "\n",
      " epoch 257 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 1ms/step\n",
      "(for 1 minibatch) Training loss 0.0240858 | PSNR training 32.9205818\n",
      "\n",
      " epoch 258 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 960us/step\n",
      "(for 1 minibatch) Training loss 0.0232517 | PSNR training 33.1392975\n",
      "\n",
      " epoch 259 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 885us/step\n",
      "(for 1 minibatch) Training loss 0.0241882 | PSNR training 32.8590469\n",
      "\n",
      " epoch 260 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 881us/step\n",
      "(for 1 minibatch) Training loss 0.0296292 | PSNR training 31.8601055\n",
      "\n",
      " epoch 261 of 2000\n",
      "5/5 [==============================] - 0s 967us/step\n",
      "11/11 [==============================] - 0s 829us/step\n",
      "(for 1 minibatch) Training loss 0.0277112 | PSNR training 32.1870079\n",
      "\n",
      " epoch 262 of 2000\n",
      "5/5 [==============================] - 0s 941us/step\n",
      "11/11 [==============================] - 0s 829us/step\n",
      "(for 1 minibatch) Training loss 0.0247588 | PSNR training 32.8823509\n",
      "\n",
      " epoch 263 of 2000\n",
      "5/5 [==============================] - 0s 945us/step\n",
      "11/11 [==============================] - 0s 939us/step\n",
      "(for 1 minibatch) Training loss 0.0223165 | PSNR training 33.1795158\n",
      "\n",
      " epoch 264 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 875us/step\n",
      "(for 1 minibatch) Training loss 0.0221936 | PSNR training 33.4696274\n",
      "\n",
      " epoch 265 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 814us/step\n",
      "(for 1 minibatch) Training loss 0.0249215 | PSNR training 32.7675667\n",
      "\n",
      " epoch 266 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 835us/step\n",
      "(for 1 minibatch) Training loss 0.0255049 | PSNR training 32.6870537\n",
      "\n",
      " epoch 267 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 865us/step\n",
      "(for 1 minibatch) Training loss 0.0262459 | PSNR training 32.3452415\n",
      "\n",
      " epoch 268 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 869us/step\n",
      "(for 1 minibatch) Training loss 0.0237064 | PSNR training 32.9314613\n",
      "\n",
      " epoch 269 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 874us/step\n",
      "(for 1 minibatch) Training loss 0.0237508 | PSNR training 32.9840469\n",
      "\n",
      " epoch 270 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 796us/step\n",
      "(for 1 minibatch) Training loss 0.0223564 | PSNR training 33.6115990\n",
      "\n",
      " epoch 271 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 836us/step\n",
      "(for 1 minibatch) Training loss 0.0222418 | PSNR training 33.3452301\n",
      "\n",
      " epoch 272 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 812us/step\n",
      "(for 1 minibatch) Training loss 0.0226197 | PSNR training 33.2016754\n",
      "\n",
      " epoch 273 of 2000\n",
      "5/5 [==============================] - 0s 957us/step\n",
      "11/11 [==============================] - 0s 868us/step\n",
      "(for 1 minibatch) Training loss 0.0229608 | PSNR training 33.6269417\n",
      "\n",
      " epoch 274 of 2000\n",
      "5/5 [==============================] - 0s 959us/step\n",
      "11/11 [==============================] - 0s 907us/step\n",
      "(for 1 minibatch) Training loss 0.0272225 | PSNR training 32.2849045\n",
      "\n",
      " epoch 275 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 866us/step\n",
      "(for 1 minibatch) Training loss 0.0268452 | PSNR training 32.6302757\n",
      "\n",
      " epoch 276 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 905us/step\n",
      "(for 1 minibatch) Training loss 0.0227373 | PSNR training 33.2990303\n",
      "\n",
      " epoch 277 of 2000\n",
      "5/5 [==============================] - 0s 942us/step\n",
      "11/11 [==============================] - 0s 865us/step\n",
      "(for 1 minibatch) Training loss 0.0220776 | PSNR training 33.5042419\n",
      "\n",
      " epoch 278 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 930us/step\n",
      "(for 1 minibatch) Training loss 0.0232056 | PSNR training 33.4654961\n",
      "\n",
      " epoch 279 of 2000\n",
      "5/5 [==============================] - 0s 993us/step\n",
      "11/11 [==============================] - 0s 848us/step\n",
      "(for 1 minibatch) Training loss 0.0266935 | PSNR training 32.6517715\n",
      "\n",
      " epoch 280 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 1ms/step\n",
      "(for 1 minibatch) Training loss 0.0255142 | PSNR training 32.6531525\n",
      "\n",
      " epoch 281 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 875us/step\n",
      "(for 1 minibatch) Training loss 0.0234689 | PSNR training 33.0335197\n",
      "\n",
      " epoch 282 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 955us/step\n",
      "(for 1 minibatch) Training loss 0.0223987 | PSNR training 33.1103745\n",
      "\n",
      " epoch 283 of 2000\n",
      "5/5 [==============================] - 0s 935us/step\n",
      "11/11 [==============================] - 0s 870us/step\n",
      "(for 1 minibatch) Training loss 0.0234802 | PSNR training 33.1307640\n",
      "\n",
      " epoch 284 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 882us/step\n",
      "(for 1 minibatch) Training loss 0.0253191 | PSNR training 33.0934601\n",
      "\n",
      " epoch 285 of 2000\n",
      "5/5 [==============================] - 0s 975us/step\n",
      "11/11 [==============================] - 0s 825us/step\n",
      "(for 1 minibatch) Training loss 0.0248752 | PSNR training 33.2069397\n",
      "\n",
      " epoch 286 of 2000\n",
      "5/5 [==============================] - 0s 921us/step\n",
      "11/11 [==============================] - 0s 835us/step\n",
      "(for 1 minibatch) Training loss 0.0221276 | PSNR training 33.6116638\n",
      "\n",
      " epoch 287 of 2000\n",
      "5/5 [==============================] - 0s 951us/step\n",
      "11/11 [==============================] - 0s 905us/step\n",
      "(for 1 minibatch) Training loss 0.0211269 | PSNR training 33.8635139\n",
      "\n",
      " epoch 288 of 2000\n",
      "5/5 [==============================] - 0s 969us/step\n",
      "11/11 [==============================] - 0s 817us/step\n",
      "(for 1 minibatch) Training loss 0.0225548 | PSNR training 33.7134705\n",
      "\n",
      " epoch 289 of 2000\n",
      "5/5 [==============================] - 0s 964us/step\n",
      "11/11 [==============================] - 0s 845us/step\n",
      "(for 1 minibatch) Training loss 0.0239565 | PSNR training 33.2767181\n",
      "\n",
      " epoch 290 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 862us/step\n",
      "(for 1 minibatch) Training loss 0.0248869 | PSNR training 33.3830948\n",
      "\n",
      " epoch 291 of 2000\n",
      "5/5 [==============================] - 0s 967us/step\n",
      "11/11 [==============================] - 0s 858us/step\n",
      "(for 1 minibatch) Training loss 0.0217190 | PSNR training 34.0545502\n",
      "\n",
      " epoch 292 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 846us/step\n",
      "(for 1 minibatch) Training loss 0.0209448 | PSNR training 34.0550842\n",
      "\n",
      " epoch 293 of 2000\n",
      "5/5 [==============================] - 0s 955us/step\n",
      "11/11 [==============================] - 0s 861us/step\n",
      "(for 1 minibatch) Training loss 0.0205281 | PSNR training 34.0023575\n",
      "\n",
      " epoch 294 of 2000\n",
      "5/5 [==============================] - 0s 944us/step\n",
      "11/11 [==============================] - 0s 924us/step\n",
      "(for 1 minibatch) Training loss 0.0214587 | PSNR training 33.8641052\n",
      "\n",
      " epoch 295 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 827us/step\n",
      "(for 1 minibatch) Training loss 0.0226920 | PSNR training 33.6244049\n",
      "\n",
      " epoch 296 of 2000\n",
      "5/5 [==============================] - 0s 919us/step\n",
      "11/11 [==============================] - 0s 851us/step\n",
      "(for 1 minibatch) Training loss 0.0243211 | PSNR training 33.6972923\n",
      "\n",
      " epoch 297 of 2000\n",
      "5/5 [==============================] - 0s 997us/step\n",
      "11/11 [==============================] - 0s 839us/step\n",
      "(for 1 minibatch) Training loss 0.0248019 | PSNR training 33.1853027\n",
      "\n",
      " epoch 298 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 900us/step\n",
      "(for 1 minibatch) Training loss 0.0223892 | PSNR training 33.8752213\n",
      "\n",
      " epoch 299 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 862us/step\n",
      "(for 1 minibatch) Training loss 0.0212271 | PSNR training 33.8241119\n",
      "\n",
      " epoch 300 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 810us/step\n",
      "(for 1 minibatch) Training loss 0.0208448 | PSNR training 34.2943459\n",
      "\n",
      " epoch 301 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 841us/step\n",
      "(for 1 minibatch) Training loss 0.0233309 | PSNR training 33.2762108\n",
      "\n",
      " epoch 302 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 909us/step\n",
      "(for 1 minibatch) Training loss 0.0252604 | PSNR training 33.2098045\n",
      "\n",
      " epoch 303 of 2000\n",
      "5/5 [==============================] - 0s 987us/step\n",
      "11/11 [==============================] - 0s 876us/step\n",
      "(for 1 minibatch) Training loss 0.0232517 | PSNR training 33.9272461\n",
      "\n",
      " epoch 304 of 2000\n",
      "5/5 [==============================] - 0s 986us/step\n",
      "11/11 [==============================] - 0s 891us/step\n",
      "(for 1 minibatch) Training loss 0.0261140 | PSNR training 33.6537285\n",
      "\n",
      " epoch 305 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 876us/step\n",
      "(for 1 minibatch) Training loss 0.0293893 | PSNR training 33.1201820\n",
      "\n",
      " epoch 306 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 821us/step\n",
      "(for 1 minibatch) Training loss 0.0280510 | PSNR training 33.4364128\n",
      "\n",
      " epoch 307 of 2000\n",
      "5/5 [==============================] - 0s 938us/step\n",
      "11/11 [==============================] - 0s 886us/step\n",
      "(for 1 minibatch) Training loss 0.0219172 | PSNR training 33.6816101\n",
      "\n",
      " epoch 308 of 2000\n",
      "5/5 [==============================] - 0s 961us/step\n",
      "11/11 [==============================] - 0s 837us/step\n",
      "(for 1 minibatch) Training loss 0.0287867 | PSNR training 32.9566422\n",
      "\n",
      " epoch 309 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 823us/step\n",
      "(for 1 minibatch) Training loss 0.0271403 | PSNR training 32.9989204\n",
      "\n",
      " epoch 310 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 885us/step\n",
      "(for 1 minibatch) Training loss 0.0232799 | PSNR training 33.8560982\n",
      "\n",
      " epoch 311 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 850us/step\n",
      "(for 1 minibatch) Training loss 0.0299466 | PSNR training 33.3056870\n",
      "\n",
      " epoch 312 of 2000\n",
      "5/5 [==============================] - 0s 984us/step\n",
      "11/11 [==============================] - 0s 842us/step\n",
      "(for 1 minibatch) Training loss 0.0245875 | PSNR training 33.3113403\n",
      "\n",
      " epoch 313 of 2000\n",
      "5/5 [==============================] - 0s 986us/step\n",
      "11/11 [==============================] - 0s 925us/step\n",
      "(for 1 minibatch) Training loss 0.0249117 | PSNR training 33.7608604\n",
      "\n",
      " epoch 314 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 851us/step\n",
      "(for 1 minibatch) Training loss 0.0278131 | PSNR training 33.1125603\n",
      "\n",
      " epoch 315 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 939us/step\n",
      "(for 1 minibatch) Training loss 0.0220178 | PSNR training 33.7445984\n",
      "\n",
      " epoch 316 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 901us/step\n",
      "(for 1 minibatch) Training loss 0.0276093 | PSNR training 33.0405197\n",
      "\n",
      " epoch 317 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 934us/step\n",
      "(for 1 minibatch) Training loss 0.0232105 | PSNR training 33.3653526\n",
      "\n",
      " epoch 318 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 850us/step\n",
      "(for 1 minibatch) Training loss 0.0253828 | PSNR training 33.6681404\n",
      "\n",
      " epoch 319 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 897us/step\n",
      "(for 1 minibatch) Training loss 0.0238271 | PSNR training 33.6014862\n",
      "\n",
      " epoch 320 of 2000\n",
      "5/5 [==============================] - 0s 990us/step\n",
      "11/11 [==============================] - 0s 849us/step\n",
      "(for 1 minibatch) Training loss 0.0233966 | PSNR training 33.5788193\n",
      "\n",
      " epoch 321 of 2000\n",
      "5/5 [==============================] - 0s 987us/step\n",
      "11/11 [==============================] - 0s 837us/step\n",
      "(for 1 minibatch) Training loss 0.0241674 | PSNR training 33.5096397\n",
      "\n",
      " epoch 322 of 2000\n",
      "5/5 [==============================] - 0s 978us/step\n",
      "11/11 [==============================] - 0s 828us/step\n",
      "(for 1 minibatch) Training loss 0.0219969 | PSNR training 34.2071304\n",
      "\n",
      " epoch 323 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 960us/step\n",
      "(for 1 minibatch) Training loss 0.0253555 | PSNR training 33.7959595\n",
      "\n",
      " epoch 324 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 902us/step\n",
      "(for 1 minibatch) Training loss 0.0217994 | PSNR training 33.7868347\n",
      "\n",
      " epoch 325 of 2000\n",
      "5/5 [==============================] - 0s 953us/step\n",
      "11/11 [==============================] - 0s 881us/step\n",
      "(for 1 minibatch) Training loss 0.0242922 | PSNR training 33.6732254\n",
      "\n",
      " epoch 326 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 910us/step\n",
      "(for 1 minibatch) Training loss 0.0221377 | PSNR training 34.3696747\n",
      "\n",
      " epoch 327 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 902us/step\n",
      "(for 1 minibatch) Training loss 0.0231044 | PSNR training 33.7449646\n",
      "\n",
      " epoch 328 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 1ms/step\n",
      "(for 1 minibatch) Training loss 0.0219362 | PSNR training 34.4108429\n",
      "\n",
      " epoch 329 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 900us/step\n",
      "(for 1 minibatch) Training loss 0.0220249 | PSNR training 34.0691681\n",
      "\n",
      " epoch 330 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 845us/step\n",
      "(for 1 minibatch) Training loss 0.0227580 | PSNR training 33.7600021\n",
      "\n",
      " epoch 331 of 2000\n",
      "5/5 [==============================] - 0s 972us/step\n",
      "11/11 [==============================] - 0s 832us/step\n",
      "(for 1 minibatch) Training loss 0.0220572 | PSNR training 33.8952789\n",
      "\n",
      " epoch 332 of 2000\n",
      "5/5 [==============================] - 0s 2ms/step\n",
      "11/11 [==============================] - 0s 947us/step\n",
      "(for 1 minibatch) Training loss 0.0221032 | PSNR training 34.3034668\n",
      "\n",
      " epoch 333 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 838us/step\n",
      "(for 1 minibatch) Training loss 0.0209299 | PSNR training 34.3287239\n",
      "\n",
      " epoch 334 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 878us/step\n",
      "(for 1 minibatch) Training loss 0.0212480 | PSNR training 34.5773621\n",
      "\n",
      " epoch 335 of 2000\n",
      "5/5 [==============================] - 0s 923us/step\n",
      "11/11 [==============================] - 0s 838us/step\n",
      "(for 1 minibatch) Training loss 0.0211591 | PSNR training 34.3689499\n",
      "\n",
      " epoch 336 of 2000\n",
      "5/5 [==============================] - 0s 977us/step\n",
      "11/11 [==============================] - 0s 816us/step\n",
      "(for 1 minibatch) Training loss 0.0205536 | PSNR training 34.4307442\n",
      "\n",
      " epoch 337 of 2000\n",
      "5/5 [==============================] - 0s 983us/step\n",
      "11/11 [==============================] - 0s 822us/step\n",
      "(for 1 minibatch) Training loss 0.0228995 | PSNR training 34.3999214\n",
      "\n",
      " epoch 338 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 884us/step\n",
      "(for 1 minibatch) Training loss 0.0219082 | PSNR training 34.2160187\n",
      "\n",
      " epoch 339 of 2000\n",
      "5/5 [==============================] - 0s 991us/step\n",
      "11/11 [==============================] - 0s 879us/step\n",
      "(for 1 minibatch) Training loss 0.0209643 | PSNR training 34.5969810\n",
      "\n",
      " epoch 340 of 2000\n",
      "5/5 [==============================] - 0s 973us/step\n",
      "11/11 [==============================] - 0s 864us/step\n",
      "(for 1 minibatch) Training loss 0.0214812 | PSNR training 34.4738312\n",
      "\n",
      " epoch 341 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 835us/step\n",
      "(for 1 minibatch) Training loss 0.0203124 | PSNR training 34.5824738\n",
      "\n",
      " epoch 342 of 2000\n",
      "5/5 [==============================] - 0s 962us/step\n",
      "11/11 [==============================] - 0s 826us/step\n",
      "(for 1 minibatch) Training loss 0.0221711 | PSNR training 34.0198860\n",
      "\n",
      " epoch 343 of 2000\n",
      "5/5 [==============================] - 0s 965us/step\n",
      "11/11 [==============================] - 0s 845us/step\n",
      "(for 1 minibatch) Training loss 0.0211899 | PSNR training 34.4744835\n",
      "\n",
      " epoch 344 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 871us/step\n",
      "(for 1 minibatch) Training loss 0.0217176 | PSNR training 34.2226715\n",
      "\n",
      " epoch 345 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 888us/step\n",
      "(for 1 minibatch) Training loss 0.0212337 | PSNR training 34.3716393\n",
      "\n",
      " epoch 346 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 900us/step\n",
      "(for 1 minibatch) Training loss 0.0205964 | PSNR training 34.4867325\n",
      "\n",
      " epoch 347 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 902us/step\n",
      "(for 1 minibatch) Training loss 0.0210097 | PSNR training 34.4412651\n",
      "\n",
      " epoch 348 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 894us/step\n",
      "(for 1 minibatch) Training loss 0.0188006 | PSNR training 35.3348465\n",
      "\n",
      " epoch 349 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 917us/step\n",
      "(for 1 minibatch) Training loss 0.0195462 | PSNR training 34.7738342\n",
      "\n",
      " epoch 350 of 2000\n",
      "5/5 [==============================] - 0s 951us/step\n",
      "11/11 [==============================] - 0s 850us/step\n",
      "(for 1 minibatch) Training loss 0.0205815 | PSNR training 34.3749313\n",
      "\n",
      " epoch 351 of 2000\n",
      "5/5 [==============================] - 0s 972us/step\n",
      "11/11 [==============================] - 0s 862us/step\n",
      "(for 1 minibatch) Training loss 0.0212137 | PSNR training 34.4027138\n",
      "\n",
      " epoch 352 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 875us/step\n",
      "(for 1 minibatch) Training loss 0.0228723 | PSNR training 34.3940926\n",
      "\n",
      " epoch 353 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 876us/step\n",
      "(for 1 minibatch) Training loss 0.0200238 | PSNR training 34.7513123\n",
      "\n",
      " epoch 354 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 843us/step\n",
      "(for 1 minibatch) Training loss 0.0192922 | PSNR training 34.8670540\n",
      "\n",
      " epoch 355 of 2000\n",
      "5/5 [==============================] - 0s 942us/step\n",
      "11/11 [==============================] - 0s 826us/step\n",
      "(for 1 minibatch) Training loss 0.0195820 | PSNR training 34.8241844\n",
      "\n",
      " epoch 356 of 2000\n",
      "5/5 [==============================] - 0s 982us/step\n",
      "11/11 [==============================] - 0s 875us/step\n",
      "(for 1 minibatch) Training loss 0.0183362 | PSNR training 35.4749260\n",
      "\n",
      " epoch 357 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 849us/step\n",
      "(for 1 minibatch) Training loss 0.0189694 | PSNR training 35.1654167\n",
      "\n",
      " epoch 358 of 2000\n",
      "5/5 [==============================] - 0s 920us/step\n",
      "11/11 [==============================] - 0s 839us/step\n",
      "(for 1 minibatch) Training loss 0.0199697 | PSNR training 34.8712730\n",
      "\n",
      " epoch 359 of 2000\n",
      "5/5 [==============================] - 0s 982us/step\n",
      "11/11 [==============================] - 0s 848us/step\n",
      "(for 1 minibatch) Training loss 0.0220854 | PSNR training 34.2196045\n",
      "\n",
      " epoch 360 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 843us/step\n",
      "(for 1 minibatch) Training loss 0.0221469 | PSNR training 34.4409523\n",
      "\n",
      " epoch 361 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 819us/step\n",
      "(for 1 minibatch) Training loss 0.0198310 | PSNR training 34.7791214\n",
      "\n",
      " epoch 362 of 2000\n",
      "5/5 [==============================] - 0s 980us/step\n",
      "11/11 [==============================] - 0s 881us/step\n",
      "(for 1 minibatch) Training loss 0.0186805 | PSNR training 35.2782784\n",
      "\n",
      " epoch 363 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 881us/step\n",
      "(for 1 minibatch) Training loss 0.0192384 | PSNR training 35.0191765\n",
      "\n",
      " epoch 364 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 830us/step\n",
      "(for 1 minibatch) Training loss 0.0213333 | PSNR training 34.6419106\n",
      "\n",
      " epoch 365 of 2000\n",
      "5/5 [==============================] - 0s 946us/step\n",
      "11/11 [==============================] - 0s 866us/step\n",
      "(for 1 minibatch) Training loss 0.0235772 | PSNR training 33.6383133\n",
      "\n",
      " epoch 366 of 2000\n",
      "5/5 [==============================] - 0s 986us/step\n",
      "11/11 [==============================] - 0s 829us/step\n",
      "(for 1 minibatch) Training loss 0.0207713 | PSNR training 34.7235146\n",
      "\n",
      " epoch 367 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 823us/step\n",
      "(for 1 minibatch) Training loss 0.0186281 | PSNR training 35.5938034\n",
      "\n",
      " epoch 368 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 858us/step\n",
      "(for 1 minibatch) Training loss 0.0188177 | PSNR training 35.2470245\n",
      "\n",
      " epoch 369 of 2000\n",
      "5/5 [==============================] - 0s 983us/step\n",
      "11/11 [==============================] - 0s 826us/step\n",
      "(for 1 minibatch) Training loss 0.0193183 | PSNR training 35.2500916\n",
      "\n",
      " epoch 370 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 842us/step\n",
      "(for 1 minibatch) Training loss 0.0209588 | PSNR training 34.8332024\n",
      "\n",
      " epoch 371 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 890us/step\n",
      "(for 1 minibatch) Training loss 0.0217395 | PSNR training 34.3820839\n",
      "\n",
      " epoch 372 of 2000\n",
      "5/5 [==============================] - 0s 953us/step\n",
      "11/11 [==============================] - 0s 861us/step\n",
      "(for 1 minibatch) Training loss 0.0199947 | PSNR training 35.0829506\n",
      "\n",
      " epoch 373 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 885us/step\n",
      "(for 1 minibatch) Training loss 0.0184059 | PSNR training 35.5358810\n",
      "\n",
      " epoch 374 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 858us/step\n",
      "(for 1 minibatch) Training loss 0.0176424 | PSNR training 35.5795937\n",
      "\n",
      " epoch 375 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 963us/step\n",
      "(for 1 minibatch) Training loss 0.0186487 | PSNR training 35.4947586\n",
      "\n",
      " epoch 376 of 2000\n",
      "5/5 [==============================] - 0s 971us/step\n",
      "11/11 [==============================] - 0s 877us/step\n",
      "(for 1 minibatch) Training loss 0.0184126 | PSNR training 35.3461456\n",
      "\n",
      " epoch 377 of 2000\n",
      "5/5 [==============================] - 0s 975us/step\n",
      "11/11 [==============================] - 0s 803us/step\n",
      "(for 1 minibatch) Training loss 0.0203806 | PSNR training 34.9172363\n",
      "\n",
      " epoch 378 of 2000\n",
      "5/5 [==============================] - 0s 963us/step\n",
      "11/11 [==============================] - 0s 862us/step\n",
      "(for 1 minibatch) Training loss 0.0212629 | PSNR training 34.8368034\n",
      "\n",
      " epoch 379 of 2000\n",
      "5/5 [==============================] - 0s 959us/step\n",
      "11/11 [==============================] - 0s 798us/step\n",
      "(for 1 minibatch) Training loss 0.0199209 | PSNR training 35.0266495\n",
      "\n",
      " epoch 380 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 871us/step\n",
      "(for 1 minibatch) Training loss 0.0183205 | PSNR training 35.4589310\n",
      "\n",
      " epoch 381 of 2000\n",
      "5/5 [==============================] - 0s 977us/step\n",
      "11/11 [==============================] - 0s 836us/step\n",
      "(for 1 minibatch) Training loss 0.0186965 | PSNR training 35.3762283\n",
      "\n",
      " epoch 382 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 842us/step\n",
      "(for 1 minibatch) Training loss 0.0186040 | PSNR training 35.6992607\n",
      "\n",
      " epoch 383 of 2000\n",
      "5/5 [==============================] - 0s 987us/step\n",
      "11/11 [==============================] - 0s 837us/step\n",
      "(for 1 minibatch) Training loss 0.0196385 | PSNR training 35.6825905\n",
      "\n",
      " epoch 384 of 2000\n",
      "5/5 [==============================] - 0s 934us/step\n",
      "11/11 [==============================] - 0s 869us/step\n",
      "(for 1 minibatch) Training loss 0.0210908 | PSNR training 35.1704292\n",
      "\n",
      " epoch 385 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 874us/step\n",
      "(for 1 minibatch) Training loss 0.0223547 | PSNR training 34.4792404\n",
      "\n",
      " epoch 386 of 2000\n",
      "5/5 [==============================] - 0s 908us/step\n",
      "11/11 [==============================] - 0s 894us/step\n",
      "(for 1 minibatch) Training loss 0.0210332 | PSNR training 34.7933006\n",
      "\n",
      " epoch 387 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 923us/step\n",
      "(for 1 minibatch) Training loss 0.0210975 | PSNR training 34.8281784\n",
      "\n",
      " epoch 388 of 2000\n",
      "5/5 [==============================] - 0s 978us/step\n",
      "11/11 [==============================] - 0s 842us/step\n",
      "(for 1 minibatch) Training loss 0.0206100 | PSNR training 35.2031059\n",
      "\n",
      " epoch 389 of 2000\n",
      "5/5 [==============================] - 0s 944us/step\n",
      "11/11 [==============================] - 0s 877us/step\n",
      "(for 1 minibatch) Training loss 0.0193043 | PSNR training 35.3190422\n",
      "\n",
      " epoch 390 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 825us/step\n",
      "(for 1 minibatch) Training loss 0.0180288 | PSNR training 35.6016083\n",
      "\n",
      " epoch 391 of 2000\n",
      "5/5 [==============================] - 0s 984us/step\n",
      "11/11 [==============================] - 0s 836us/step\n",
      "(for 1 minibatch) Training loss 0.0190686 | PSNR training 35.4569130\n",
      "\n",
      " epoch 392 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 814us/step\n",
      "(for 1 minibatch) Training loss 0.0212692 | PSNR training 34.9492798\n",
      "\n",
      " epoch 393 of 2000\n",
      "5/5 [==============================] - 0s 961us/step\n",
      "11/11 [==============================] - 0s 815us/step\n",
      "(for 1 minibatch) Training loss 0.0197924 | PSNR training 35.1374779\n",
      "\n",
      " epoch 394 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 865us/step\n",
      "(for 1 minibatch) Training loss 0.0185108 | PSNR training 35.7039642\n",
      "\n",
      " epoch 395 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 837us/step\n",
      "(for 1 minibatch) Training loss 0.0188685 | PSNR training 35.1292381\n",
      "\n",
      " epoch 396 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 841us/step\n",
      "(for 1 minibatch) Training loss 0.0195968 | PSNR training 35.1705742\n",
      "\n",
      " epoch 397 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 829us/step\n",
      "(for 1 minibatch) Training loss 0.0182642 | PSNR training 35.9394684\n",
      "\n",
      " epoch 398 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 853us/step\n",
      "(for 1 minibatch) Training loss 0.0194987 | PSNR training 35.4970665\n",
      "\n",
      " epoch 399 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 850us/step\n",
      "(for 1 minibatch) Training loss 0.0200885 | PSNR training 35.1916542\n",
      "\n",
      " epoch 400 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 855us/step\n",
      "(for 1 minibatch) Training loss 0.0198827 | PSNR training 34.8969116\n",
      "\n",
      " epoch 401 of 2000\n",
      "5/5 [==============================] - 0s 983us/step\n",
      "11/11 [==============================] - 0s 858us/step\n",
      "(for 1 minibatch) Training loss 0.0197627 | PSNR training 35.2135391\n",
      "\n",
      " epoch 402 of 2000\n",
      "5/5 [==============================] - 0s 987us/step\n",
      "11/11 [==============================] - 0s 845us/step\n",
      "(for 1 minibatch) Training loss 0.0183303 | PSNR training 35.5160332\n",
      "\n",
      " epoch 403 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 879us/step\n",
      "(for 1 minibatch) Training loss 0.0179406 | PSNR training 35.8312531\n",
      "\n",
      " epoch 404 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 979us/step\n",
      "(for 1 minibatch) Training loss 0.0185791 | PSNR training 35.7043991\n",
      "\n",
      " epoch 405 of 2000\n",
      "5/5 [==============================] - 0s 993us/step\n",
      "11/11 [==============================] - 0s 819us/step\n",
      "(for 1 minibatch) Training loss 0.0193121 | PSNR training 35.2395935\n",
      "\n",
      " epoch 406 of 2000\n",
      "5/5 [==============================] - 0s 983us/step\n",
      "11/11 [==============================] - 0s 875us/step\n",
      "(for 1 minibatch) Training loss 0.0206801 | PSNR training 34.8901901\n",
      "\n",
      " epoch 407 of 2000\n",
      "5/5 [==============================] - 0s 963us/step\n",
      "11/11 [==============================] - 0s 887us/step\n",
      "(for 1 minibatch) Training loss 0.0199580 | PSNR training 35.2523499\n",
      "\n",
      " epoch 408 of 2000\n",
      "5/5 [==============================] - 0s 969us/step\n",
      "11/11 [==============================] - 0s 876us/step\n",
      "(for 1 minibatch) Training loss 0.0187964 | PSNR training 35.5512924\n",
      "\n",
      " epoch 409 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 832us/step\n",
      "(for 1 minibatch) Training loss 0.0173865 | PSNR training 36.1714630\n",
      "\n",
      " epoch 410 of 2000\n",
      "5/5 [==============================] - 0s 950us/step\n",
      "11/11 [==============================] - 0s 885us/step\n",
      "(for 1 minibatch) Training loss 0.0180547 | PSNR training 35.6608925\n",
      "\n",
      " epoch 411 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 823us/step\n",
      "(for 1 minibatch) Training loss 0.0182151 | PSNR training 35.5821915\n",
      "\n",
      " epoch 412 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 899us/step\n",
      "(for 1 minibatch) Training loss 0.0186422 | PSNR training 35.6582451\n",
      "\n",
      " epoch 413 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 865us/step\n",
      "(for 1 minibatch) Training loss 0.0205417 | PSNR training 35.0941887\n",
      "\n",
      " epoch 414 of 2000\n",
      "5/5 [==============================] - 0s 935us/step\n",
      "11/11 [==============================] - 0s 875us/step\n",
      "(for 1 minibatch) Training loss 0.0202439 | PSNR training 35.1696663\n",
      "\n",
      " epoch 415 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 911us/step\n",
      "(for 1 minibatch) Training loss 0.0181406 | PSNR training 35.7743835\n",
      "\n",
      " epoch 416 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 856us/step\n",
      "(for 1 minibatch) Training loss 0.0175053 | PSNR training 36.1418076\n",
      "\n",
      " epoch 417 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 836us/step\n",
      "(for 1 minibatch) Training loss 0.0171115 | PSNR training 36.1528358\n",
      "\n",
      " epoch 418 of 2000\n",
      "5/5 [==============================] - 0s 945us/step\n",
      "11/11 [==============================] - 0s 925us/step\n",
      "(for 1 minibatch) Training loss 0.0186492 | PSNR training 35.7433739\n",
      "\n",
      " epoch 419 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 855us/step\n",
      "(for 1 minibatch) Training loss 0.0200257 | PSNR training 35.2979050\n",
      "\n",
      " epoch 420 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 848us/step\n",
      "(for 1 minibatch) Training loss 0.0207931 | PSNR training 35.0315895\n",
      "\n",
      " epoch 421 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 803us/step\n",
      "(for 1 minibatch) Training loss 0.0188710 | PSNR training 35.7102318\n",
      "\n",
      " epoch 422 of 2000\n",
      "5/5 [==============================] - 0s 976us/step\n",
      "11/11 [==============================] - 0s 871us/step\n",
      "(for 1 minibatch) Training loss 0.0171970 | PSNR training 36.2066994\n",
      "\n",
      " epoch 423 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 808us/step\n",
      "(for 1 minibatch) Training loss 0.0173137 | PSNR training 36.2816544\n",
      "\n",
      " epoch 424 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 876us/step\n",
      "(for 1 minibatch) Training loss 0.0174584 | PSNR training 35.9109344\n",
      "\n",
      " epoch 425 of 2000\n",
      "5/5 [==============================] - 0s 928us/step\n",
      "11/11 [==============================] - 0s 809us/step\n",
      "(for 1 minibatch) Training loss 0.0183116 | PSNR training 36.1666832\n",
      "\n",
      " epoch 426 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 898us/step\n",
      "(for 1 minibatch) Training loss 0.0188780 | PSNR training 35.8680038\n",
      "\n",
      " epoch 427 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 829us/step\n",
      "(for 1 minibatch) Training loss 0.0190911 | PSNR training 35.7682037\n",
      "\n",
      " epoch 428 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 911us/step\n",
      "(for 1 minibatch) Training loss 0.0205458 | PSNR training 35.1558456\n",
      "\n",
      " epoch 429 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 812us/step\n",
      "(for 1 minibatch) Training loss 0.0201464 | PSNR training 35.3529320\n",
      "\n",
      " epoch 430 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 841us/step\n",
      "(for 1 minibatch) Training loss 0.0179826 | PSNR training 36.0269356\n",
      "\n",
      " epoch 431 of 2000\n",
      "5/5 [==============================] - 0s 932us/step\n",
      "11/11 [==============================] - 0s 813us/step\n",
      "(for 1 minibatch) Training loss 0.0177113 | PSNR training 35.6971207\n",
      "\n",
      " epoch 432 of 2000\n",
      "5/5 [==============================] - 0s 987us/step\n",
      "11/11 [==============================] - 0s 816us/step\n",
      "(for 1 minibatch) Training loss 0.0178545 | PSNR training 36.1561508\n",
      "\n",
      " epoch 433 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 825us/step\n",
      "(for 1 minibatch) Training loss 0.0187516 | PSNR training 35.6999245\n",
      "\n",
      " epoch 434 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 854us/step\n",
      "(for 1 minibatch) Training loss 0.0198051 | PSNR training 35.4650726\n",
      "\n",
      " epoch 435 of 2000\n",
      "5/5 [==============================] - 0s 980us/step\n",
      "11/11 [==============================] - 0s 908us/step\n",
      "(for 1 minibatch) Training loss 0.0193127 | PSNR training 35.4743881\n",
      "\n",
      " epoch 436 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 834us/step\n",
      "(for 1 minibatch) Training loss 0.0176809 | PSNR training 35.9013405\n",
      "\n",
      " epoch 437 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 818us/step\n",
      "(for 1 minibatch) Training loss 0.0168361 | PSNR training 36.4278259\n",
      "\n",
      " epoch 438 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 873us/step\n",
      "(for 1 minibatch) Training loss 0.0168379 | PSNR training 36.4445457\n",
      "\n",
      " epoch 439 of 2000\n",
      "5/5 [==============================] - 0s 989us/step\n",
      "11/11 [==============================] - 0s 815us/step\n",
      "(for 1 minibatch) Training loss 0.0174356 | PSNR training 35.8710098\n",
      "\n",
      " epoch 440 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 920us/step\n",
      "(for 1 minibatch) Training loss 0.0189187 | PSNR training 35.5741386\n",
      "\n",
      " epoch 441 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 856us/step\n",
      "(for 1 minibatch) Training loss 0.0205283 | PSNR training 35.1330948\n",
      "\n",
      " epoch 442 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 1ms/step\n",
      "(for 1 minibatch) Training loss 0.0191759 | PSNR training 35.6301956\n",
      "\n",
      " epoch 443 of 2000\n",
      "5/5 [==============================] - 0s 956us/step\n",
      "11/11 [==============================] - 0s 870us/step\n",
      "(for 1 minibatch) Training loss 0.0174854 | PSNR training 36.3073959\n",
      "\n",
      " epoch 444 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 1ms/step\n",
      "(for 1 minibatch) Training loss 0.0174401 | PSNR training 36.1383896\n",
      "\n",
      " epoch 445 of 2000\n",
      "5/5 [==============================] - 0s 940us/step\n",
      "11/11 [==============================] - 0s 869us/step\n",
      "(for 1 minibatch) Training loss 0.0187729 | PSNR training 35.9947815\n",
      "\n",
      " epoch 446 of 2000\n",
      "5/5 [==============================] - 0s 949us/step\n",
      "11/11 [==============================] - 0s 801us/step\n",
      "(for 1 minibatch) Training loss 0.0199202 | PSNR training 35.4260559\n",
      "\n",
      " epoch 447 of 2000\n",
      "5/5 [==============================] - 0s 969us/step\n",
      "11/11 [==============================] - 0s 844us/step\n",
      "(for 1 minibatch) Training loss 0.0188868 | PSNR training 35.7033272\n",
      "\n",
      " epoch 448 of 2000\n",
      "5/5 [==============================] - 0s 943us/step\n",
      "11/11 [==============================] - 0s 891us/step\n",
      "(for 1 minibatch) Training loss 0.0166759 | PSNR training 36.5273781\n",
      "\n",
      " epoch 449 of 2000\n",
      "5/5 [==============================] - 0s 959us/step\n",
      "11/11 [==============================] - 0s 921us/step\n",
      "(for 1 minibatch) Training loss 0.0167805 | PSNR training 36.7119026\n",
      "\n",
      " epoch 450 of 2000\n",
      "5/5 [==============================] - 0s 959us/step\n",
      "11/11 [==============================] - 0s 880us/step\n",
      "(for 1 minibatch) Training loss 0.0175987 | PSNR training 36.3561630\n",
      "\n",
      " epoch 451 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 850us/step\n",
      "(for 1 minibatch) Training loss 0.0189328 | PSNR training 35.8998184\n",
      "\n",
      " epoch 452 of 2000\n",
      "5/5 [==============================] - 0s 969us/step\n",
      "11/11 [==============================] - 0s 898us/step\n",
      "(for 1 minibatch) Training loss 0.0166606 | PSNR training 36.5835724\n",
      "\n",
      " epoch 453 of 2000\n",
      "5/5 [==============================] - 0s 977us/step\n",
      "11/11 [==============================] - 0s 863us/step\n",
      "(for 1 minibatch) Training loss 0.0175907 | PSNR training 36.3690147\n",
      "\n",
      " epoch 454 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 1ms/step\n",
      "(for 1 minibatch) Training loss 0.0167225 | PSNR training 36.5985527\n",
      "\n",
      " epoch 455 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 896us/step\n",
      "(for 1 minibatch) Training loss 0.0176807 | PSNR training 36.5080719\n",
      "\n",
      " epoch 456 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 889us/step\n",
      "(for 1 minibatch) Training loss 0.0168976 | PSNR training 36.7013016\n",
      "\n",
      " epoch 457 of 2000\n",
      "5/5 [==============================] - 0s 978us/step\n",
      "11/11 [==============================] - 0s 881us/step\n",
      "(for 1 minibatch) Training loss 0.0170353 | PSNR training 36.3327637\n",
      "\n",
      " epoch 458 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 1ms/step\n",
      "(for 1 minibatch) Training loss 0.0169144 | PSNR training 36.3896561\n",
      "\n",
      " epoch 459 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 995us/step\n",
      "(for 1 minibatch) Training loss 0.0163065 | PSNR training 36.5298538\n",
      "\n",
      " epoch 460 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 857us/step\n",
      "(for 1 minibatch) Training loss 0.0167197 | PSNR training 36.6638870\n",
      "\n",
      " epoch 461 of 2000\n",
      "5/5 [==============================] - 0s 983us/step\n",
      "11/11 [==============================] - 0s 856us/step\n",
      "(for 1 minibatch) Training loss 0.0169278 | PSNR training 36.3871994\n",
      "\n",
      " epoch 462 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 878us/step\n",
      "(for 1 minibatch) Training loss 0.0161325 | PSNR training 36.6972046\n",
      "\n",
      " epoch 463 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 960us/step\n",
      "(for 1 minibatch) Training loss 0.0156993 | PSNR training 36.8638306\n",
      "\n",
      " epoch 464 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 933us/step\n",
      "(for 1 minibatch) Training loss 0.0166917 | PSNR training 36.4951668\n",
      "\n",
      " epoch 465 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 893us/step\n",
      "(for 1 minibatch) Training loss 0.0163328 | PSNR training 36.5846558\n",
      "\n",
      " epoch 466 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 935us/step\n",
      "(for 1 minibatch) Training loss 0.0154464 | PSNR training 37.0590935\n",
      "\n",
      " epoch 467 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 889us/step\n",
      "(for 1 minibatch) Training loss 0.0162376 | PSNR training 36.6761971\n",
      "\n",
      " epoch 468 of 2000\n",
      "5/5 [==============================] - 0s 981us/step\n",
      "11/11 [==============================] - 0s 845us/step\n",
      "(for 1 minibatch) Training loss 0.0158878 | PSNR training 36.7345467\n",
      "\n",
      " epoch 469 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 917us/step\n",
      "(for 1 minibatch) Training loss 0.0159193 | PSNR training 36.9554749\n",
      "\n",
      " epoch 470 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 898us/step\n",
      "(for 1 minibatch) Training loss 0.0158982 | PSNR training 36.8776741\n",
      "\n",
      " epoch 471 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 837us/step\n",
      "(for 1 minibatch) Training loss 0.0163769 | PSNR training 36.7762566\n",
      "\n",
      " epoch 472 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 858us/step\n",
      "(for 1 minibatch) Training loss 0.0156314 | PSNR training 36.9696579\n",
      "\n",
      " epoch 473 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 988us/step\n",
      "(for 1 minibatch) Training loss 0.0157520 | PSNR training 36.9402542\n",
      "\n",
      " epoch 474 of 2000\n",
      "5/5 [==============================] - 0s 989us/step\n",
      "11/11 [==============================] - 0s 903us/step\n",
      "(for 1 minibatch) Training loss 0.0158728 | PSNR training 36.9684677\n",
      "\n",
      " epoch 475 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 902us/step\n",
      "(for 1 minibatch) Training loss 0.0160339 | PSNR training 36.7643890\n",
      "\n",
      " epoch 476 of 2000\n",
      "5/5 [==============================] - 0s 924us/step\n",
      "11/11 [==============================] - 0s 862us/step\n",
      "(for 1 minibatch) Training loss 0.0160772 | PSNR training 36.5642586\n",
      "\n",
      " epoch 477 of 2000\n",
      "5/5 [==============================] - 0s 976us/step\n",
      "11/11 [==============================] - 0s 865us/step\n",
      "(for 1 minibatch) Training loss 0.0156323 | PSNR training 37.1150665\n",
      "\n",
      " epoch 478 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 856us/step\n",
      "(for 1 minibatch) Training loss 0.0159271 | PSNR training 37.0491333\n",
      "\n",
      " epoch 479 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 904us/step\n",
      "(for 1 minibatch) Training loss 0.0159326 | PSNR training 36.9406204\n",
      "\n",
      " epoch 480 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 797us/step\n",
      "(for 1 minibatch) Training loss 0.0160520 | PSNR training 36.6735535\n",
      "\n",
      " epoch 481 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 826us/step\n",
      "(for 1 minibatch) Training loss 0.0155233 | PSNR training 37.3305435\n",
      "\n",
      " epoch 482 of 2000\n",
      "5/5 [==============================] - 0s 952us/step\n",
      "11/11 [==============================] - 0s 795us/step\n",
      "(for 1 minibatch) Training loss 0.0153193 | PSNR training 36.9734230\n",
      "\n",
      " epoch 483 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 842us/step\n",
      "(for 1 minibatch) Training loss 0.0156224 | PSNR training 36.8377724\n",
      "\n",
      " epoch 484 of 2000\n",
      "5/5 [==============================] - 0s 987us/step\n",
      "11/11 [==============================] - 0s 831us/step\n",
      "(for 1 minibatch) Training loss 0.0153059 | PSNR training 37.1149101\n",
      "\n",
      " epoch 485 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 829us/step\n",
      "(for 1 minibatch) Training loss 0.0158680 | PSNR training 37.0924225\n",
      "\n",
      " epoch 486 of 2000\n",
      "5/5 [==============================] - 0s 923us/step\n",
      "11/11 [==============================] - 0s 840us/step\n",
      "(for 1 minibatch) Training loss 0.0159157 | PSNR training 36.8138847\n",
      "\n",
      " epoch 487 of 2000\n",
      "5/5 [==============================] - 0s 941us/step\n",
      "11/11 [==============================] - 0s 859us/step\n",
      "(for 1 minibatch) Training loss 0.0156235 | PSNR training 36.8396149\n",
      "\n",
      " epoch 488 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 813us/step\n",
      "(for 1 minibatch) Training loss 0.0151128 | PSNR training 37.3641205\n",
      "\n",
      " epoch 489 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 812us/step\n",
      "(for 1 minibatch) Training loss 0.0155377 | PSNR training 37.1520882\n",
      "\n",
      " epoch 490 of 2000\n",
      "5/5 [==============================] - 0s 957us/step\n",
      "11/11 [==============================] - 0s 822us/step\n",
      "(for 1 minibatch) Training loss 0.0155718 | PSNR training 36.9480553\n",
      "\n",
      " epoch 491 of 2000\n",
      "5/5 [==============================] - 0s 949us/step\n",
      "11/11 [==============================] - 0s 876us/step\n",
      "(for 1 minibatch) Training loss 0.0159613 | PSNR training 36.8992386\n",
      "\n",
      " epoch 492 of 2000\n",
      "5/5 [==============================] - 0s 990us/step\n",
      "11/11 [==============================] - 0s 816us/step\n",
      "(for 1 minibatch) Training loss 0.0151395 | PSNR training 37.5884018\n",
      "\n",
      " epoch 493 of 2000\n",
      "5/5 [==============================] - 0s 983us/step\n",
      "11/11 [==============================] - 0s 840us/step\n",
      "(for 1 minibatch) Training loss 0.0158687 | PSNR training 37.0949936\n",
      "\n",
      " epoch 494 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 853us/step\n",
      "(for 1 minibatch) Training loss 0.0157911 | PSNR training 37.2574158\n",
      "\n",
      " epoch 495 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 861us/step\n",
      "(for 1 minibatch) Training loss 0.0159664 | PSNR training 36.9664536\n",
      "\n",
      " epoch 496 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 822us/step\n",
      "(for 1 minibatch) Training loss 0.0163338 | PSNR training 36.9138908\n",
      "\n",
      " epoch 497 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 785us/step\n",
      "(for 1 minibatch) Training loss 0.0150926 | PSNR training 37.4454269\n",
      "\n",
      " epoch 498 of 2000\n",
      "5/5 [==============================] - 0s 980us/step\n",
      "11/11 [==============================] - 0s 830us/step\n",
      "(for 1 minibatch) Training loss 0.0153766 | PSNR training 37.3038902\n",
      "\n",
      " epoch 499 of 2000\n",
      "5/5 [==============================] - 0s 973us/step\n",
      "11/11 [==============================] - 0s 835us/step\n",
      "(for 1 minibatch) Training loss 0.0149899 | PSNR training 37.2328987\n",
      "\n",
      " epoch 500 of 2000\n",
      "5/5 [==============================] - 0s 945us/step\n",
      "11/11 [==============================] - 0s 820us/step\n",
      "(for 1 minibatch) Training loss 0.0152159 | PSNR training 37.1065559\n",
      "\n",
      " epoch 501 of 2000\n",
      "5/5 [==============================] - 0s 989us/step\n",
      "11/11 [==============================] - 0s 819us/step\n",
      "(for 1 minibatch) Training loss 0.0149670 | PSNR training 37.3389664\n",
      "\n",
      " epoch 502 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 833us/step\n",
      "(for 1 minibatch) Training loss 0.0155101 | PSNR training 37.3581238\n",
      "\n",
      " epoch 503 of 2000\n",
      "5/5 [==============================] - 0s 980us/step\n",
      "11/11 [==============================] - 0s 825us/step\n",
      "(for 1 minibatch) Training loss 0.0155780 | PSNR training 37.1172371\n",
      "\n",
      " epoch 504 of 2000\n",
      "5/5 [==============================] - 0s 931us/step\n",
      "11/11 [==============================] - 0s 834us/step\n",
      "(for 1 minibatch) Training loss 0.0153592 | PSNR training 37.2005577\n",
      "\n",
      " epoch 505 of 2000\n",
      "5/5 [==============================] - 0s 985us/step\n",
      "11/11 [==============================] - 0s 824us/step\n",
      "(for 1 minibatch) Training loss 0.0151743 | PSNR training 37.2810059\n",
      "\n",
      " epoch 506 of 2000\n",
      "5/5 [==============================] - 0s 898us/step\n",
      "11/11 [==============================] - 0s 848us/step\n",
      "(for 1 minibatch) Training loss 0.0151045 | PSNR training 37.1350327\n",
      "\n",
      " epoch 507 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 802us/step\n",
      "(for 1 minibatch) Training loss 0.0151567 | PSNR training 37.3484039\n",
      "\n",
      " epoch 508 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 830us/step\n",
      "(for 1 minibatch) Training loss 0.0148843 | PSNR training 37.2967339\n",
      "\n",
      " epoch 509 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 842us/step\n",
      "(for 1 minibatch) Training loss 0.0156007 | PSNR training 37.1347885\n",
      "\n",
      " epoch 510 of 2000\n",
      "5/5 [==============================] - 0s 952us/step\n",
      "11/11 [==============================] - 0s 839us/step\n",
      "(for 1 minibatch) Training loss 0.0157289 | PSNR training 36.9363861\n",
      "\n",
      " epoch 511 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 828us/step\n",
      "(for 1 minibatch) Training loss 0.0152212 | PSNR training 37.4750061\n",
      "\n",
      " epoch 512 of 2000\n",
      "5/5 [==============================] - 0s 960us/step\n",
      "11/11 [==============================] - 0s 873us/step\n",
      "(for 1 minibatch) Training loss 0.0153509 | PSNR training 37.3367691\n",
      "\n",
      " epoch 513 of 2000\n",
      "5/5 [==============================] - 0s 976us/step\n",
      "11/11 [==============================] - 0s 858us/step\n",
      "(for 1 minibatch) Training loss 0.0150958 | PSNR training 37.8025246\n",
      "\n",
      " epoch 514 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 844us/step\n",
      "(for 1 minibatch) Training loss 0.0154157 | PSNR training 37.2899284\n",
      "\n",
      " epoch 515 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 869us/step\n",
      "(for 1 minibatch) Training loss 0.0158526 | PSNR training 36.8814545\n",
      "\n",
      " epoch 516 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 975us/step\n",
      "(for 1 minibatch) Training loss 0.0156708 | PSNR training 37.1396751\n",
      "\n",
      " epoch 517 of 2000\n",
      "5/5 [==============================] - 0s 967us/step\n",
      "11/11 [==============================] - 0s 840us/step\n",
      "(for 1 minibatch) Training loss 0.0153701 | PSNR training 37.1615448\n",
      "\n",
      " epoch 518 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 816us/step\n",
      "(for 1 minibatch) Training loss 0.0154718 | PSNR training 37.3708801\n",
      "\n",
      " epoch 519 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 865us/step\n",
      "(for 1 minibatch) Training loss 0.0146652 | PSNR training 37.3390732\n",
      "\n",
      " epoch 520 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 972us/step\n",
      "(for 1 minibatch) Training loss 0.0155317 | PSNR training 37.1025505\n",
      "\n",
      " epoch 521 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 874us/step\n",
      "(for 1 minibatch) Training loss 0.0148494 | PSNR training 37.3450546\n",
      "\n",
      " epoch 522 of 2000\n",
      "5/5 [==============================] - 0s 979us/step\n",
      "11/11 [==============================] - 0s 867us/step\n",
      "(for 1 minibatch) Training loss 0.0151135 | PSNR training 37.2663307\n",
      "\n",
      " epoch 523 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 868us/step\n",
      "(for 1 minibatch) Training loss 0.0149644 | PSNR training 37.5620689\n",
      "\n",
      " epoch 524 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 843us/step\n",
      "(for 1 minibatch) Training loss 0.0154948 | PSNR training 37.3676987\n",
      "\n",
      " epoch 525 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 908us/step\n",
      "(for 1 minibatch) Training loss 0.0149375 | PSNR training 37.6496239\n",
      "\n",
      " epoch 526 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 966us/step\n",
      "(for 1 minibatch) Training loss 0.0152153 | PSNR training 37.4758568\n",
      "\n",
      " epoch 527 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 801us/step\n",
      "(for 1 minibatch) Training loss 0.0151926 | PSNR training 37.4790230\n",
      "\n",
      " epoch 528 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 812us/step\n",
      "(for 1 minibatch) Training loss 0.0153840 | PSNR training 37.4412994\n",
      "\n",
      " epoch 529 of 2000\n",
      "5/5 [==============================] - 0s 944us/step\n",
      "11/11 [==============================] - 0s 902us/step\n",
      "(for 1 minibatch) Training loss 0.0151517 | PSNR training 37.2954559\n",
      "\n",
      " epoch 530 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 793us/step\n",
      "(for 1 minibatch) Training loss 0.0151377 | PSNR training 37.6639786\n",
      "\n",
      " epoch 531 of 2000\n",
      "5/5 [==============================] - 0s 961us/step\n",
      "11/11 [==============================] - 0s 809us/step\n",
      "(for 1 minibatch) Training loss 0.0154701 | PSNR training 37.0085106\n",
      "\n",
      " epoch 532 of 2000\n",
      "5/5 [==============================] - 0s 996us/step\n",
      "11/11 [==============================] - 0s 976us/step\n",
      "(for 1 minibatch) Training loss 0.0149881 | PSNR training 37.3566895\n",
      "\n",
      " epoch 533 of 2000\n",
      "5/5 [==============================] - 0s 955us/step\n",
      "11/11 [==============================] - 0s 820us/step\n",
      "(for 1 minibatch) Training loss 0.0154880 | PSNR training 37.2995148\n",
      "\n",
      " epoch 534 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 849us/step\n",
      "(for 1 minibatch) Training loss 0.0149848 | PSNR training 37.5365906\n",
      "\n",
      " epoch 535 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 865us/step\n",
      "(for 1 minibatch) Training loss 0.0149187 | PSNR training 37.6598511\n",
      "\n",
      " epoch 536 of 2000\n",
      "5/5 [==============================] - 0s 972us/step\n",
      "11/11 [==============================] - 0s 899us/step\n",
      "(for 1 minibatch) Training loss 0.0152427 | PSNR training 37.6428490\n",
      "\n",
      " epoch 537 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 862us/step\n",
      "(for 1 minibatch) Training loss 0.0154589 | PSNR training 36.9483528\n",
      "\n",
      " epoch 538 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 823us/step\n",
      "(for 1 minibatch) Training loss 0.0146145 | PSNR training 37.5689659\n",
      "\n",
      " epoch 539 of 2000\n",
      "5/5 [==============================] - 0s 999us/step\n",
      "11/11 [==============================] - 0s 883us/step\n",
      "(for 1 minibatch) Training loss 0.0149803 | PSNR training 37.3860207\n",
      "\n",
      " epoch 540 of 2000\n",
      "5/5 [==============================] - 0s 954us/step\n",
      "11/11 [==============================] - 0s 815us/step\n",
      "(for 1 minibatch) Training loss 0.0147539 | PSNR training 37.6739807\n",
      "\n",
      " epoch 541 of 2000\n",
      "5/5 [==============================] - 0s 980us/step\n",
      "11/11 [==============================] - 0s 896us/step\n",
      "(for 1 minibatch) Training loss 0.0146281 | PSNR training 37.5013008\n",
      "\n",
      " epoch 542 of 2000\n",
      "5/5 [==============================] - 0s 992us/step\n",
      "11/11 [==============================] - 0s 858us/step\n",
      "(for 1 minibatch) Training loss 0.0148790 | PSNR training 37.5976334\n",
      "\n",
      " epoch 543 of 2000\n",
      "5/5 [==============================] - 0s 960us/step\n",
      "11/11 [==============================] - 0s 933us/step\n",
      "(for 1 minibatch) Training loss 0.0153531 | PSNR training 37.3172646\n",
      "\n",
      " epoch 544 of 2000\n",
      "5/5 [==============================] - 0s 992us/step\n",
      "11/11 [==============================] - 0s 876us/step\n",
      "(for 1 minibatch) Training loss 0.0158476 | PSNR training 37.2332458\n",
      "\n",
      " epoch 545 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 886us/step\n",
      "(for 1 minibatch) Training loss 0.0155954 | PSNR training 37.4391212\n",
      "\n",
      " epoch 546 of 2000\n",
      "5/5 [==============================] - 0s 962us/step\n",
      "11/11 [==============================] - 0s 896us/step\n",
      "(for 1 minibatch) Training loss 0.0154736 | PSNR training 37.4224548\n",
      "\n",
      " epoch 547 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 852us/step\n",
      "(for 1 minibatch) Training loss 0.0148461 | PSNR training 37.8163681\n",
      "\n",
      " epoch 548 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 879us/step\n",
      "(for 1 minibatch) Training loss 0.0150850 | PSNR training 37.6019211\n",
      "\n",
      " epoch 549 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 892us/step\n",
      "(for 1 minibatch) Training loss 0.0146860 | PSNR training 37.5084496\n",
      "\n",
      " epoch 550 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 809us/step\n",
      "(for 1 minibatch) Training loss 0.0151719 | PSNR training 37.4826393\n",
      "\n",
      " epoch 551 of 2000\n",
      "5/5 [==============================] - 0s 953us/step\n",
      "11/11 [==============================] - 0s 847us/step\n",
      "(for 1 minibatch) Training loss 0.0155914 | PSNR training 37.3339767\n",
      "\n",
      " epoch 552 of 2000\n",
      "5/5 [==============================] - 0s 925us/step\n",
      "11/11 [==============================] - 0s 886us/step\n",
      "(for 1 minibatch) Training loss 0.0149986 | PSNR training 37.4657745\n",
      "\n",
      " epoch 553 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 832us/step\n",
      "(for 1 minibatch) Training loss 0.0153784 | PSNR training 37.4859161\n",
      "\n",
      " epoch 554 of 2000\n",
      "5/5 [==============================] - 0s 969us/step\n",
      "11/11 [==============================] - 0s 820us/step\n",
      "(for 1 minibatch) Training loss 0.0151687 | PSNR training 37.5142479\n",
      "\n",
      " epoch 555 of 2000\n",
      "5/5 [==============================] - 0s 927us/step\n",
      "11/11 [==============================] - 0s 800us/step\n",
      "(for 1 minibatch) Training loss 0.0148528 | PSNR training 37.7145538\n",
      "\n",
      " epoch 556 of 2000\n",
      "5/5 [==============================] - 0s 990us/step\n",
      "11/11 [==============================] - 0s 980us/step\n",
      "(for 1 minibatch) Training loss 0.0152678 | PSNR training 37.3736992\n",
      "\n",
      " epoch 557 of 2000\n",
      "5/5 [==============================] - 0s 996us/step\n",
      "11/11 [==============================] - 0s 845us/step\n",
      "(for 1 minibatch) Training loss 0.0147446 | PSNR training 37.7493324\n",
      "\n",
      " epoch 558 of 2000\n",
      "5/5 [==============================] - 0s 954us/step\n",
      "11/11 [==============================] - 0s 864us/step\n",
      "(for 1 minibatch) Training loss 0.0150188 | PSNR training 37.5550919\n",
      "\n",
      " epoch 559 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 832us/step\n",
      "(for 1 minibatch) Training loss 0.0151881 | PSNR training 37.3103981\n",
      "\n",
      " epoch 560 of 2000\n",
      "5/5 [==============================] - 0s 940us/step\n",
      "11/11 [==============================] - 0s 826us/step\n",
      "(for 1 minibatch) Training loss 0.0150907 | PSNR training 37.5364685\n",
      "\n",
      " epoch 561 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 842us/step\n",
      "(for 1 minibatch) Training loss 0.0153896 | PSNR training 37.1483307\n",
      "\n",
      " epoch 562 of 2000\n",
      "5/5 [==============================] - 0s 968us/step\n",
      "11/11 [==============================] - 0s 874us/step\n",
      "(for 1 minibatch) Training loss 0.0153784 | PSNR training 37.5182381\n",
      "\n",
      " epoch 563 of 2000\n",
      "5/5 [==============================] - 0s 904us/step\n",
      "11/11 [==============================] - 0s 833us/step\n",
      "(for 1 minibatch) Training loss 0.0153898 | PSNR training 37.4905815\n",
      "\n",
      " epoch 564 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 866us/step\n",
      "(for 1 minibatch) Training loss 0.0157923 | PSNR training 37.1416817\n",
      "\n",
      " epoch 565 of 2000\n",
      "5/5 [==============================] - 0s 987us/step\n",
      "11/11 [==============================] - 0s 854us/step\n",
      "(for 1 minibatch) Training loss 0.0152698 | PSNR training 37.5999031\n",
      "\n",
      " epoch 566 of 2000\n",
      "5/5 [==============================] - 0s 973us/step\n",
      "11/11 [==============================] - 0s 847us/step\n",
      "(for 1 minibatch) Training loss 0.0151146 | PSNR training 37.4768524\n",
      "\n",
      " epoch 567 of 2000\n",
      "5/5 [==============================] - 0s 995us/step\n",
      "11/11 [==============================] - 0s 864us/step\n",
      "(for 1 minibatch) Training loss 0.0143421 | PSNR training 37.8867798\n",
      "\n",
      " epoch 568 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 845us/step\n",
      "(for 1 minibatch) Training loss 0.0146639 | PSNR training 37.6581688\n",
      "\n",
      " epoch 569 of 2000\n",
      "5/5 [==============================] - 0s 953us/step\n",
      "11/11 [==============================] - 0s 831us/step\n",
      "(for 1 minibatch) Training loss 0.0151306 | PSNR training 37.7637062\n",
      "\n",
      " epoch 570 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 852us/step\n",
      "(for 1 minibatch) Training loss 0.0145593 | PSNR training 37.8474274\n",
      "\n",
      " epoch 571 of 2000\n",
      "5/5 [==============================] - 0s 941us/step\n",
      "11/11 [==============================] - 0s 862us/step\n",
      "(for 1 minibatch) Training loss 0.0152524 | PSNR training 37.5883713\n",
      "\n",
      " epoch 572 of 2000\n",
      "5/5 [==============================] - 0s 949us/step\n",
      "11/11 [==============================] - 0s 802us/step\n",
      "(for 1 minibatch) Training loss 0.0145454 | PSNR training 37.6782837\n",
      "\n",
      " epoch 573 of 2000\n",
      "5/5 [==============================] - 0s 926us/step\n",
      "11/11 [==============================] - 0s 811us/step\n",
      "(for 1 minibatch) Training loss 0.0146426 | PSNR training 37.9603806\n",
      "\n",
      " epoch 574 of 2000\n",
      "5/5 [==============================] - 0s 964us/step\n",
      "11/11 [==============================] - 0s 851us/step\n",
      "(for 1 minibatch) Training loss 0.0155290 | PSNR training 37.4329910\n",
      "\n",
      " epoch 575 of 2000\n",
      "5/5 [==============================] - 0s 984us/step\n",
      "11/11 [==============================] - 0s 800us/step\n",
      "(for 1 minibatch) Training loss 0.0149902 | PSNR training 37.6693001\n",
      "\n",
      " epoch 576 of 2000\n",
      "5/5 [==============================] - 0s 952us/step\n",
      "11/11 [==============================] - 0s 829us/step\n",
      "(for 1 minibatch) Training loss 0.0155331 | PSNR training 37.4489937\n",
      "\n",
      " epoch 577 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 905us/step\n",
      "(for 1 minibatch) Training loss 0.0157003 | PSNR training 37.2500916\n",
      "\n",
      " epoch 578 of 2000\n",
      "5/5 [==============================] - 0s 974us/step\n",
      "11/11 [==============================] - 0s 838us/step\n",
      "(for 1 minibatch) Training loss 0.0153801 | PSNR training 37.5464134\n",
      "\n",
      " epoch 579 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 849us/step\n",
      "(for 1 minibatch) Training loss 0.0155640 | PSNR training 37.1835251\n",
      "\n",
      " epoch 580 of 2000\n",
      "5/5 [==============================] - 0s 970us/step\n",
      "11/11 [==============================] - 0s 808us/step\n",
      "(for 1 minibatch) Training loss 0.0155037 | PSNR training 37.6043396\n",
      "\n",
      " epoch 581 of 2000\n",
      "5/5 [==============================] - 0s 942us/step\n",
      "11/11 [==============================] - 0s 833us/step\n",
      "(for 1 minibatch) Training loss 0.0149171 | PSNR training 37.6186676\n",
      "\n",
      " epoch 582 of 2000\n",
      "5/5 [==============================] - 0s 944us/step\n",
      "11/11 [==============================] - 0s 810us/step\n",
      "(for 1 minibatch) Training loss 0.0143968 | PSNR training 38.1046982\n",
      "\n",
      " epoch 583 of 2000\n",
      "5/5 [==============================] - 0s 958us/step\n",
      "11/11 [==============================] - 0s 856us/step\n",
      "(for 1 minibatch) Training loss 0.0147192 | PSNR training 37.9757957\n",
      "\n",
      " epoch 584 of 2000\n",
      "5/5 [==============================] - 0s 980us/step\n",
      "11/11 [==============================] - 0s 865us/step\n",
      "(for 1 minibatch) Training loss 0.0147790 | PSNR training 37.9039536\n",
      "\n",
      " epoch 585 of 2000\n",
      "5/5 [==============================] - 0s 997us/step\n",
      "11/11 [==============================] - 0s 835us/step\n",
      "(for 1 minibatch) Training loss 0.0142497 | PSNR training 37.7055435\n",
      "\n",
      " epoch 586 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 900us/step\n",
      "(for 1 minibatch) Training loss 0.0148638 | PSNR training 37.7050247\n",
      "\n",
      " epoch 587 of 2000\n",
      "5/5 [==============================] - 0s 968us/step\n",
      "11/11 [==============================] - 0s 863us/step\n",
      "(for 1 minibatch) Training loss 0.0148250 | PSNR training 37.8203239\n",
      "\n",
      " epoch 588 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 918us/step\n",
      "(for 1 minibatch) Training loss 0.0152592 | PSNR training 37.2695007\n",
      "\n",
      " epoch 589 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 900us/step\n",
      "(for 1 minibatch) Training loss 0.0155868 | PSNR training 37.5073776\n",
      "\n",
      " epoch 590 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 854us/step\n",
      "(for 1 minibatch) Training loss 0.0161975 | PSNR training 37.1804733\n",
      "\n",
      " epoch 591 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 844us/step\n",
      "(for 1 minibatch) Training loss 0.0159802 | PSNR training 37.3670578\n",
      "\n",
      " epoch 592 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 829us/step\n",
      "(for 1 minibatch) Training loss 0.0146691 | PSNR training 37.7078247\n",
      "\n",
      " epoch 593 of 2000\n",
      "5/5 [==============================] - 0s 936us/step\n",
      "11/11 [==============================] - 0s 883us/step\n",
      "(for 1 minibatch) Training loss 0.0146558 | PSNR training 37.8484879\n",
      "\n",
      " epoch 594 of 2000\n",
      "5/5 [==============================] - 0s 949us/step\n",
      "11/11 [==============================] - 0s 829us/step\n",
      "(for 1 minibatch) Training loss 0.0145002 | PSNR training 38.0655861\n",
      "\n",
      " epoch 595 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 825us/step\n",
      "(for 1 minibatch) Training loss 0.0152626 | PSNR training 37.4692993\n",
      "\n",
      " epoch 596 of 2000\n",
      "5/5 [==============================] - 0s 968us/step\n",
      "11/11 [==============================] - 0s 859us/step\n",
      "(for 1 minibatch) Training loss 0.0162147 | PSNR training 37.2482719\n",
      "\n",
      " epoch 597 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 835us/step\n",
      "(for 1 minibatch) Training loss 0.0159159 | PSNR training 37.2926712\n",
      "\n",
      " epoch 598 of 2000\n",
      "5/5 [==============================] - 0s 989us/step\n",
      "11/11 [==============================] - 0s 796us/step\n",
      "(for 1 minibatch) Training loss 0.0149503 | PSNR training 37.8536797\n",
      "\n",
      " epoch 599 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 817us/step\n",
      "(for 1 minibatch) Training loss 0.0149746 | PSNR training 37.8434296\n",
      "\n",
      " epoch 600 of 2000\n",
      "5/5 [==============================] - 0s 999us/step\n",
      "11/11 [==============================] - 0s 824us/step\n",
      "(for 1 minibatch) Training loss 0.0143126 | PSNR training 37.6979523\n",
      "\n",
      " epoch 601 of 2000\n",
      "5/5 [==============================] - 0s 969us/step\n",
      "11/11 [==============================] - 0s 869us/step\n",
      "(for 1 minibatch) Training loss 0.0145181 | PSNR training 37.9193459\n",
      "\n",
      " epoch 602 of 2000\n",
      "5/5 [==============================] - 0s 951us/step\n",
      "11/11 [==============================] - 0s 840us/step\n",
      "(for 1 minibatch) Training loss 0.0148953 | PSNR training 37.8427391\n",
      "\n",
      " epoch 603 of 2000\n",
      "5/5 [==============================] - 0s 913us/step\n",
      "11/11 [==============================] - 0s 901us/step\n",
      "(for 1 minibatch) Training loss 0.0150781 | PSNR training 37.8057938\n",
      "\n",
      " epoch 604 of 2000\n",
      "5/5 [==============================] - 0s 999us/step\n",
      "11/11 [==============================] - 0s 823us/step\n",
      "(for 1 minibatch) Training loss 0.0153735 | PSNR training 37.5212708\n",
      "\n",
      " epoch 605 of 2000\n",
      "5/5 [==============================] - 0s 925us/step\n",
      "11/11 [==============================] - 0s 815us/step\n",
      "(for 1 minibatch) Training loss 0.0153514 | PSNR training 37.2331085\n",
      "\n",
      " epoch 606 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 816us/step\n",
      "(for 1 minibatch) Training loss 0.0151765 | PSNR training 37.7264252\n",
      "\n",
      " epoch 607 of 2000\n",
      "5/5 [==============================] - 0s 995us/step\n",
      "11/11 [==============================] - 0s 847us/step\n",
      "(for 1 minibatch) Training loss 0.0148400 | PSNR training 37.8449554\n",
      "\n",
      " epoch 608 of 2000\n",
      "5/5 [==============================] - 0s 962us/step\n",
      "11/11 [==============================] - 0s 802us/step\n",
      "(for 1 minibatch) Training loss 0.0146189 | PSNR training 37.6598816\n",
      "\n",
      " epoch 609 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 984us/step\n",
      "(for 1 minibatch) Training loss 0.0142457 | PSNR training 37.9501305\n",
      "\n",
      " epoch 610 of 2000\n",
      "5/5 [==============================] - 0s 1000us/step\n",
      "11/11 [==============================] - 0s 922us/step\n",
      "(for 1 minibatch) Training loss 0.0150957 | PSNR training 37.7667084\n",
      "\n",
      " epoch 611 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 829us/step\n",
      "(for 1 minibatch) Training loss 0.0148956 | PSNR training 37.5153008\n",
      "\n",
      " epoch 612 of 2000\n",
      "5/5 [==============================] - 0s 956us/step\n",
      "11/11 [==============================] - 0s 887us/step\n",
      "(for 1 minibatch) Training loss 0.0153207 | PSNR training 37.6523209\n",
      "\n",
      " epoch 613 of 2000\n",
      "5/5 [==============================] - 0s 963us/step\n",
      "11/11 [==============================] - 0s 821us/step\n",
      "(for 1 minibatch) Training loss 0.0145192 | PSNR training 38.0331078\n",
      "\n",
      " epoch 614 of 2000\n",
      "5/5 [==============================] - 0s 982us/step\n",
      "11/11 [==============================] - 0s 815us/step\n",
      "(for 1 minibatch) Training loss 0.0144046 | PSNR training 37.7495270\n",
      "\n",
      " epoch 615 of 2000\n",
      "5/5 [==============================] - 0s 996us/step\n",
      "11/11 [==============================] - 0s 808us/step\n",
      "(for 1 minibatch) Training loss 0.0143623 | PSNR training 37.9641647\n",
      "\n",
      " epoch 616 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 908us/step\n",
      "(for 1 minibatch) Training loss 0.0140700 | PSNR training 38.1584206\n",
      "\n",
      " epoch 617 of 2000\n",
      "5/5 [==============================] - 0s 938us/step\n",
      "11/11 [==============================] - 0s 848us/step\n",
      "(for 1 minibatch) Training loss 0.0142227 | PSNR training 38.0584564\n",
      "\n",
      " epoch 618 of 2000\n",
      "5/5 [==============================] - 0s 956us/step\n",
      "11/11 [==============================] - 0s 825us/step\n",
      "(for 1 minibatch) Training loss 0.0144575 | PSNR training 37.8162651\n",
      "\n",
      " epoch 619 of 2000\n",
      "5/5 [==============================] - 0s 982us/step\n",
      "11/11 [==============================] - 0s 826us/step\n",
      "(for 1 minibatch) Training loss 0.0146355 | PSNR training 37.9215889\n",
      "\n",
      " epoch 620 of 2000\n",
      "5/5 [==============================] - 0s 926us/step\n",
      "11/11 [==============================] - 0s 854us/step\n",
      "(for 1 minibatch) Training loss 0.0147171 | PSNR training 37.5978050\n",
      "\n",
      " epoch 621 of 2000\n",
      "5/5 [==============================] - 0s 941us/step\n",
      "11/11 [==============================] - 0s 825us/step\n",
      "(for 1 minibatch) Training loss 0.0146449 | PSNR training 38.1067505\n",
      "\n",
      " epoch 622 of 2000\n",
      "5/5 [==============================] - 0s 927us/step\n",
      "11/11 [==============================] - 0s 842us/step\n",
      "(for 1 minibatch) Training loss 0.0146700 | PSNR training 37.7778473\n",
      "\n",
      " epoch 623 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 820us/step\n",
      "(for 1 minibatch) Training loss 0.0143340 | PSNR training 37.9570503\n",
      "\n",
      " epoch 624 of 2000\n",
      "5/5 [==============================] - 0s 948us/step\n",
      "11/11 [==============================] - 0s 829us/step\n",
      "(for 1 minibatch) Training loss 0.0155703 | PSNR training 37.5142937\n",
      "\n",
      " epoch 625 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 793us/step\n",
      "(for 1 minibatch) Training loss 0.0156639 | PSNR training 37.3746338\n",
      "\n",
      " epoch 626 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 834us/step\n",
      "(for 1 minibatch) Training loss 0.0155202 | PSNR training 37.5376549\n",
      "\n",
      " epoch 627 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 795us/step\n",
      "(for 1 minibatch) Training loss 0.0147954 | PSNR training 37.9305115\n",
      "\n",
      " epoch 628 of 2000\n",
      "5/5 [==============================] - 0s 959us/step\n",
      "11/11 [==============================] - 0s 864us/step\n",
      "(for 1 minibatch) Training loss 0.0146505 | PSNR training 37.9150696\n",
      "\n",
      " epoch 629 of 2000\n",
      "5/5 [==============================] - 0s 907us/step\n",
      "11/11 [==============================] - 0s 829us/step\n",
      "(for 1 minibatch) Training loss 0.0142553 | PSNR training 37.9989014\n",
      "\n",
      " epoch 630 of 2000\n",
      "5/5 [==============================] - 0s 952us/step\n",
      "11/11 [==============================] - 0s 824us/step\n",
      "(for 1 minibatch) Training loss 0.0138539 | PSNR training 38.2972221\n",
      "\n",
      " epoch 631 of 2000\n",
      "5/5 [==============================] - 0s 997us/step\n",
      "11/11 [==============================] - 0s 826us/step\n",
      "(for 1 minibatch) Training loss 0.0139102 | PSNR training 38.3623352\n",
      "\n",
      " epoch 632 of 2000\n",
      "5/5 [==============================] - 0s 943us/step\n",
      "11/11 [==============================] - 0s 883us/step\n",
      "(for 1 minibatch) Training loss 0.0147288 | PSNR training 37.8141060\n",
      "\n",
      " epoch 633 of 2000\n",
      "5/5 [==============================] - 0s 958us/step\n",
      "11/11 [==============================] - 0s 891us/step\n",
      "(for 1 minibatch) Training loss 0.0146284 | PSNR training 37.7320137\n",
      "\n",
      " epoch 634 of 2000\n",
      "5/5 [==============================] - 0s 956us/step\n",
      "11/11 [==============================] - 0s 836us/step\n",
      "(for 1 minibatch) Training loss 0.0147217 | PSNR training 37.5904350\n",
      "\n",
      " epoch 635 of 2000\n",
      "5/5 [==============================] - 0s 915us/step\n",
      "11/11 [==============================] - 0s 813us/step\n",
      "(for 1 minibatch) Training loss 0.0151591 | PSNR training 37.7948723\n",
      "\n",
      " epoch 636 of 2000\n",
      "5/5 [==============================] - 0s 978us/step\n",
      "11/11 [==============================] - 0s 819us/step\n",
      "(for 1 minibatch) Training loss 0.0153525 | PSNR training 37.4874077\n",
      "\n",
      " epoch 637 of 2000\n",
      "5/5 [==============================] - 0s 996us/step\n",
      "11/11 [==============================] - 0s 847us/step\n",
      "(for 1 minibatch) Training loss 0.0151863 | PSNR training 37.7745705\n",
      "\n",
      " epoch 638 of 2000\n",
      "5/5 [==============================] - 0s 957us/step\n",
      "11/11 [==============================] - 0s 922us/step\n",
      "(for 1 minibatch) Training loss 0.0145380 | PSNR training 37.9973030\n",
      "\n",
      " epoch 639 of 2000\n",
      "5/5 [==============================] - 0s 917us/step\n",
      "11/11 [==============================] - 0s 946us/step\n",
      "(for 1 minibatch) Training loss 0.0148525 | PSNR training 37.7774963\n",
      "\n",
      " epoch 640 of 2000\n",
      "5/5 [==============================] - 0s 977us/step\n",
      "11/11 [==============================] - 0s 932us/step\n",
      "(for 1 minibatch) Training loss 0.0147698 | PSNR training 37.9101257\n",
      "\n",
      " epoch 641 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 839us/step\n",
      "(for 1 minibatch) Training loss 0.0149295 | PSNR training 37.8323402\n",
      "\n",
      " epoch 642 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 1ms/step\n",
      "(for 1 minibatch) Training loss 0.0146651 | PSNR training 37.7773743\n",
      "\n",
      " epoch 643 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 839us/step\n",
      "(for 1 minibatch) Training loss 0.0152196 | PSNR training 37.6619797\n",
      "\n",
      " epoch 644 of 2000\n",
      "5/5 [==============================] - 0s 916us/step\n",
      "11/11 [==============================] - 0s 913us/step\n",
      "(for 1 minibatch) Training loss 0.0151100 | PSNR training 37.7039871\n",
      "\n",
      " epoch 645 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 1ms/step\n",
      "(for 1 minibatch) Training loss 0.0151486 | PSNR training 37.7599411\n",
      "\n",
      " epoch 646 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 1000us/step\n",
      "(for 1 minibatch) Training loss 0.0145759 | PSNR training 37.8879509\n",
      "\n",
      " epoch 647 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 871us/step\n",
      "(for 1 minibatch) Training loss 0.0140436 | PSNR training 38.1915512\n",
      "\n",
      " epoch 648 of 2000\n",
      "5/5 [==============================] - 0s 941us/step\n",
      "11/11 [==============================] - 0s 968us/step\n",
      "(for 1 minibatch) Training loss 0.0145772 | PSNR training 37.8519783\n",
      "\n",
      " epoch 649 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 858us/step\n",
      "(for 1 minibatch) Training loss 0.0136989 | PSNR training 38.4110680\n",
      "\n",
      " epoch 650 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 856us/step\n",
      "(for 1 minibatch) Training loss 0.0141780 | PSNR training 38.1640892\n",
      "\n",
      " epoch 651 of 2000\n",
      "5/5 [==============================] - 0s 975us/step\n",
      "11/11 [==============================] - 0s 863us/step\n",
      "(for 1 minibatch) Training loss 0.0139124 | PSNR training 38.2548485\n",
      "\n",
      " epoch 652 of 2000\n",
      "5/5 [==============================] - 0s 947us/step\n",
      "11/11 [==============================] - 0s 887us/step\n",
      "(for 1 minibatch) Training loss 0.0137892 | PSNR training 38.2973442\n",
      "\n",
      " epoch 653 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 814us/step\n",
      "(for 1 minibatch) Training loss 0.0139205 | PSNR training 38.2131729\n",
      "\n",
      " epoch 654 of 2000\n",
      "5/5 [==============================] - 0s 954us/step\n",
      "11/11 [==============================] - 0s 875us/step\n",
      "(for 1 minibatch) Training loss 0.0143414 | PSNR training 37.9982338\n",
      "\n",
      " epoch 655 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 889us/step\n",
      "(for 1 minibatch) Training loss 0.0145283 | PSNR training 38.1287537\n",
      "\n",
      " epoch 656 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 1ms/step\n",
      "(for 1 minibatch) Training loss 0.0150200 | PSNR training 37.6707497\n",
      "\n",
      " epoch 657 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 843us/step\n",
      "(for 1 minibatch) Training loss 0.0151415 | PSNR training 37.4237366\n",
      "\n",
      " epoch 658 of 2000\n",
      "5/5 [==============================] - 0s 905us/step\n",
      "11/11 [==============================] - 0s 972us/step\n",
      "(for 1 minibatch) Training loss 0.0145442 | PSNR training 37.9583282\n",
      "\n",
      " epoch 659 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 815us/step\n",
      "(for 1 minibatch) Training loss 0.0140343 | PSNR training 38.2453079\n",
      "\n",
      " epoch 660 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 833us/step\n",
      "(for 1 minibatch) Training loss 0.0142908 | PSNR training 38.0588379\n",
      "\n",
      " epoch 661 of 2000\n",
      "5/5 [==============================] - 0s 960us/step\n",
      "11/11 [==============================] - 0s 896us/step\n",
      "(for 1 minibatch) Training loss 0.0137906 | PSNR training 38.2667122\n",
      "\n",
      " epoch 662 of 2000\n",
      "5/5 [==============================] - 0s 996us/step\n",
      "11/11 [==============================] - 0s 843us/step\n",
      "(for 1 minibatch) Training loss 0.0136977 | PSNR training 38.3203545\n",
      "\n",
      " epoch 663 of 2000\n",
      "5/5 [==============================] - 0s 959us/step\n",
      "11/11 [==============================] - 0s 909us/step\n",
      "(for 1 minibatch) Training loss 0.0140721 | PSNR training 38.0190735\n",
      "\n",
      " epoch 664 of 2000\n",
      "5/5 [==============================] - 0s 975us/step\n",
      "11/11 [==============================] - 0s 906us/step\n",
      "(for 1 minibatch) Training loss 0.0146612 | PSNR training 38.0030365\n",
      "\n",
      " epoch 665 of 2000\n",
      "5/5 [==============================] - 0s 988us/step\n",
      "11/11 [==============================] - 0s 863us/step\n",
      "(for 1 minibatch) Training loss 0.0147655 | PSNR training 37.8384361\n",
      "\n",
      " epoch 666 of 2000\n",
      "5/5 [==============================] - 0s 975us/step\n",
      "11/11 [==============================] - 0s 797us/step\n",
      "(for 1 minibatch) Training loss 0.0151567 | PSNR training 37.4733658\n",
      "\n",
      " epoch 667 of 2000\n",
      "5/5 [==============================] - 0s 987us/step\n",
      "11/11 [==============================] - 0s 874us/step\n",
      "(for 1 minibatch) Training loss 0.0146701 | PSNR training 37.8298798\n",
      "\n",
      " epoch 668 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 913us/step\n",
      "(for 1 minibatch) Training loss 0.0141449 | PSNR training 38.0024414\n",
      "\n",
      " epoch 669 of 2000\n",
      "5/5 [==============================] - 0s 979us/step\n",
      "11/11 [==============================] - 0s 877us/step\n",
      "(for 1 minibatch) Training loss 0.0134863 | PSNR training 38.4305115\n",
      "\n",
      " epoch 670 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 1ms/step\n",
      "(for 1 minibatch) Training loss 0.0135903 | PSNR training 38.3040276\n",
      "\n",
      " epoch 671 of 2000\n",
      "5/5 [==============================] - 0s 994us/step\n",
      "11/11 [==============================] - 0s 871us/step\n",
      "(for 1 minibatch) Training loss 0.0143238 | PSNR training 38.0629578\n",
      "\n",
      " epoch 672 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 806us/step\n",
      "(for 1 minibatch) Training loss 0.0139823 | PSNR training 38.0921745\n",
      "\n",
      " epoch 673 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 858us/step\n",
      "(for 1 minibatch) Training loss 0.0144321 | PSNR training 38.2157822\n",
      "\n",
      " epoch 674 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 813us/step\n",
      "(for 1 minibatch) Training loss 0.0148492 | PSNR training 37.9039803\n",
      "\n",
      " epoch 675 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 836us/step\n",
      "(for 1 minibatch) Training loss 0.0146796 | PSNR training 37.9543381\n",
      "\n",
      " epoch 676 of 2000\n",
      "5/5 [==============================] - 0s 992us/step\n",
      "11/11 [==============================] - 0s 843us/step\n",
      "(for 1 minibatch) Training loss 0.0149505 | PSNR training 37.9456635\n",
      "\n",
      " epoch 677 of 2000\n",
      "5/5 [==============================] - 0s 978us/step\n",
      "11/11 [==============================] - 0s 848us/step\n",
      "(for 1 minibatch) Training loss 0.0154129 | PSNR training 37.6086769\n",
      "\n",
      " epoch 678 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 843us/step\n",
      "(for 1 minibatch) Training loss 0.0150173 | PSNR training 37.7979126\n",
      "\n",
      " epoch 679 of 2000\n",
      "5/5 [==============================] - 0s 981us/step\n",
      "11/11 [==============================] - 0s 828us/step\n",
      "(for 1 minibatch) Training loss 0.0139422 | PSNR training 38.4447441\n",
      "\n",
      " epoch 680 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 813us/step\n",
      "(for 1 minibatch) Training loss 0.0142927 | PSNR training 38.3334694\n",
      "\n",
      " epoch 681 of 2000\n",
      "5/5 [==============================] - 0s 981us/step\n",
      "11/11 [==============================] - 0s 810us/step\n",
      "(for 1 minibatch) Training loss 0.0144155 | PSNR training 38.1878281\n",
      "\n",
      " epoch 682 of 2000\n",
      "5/5 [==============================] - 0s 991us/step\n",
      "11/11 [==============================] - 0s 862us/step\n",
      "(for 1 minibatch) Training loss 0.0141361 | PSNR training 38.3621063\n",
      "\n",
      " epoch 683 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 827us/step\n",
      "(for 1 minibatch) Training loss 0.0141031 | PSNR training 38.2564964\n",
      "\n",
      " epoch 684 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 886us/step\n",
      "(for 1 minibatch) Training loss 0.0140148 | PSNR training 38.2067909\n",
      "\n",
      " epoch 685 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 922us/step\n",
      "(for 1 minibatch) Training loss 0.0137965 | PSNR training 38.4612389\n",
      "\n",
      " epoch 686 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 883us/step\n",
      "(for 1 minibatch) Training loss 0.0138495 | PSNR training 38.5312386\n",
      "\n",
      " epoch 687 of 2000\n",
      "5/5 [==============================] - 0s 910us/step\n",
      "11/11 [==============================] - 0s 827us/step\n",
      "(for 1 minibatch) Training loss 0.0138911 | PSNR training 38.4922981\n",
      "\n",
      " epoch 688 of 2000\n",
      "5/5 [==============================] - 0s 985us/step\n",
      "11/11 [==============================] - 0s 864us/step\n",
      "(for 1 minibatch) Training loss 0.0141280 | PSNR training 38.1886406\n",
      "\n",
      " epoch 689 of 2000\n",
      "5/5 [==============================] - 0s 987us/step\n",
      "11/11 [==============================] - 0s 880us/step\n",
      "(for 1 minibatch) Training loss 0.0152467 | PSNR training 37.8324623\n",
      "\n",
      " epoch 690 of 2000\n",
      "5/5 [==============================] - 0s 988us/step\n",
      "11/11 [==============================] - 0s 820us/step\n",
      "(for 1 minibatch) Training loss 0.0154195 | PSNR training 37.8604851\n",
      "\n",
      " epoch 691 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 838us/step\n",
      "(for 1 minibatch) Training loss 0.0154508 | PSNR training 37.6334076\n",
      "\n",
      " epoch 692 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 864us/step\n",
      "(for 1 minibatch) Training loss 0.0151954 | PSNR training 37.6379280\n",
      "\n",
      " epoch 693 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 829us/step\n",
      "(for 1 minibatch) Training loss 0.0144756 | PSNR training 38.0542488\n",
      "\n",
      " epoch 694 of 2000\n",
      "5/5 [==============================] - 0s 987us/step\n",
      "11/11 [==============================] - 0s 872us/step\n",
      "(for 1 minibatch) Training loss 0.0142694 | PSNR training 38.2386894\n",
      "\n",
      " epoch 695 of 2000\n",
      "5/5 [==============================] - 0s 965us/step\n",
      "11/11 [==============================] - 0s 818us/step\n",
      "(for 1 minibatch) Training loss 0.0139859 | PSNR training 38.2175789\n",
      "\n",
      " epoch 696 of 2000\n",
      "5/5 [==============================] - 0s 961us/step\n",
      "11/11 [==============================] - 0s 873us/step\n",
      "(for 1 minibatch) Training loss 0.0136920 | PSNR training 38.3721848\n",
      "\n",
      " epoch 697 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 827us/step\n",
      "(for 1 minibatch) Training loss 0.0145200 | PSNR training 38.1794090\n",
      "\n",
      " epoch 698 of 2000\n",
      "5/5 [==============================] - 0s 984us/step\n",
      "11/11 [==============================] - 0s 803us/step\n",
      "(for 1 minibatch) Training loss 0.0156624 | PSNR training 37.6988068\n",
      "\n",
      " epoch 699 of 2000\n",
      "5/5 [==============================] - 0s 968us/step\n",
      "11/11 [==============================] - 0s 844us/step\n",
      "(for 1 minibatch) Training loss 0.0148060 | PSNR training 37.9983292\n",
      "\n",
      " epoch 700 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 860us/step\n",
      "(for 1 minibatch) Training loss 0.0137832 | PSNR training 38.3010712\n",
      "\n",
      " epoch 701 of 2000\n",
      "5/5 [==============================] - 0s 991us/step\n",
      "11/11 [==============================] - 0s 851us/step\n",
      "(for 1 minibatch) Training loss 0.0136473 | PSNR training 38.5437355\n",
      "\n",
      " epoch 702 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 864us/step\n",
      "(for 1 minibatch) Training loss 0.0140371 | PSNR training 38.4787712\n",
      "\n",
      " epoch 703 of 2000\n",
      "5/5 [==============================] - 0s 984us/step\n",
      "11/11 [==============================] - 0s 876us/step\n",
      "(for 1 minibatch) Training loss 0.0146518 | PSNR training 37.8237228\n",
      "\n",
      " epoch 704 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 888us/step\n",
      "(for 1 minibatch) Training loss 0.0141722 | PSNR training 38.1771507\n",
      "\n",
      " epoch 705 of 2000\n",
      "5/5 [==============================] - 0s 961us/step\n",
      "11/11 [==============================] - 0s 869us/step\n",
      "(for 1 minibatch) Training loss 0.0142654 | PSNR training 38.1453285\n",
      "\n",
      " epoch 706 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 834us/step\n",
      "(for 1 minibatch) Training loss 0.0144281 | PSNR training 38.0672760\n",
      "\n",
      " epoch 707 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 830us/step\n",
      "(for 1 minibatch) Training loss 0.0143481 | PSNR training 38.2680244\n",
      "\n",
      " epoch 708 of 2000\n",
      "5/5 [==============================] - 0s 992us/step\n",
      "11/11 [==============================] - 0s 814us/step\n",
      "(for 1 minibatch) Training loss 0.0136567 | PSNR training 38.4688530\n",
      "\n",
      " epoch 709 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 844us/step\n",
      "(for 1 minibatch) Training loss 0.0134200 | PSNR training 38.6780243\n",
      "\n",
      " epoch 710 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 893us/step\n",
      "(for 1 minibatch) Training loss 0.0137974 | PSNR training 38.3909492\n",
      "\n",
      " epoch 711 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 861us/step\n",
      "(for 1 minibatch) Training loss 0.0135001 | PSNR training 38.7450943\n",
      "\n",
      " epoch 712 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 875us/step\n",
      "(for 1 minibatch) Training loss 0.0133029 | PSNR training 38.3728371\n",
      "\n",
      " epoch 713 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 864us/step\n",
      "(for 1 minibatch) Training loss 0.0134509 | PSNR training 38.4081573\n",
      "\n",
      " epoch 714 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 991us/step\n",
      "(for 1 minibatch) Training loss 0.0134527 | PSNR training 38.7142639\n",
      "\n",
      " epoch 715 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 867us/step\n",
      "(for 1 minibatch) Training loss 0.0138736 | PSNR training 38.4266510\n",
      "\n",
      " epoch 716 of 2000\n",
      "5/5 [==============================] - 0s 975us/step\n",
      "11/11 [==============================] - 0s 850us/step\n",
      "(for 1 minibatch) Training loss 0.0140011 | PSNR training 38.3834000\n",
      "\n",
      " epoch 717 of 2000\n",
      "5/5 [==============================] - 0s 961us/step\n",
      "11/11 [==============================] - 0s 844us/step\n",
      "(for 1 minibatch) Training loss 0.0139757 | PSNR training 38.2045135\n",
      "\n",
      " epoch 718 of 2000\n",
      "5/5 [==============================] - 0s 984us/step\n",
      "11/11 [==============================] - 0s 875us/step\n",
      "(for 1 minibatch) Training loss 0.0146868 | PSNR training 37.8749046\n",
      "\n",
      " epoch 719 of 2000\n",
      "5/5 [==============================] - 0s 985us/step\n",
      "11/11 [==============================] - 0s 841us/step\n",
      "(for 1 minibatch) Training loss 0.0143283 | PSNR training 38.0588684\n",
      "\n",
      " epoch 720 of 2000\n",
      "5/5 [==============================] - 0s 941us/step\n",
      "11/11 [==============================] - 0s 854us/step\n",
      "(for 1 minibatch) Training loss 0.0144209 | PSNR training 38.2141266\n",
      "\n",
      " epoch 721 of 2000\n",
      "5/5 [==============================] - 0s 993us/step\n",
      "11/11 [==============================] - 0s 868us/step\n",
      "(for 1 minibatch) Training loss 0.0144088 | PSNR training 38.2120171\n",
      "\n",
      " epoch 722 of 2000\n",
      "5/5 [==============================] - 0s 956us/step\n",
      "11/11 [==============================] - 0s 868us/step\n",
      "(for 1 minibatch) Training loss 0.0136526 | PSNR training 38.4972839\n",
      "\n",
      " epoch 723 of 2000\n",
      "5/5 [==============================] - 0s 932us/step\n",
      "11/11 [==============================] - 0s 869us/step\n",
      "(for 1 minibatch) Training loss 0.0142796 | PSNR training 38.3424683\n",
      "\n",
      " epoch 724 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 920us/step\n",
      "(for 1 minibatch) Training loss 0.0138544 | PSNR training 38.3605690\n",
      "\n",
      " epoch 725 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 842us/step\n",
      "(for 1 minibatch) Training loss 0.0149206 | PSNR training 37.9094276\n",
      "\n",
      " epoch 726 of 2000\n",
      "5/5 [==============================] - 0s 930us/step\n",
      "11/11 [==============================] - 0s 815us/step\n",
      "(for 1 minibatch) Training loss 0.0141428 | PSNR training 38.0917244\n",
      "\n",
      " epoch 727 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 800us/step\n",
      "(for 1 minibatch) Training loss 0.0135223 | PSNR training 38.4100456\n",
      "\n",
      " epoch 728 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 813us/step\n",
      "(for 1 minibatch) Training loss 0.0135954 | PSNR training 38.5472717\n",
      "\n",
      " epoch 729 of 2000\n",
      "5/5 [==============================] - 0s 970us/step\n",
      "11/11 [==============================] - 0s 819us/step\n",
      "(for 1 minibatch) Training loss 0.0133475 | PSNR training 38.6257172\n",
      "\n",
      " epoch 730 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 858us/step\n",
      "(for 1 minibatch) Training loss 0.0134180 | PSNR training 38.6473312\n",
      "\n",
      " epoch 731 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 966us/step\n",
      "(for 1 minibatch) Training loss 0.0135947 | PSNR training 38.4917908\n",
      "\n",
      " epoch 732 of 2000\n",
      "5/5 [==============================] - 0s 966us/step\n",
      "11/11 [==============================] - 0s 887us/step\n",
      "(for 1 minibatch) Training loss 0.0143835 | PSNR training 38.1077309\n",
      "\n",
      " epoch 733 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 837us/step\n",
      "(for 1 minibatch) Training loss 0.0149403 | PSNR training 37.5940170\n",
      "\n",
      " epoch 734 of 2000\n",
      "5/5 [==============================] - 0s 994us/step\n",
      "11/11 [==============================] - 0s 850us/step\n",
      "(for 1 minibatch) Training loss 0.0142862 | PSNR training 37.8947449\n",
      "\n",
      " epoch 735 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 863us/step\n",
      "(for 1 minibatch) Training loss 0.0132132 | PSNR training 38.7573013\n",
      "\n",
      " epoch 736 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 841us/step\n",
      "(for 1 minibatch) Training loss 0.0137045 | PSNR training 38.6667175\n",
      "\n",
      " epoch 737 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 829us/step\n",
      "(for 1 minibatch) Training loss 0.0134542 | PSNR training 38.6632156\n",
      "\n",
      " epoch 738 of 2000\n",
      "5/5 [==============================] - 0s 926us/step\n",
      "11/11 [==============================] - 0s 984us/step\n",
      "(for 1 minibatch) Training loss 0.0134816 | PSNR training 38.7201271\n",
      "\n",
      " epoch 739 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 836us/step\n",
      "(for 1 minibatch) Training loss 0.0147125 | PSNR training 38.2225609\n",
      "\n",
      " epoch 740 of 2000\n",
      "5/5 [==============================] - 0s 974us/step\n",
      "11/11 [==============================] - 0s 814us/step\n",
      "(for 1 minibatch) Training loss 0.0144200 | PSNR training 38.1553345\n",
      "\n",
      " epoch 741 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 821us/step\n",
      "(for 1 minibatch) Training loss 0.0141668 | PSNR training 38.1459122\n",
      "\n",
      " epoch 742 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 830us/step\n",
      "(for 1 minibatch) Training loss 0.0141802 | PSNR training 38.3381462\n",
      "\n",
      " epoch 743 of 2000\n",
      "5/5 [==============================] - 0s 992us/step\n",
      "11/11 [==============================] - 0s 852us/step\n",
      "(for 1 minibatch) Training loss 0.0136951 | PSNR training 38.6168404\n",
      "\n",
      " epoch 744 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 826us/step\n",
      "(for 1 minibatch) Training loss 0.0132385 | PSNR training 38.6280556\n",
      "\n",
      " epoch 745 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 844us/step\n",
      "(for 1 minibatch) Training loss 0.0136051 | PSNR training 38.2801476\n",
      "\n",
      " epoch 746 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 848us/step\n",
      "(for 1 minibatch) Training loss 0.0136564 | PSNR training 38.4900055\n",
      "\n",
      " epoch 747 of 2000\n",
      "5/5 [==============================] - 0s 994us/step\n",
      "11/11 [==============================] - 0s 871us/step\n",
      "(for 1 minibatch) Training loss 0.0140494 | PSNR training 38.4007721\n",
      "\n",
      " epoch 748 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 826us/step\n",
      "(for 1 minibatch) Training loss 0.0139400 | PSNR training 38.3553658\n",
      "\n",
      " epoch 749 of 2000\n",
      "5/5 [==============================] - 0s 984us/step\n",
      "11/11 [==============================] - 0s 869us/step\n",
      "(for 1 minibatch) Training loss 0.0145003 | PSNR training 38.3665009\n",
      "\n",
      " epoch 750 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 1ms/step\n",
      "(for 1 minibatch) Training loss 0.0141835 | PSNR training 38.1370430\n",
      "\n",
      " epoch 751 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 847us/step\n",
      "(for 1 minibatch) Training loss 0.0146411 | PSNR training 37.9263992\n",
      "\n",
      " epoch 752 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 1ms/step\n",
      "(for 1 minibatch) Training loss 0.0137426 | PSNR training 38.5604630\n",
      "\n",
      " epoch 753 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 942us/step\n",
      "(for 1 minibatch) Training loss 0.0133761 | PSNR training 38.7201729\n",
      "\n",
      " epoch 754 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 1ms/step\n",
      "(for 1 minibatch) Training loss 0.0134815 | PSNR training 38.6757736\n",
      "\n",
      " epoch 755 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 1ms/step\n",
      "(for 1 minibatch) Training loss 0.0139779 | PSNR training 38.2471390\n",
      "\n",
      " epoch 756 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 830us/step\n",
      "(for 1 minibatch) Training loss 0.0134336 | PSNR training 38.2476273\n",
      "\n",
      " epoch 757 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 839us/step\n",
      "(for 1 minibatch) Training loss 0.0142977 | PSNR training 38.2682877\n",
      "\n",
      " epoch 758 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 884us/step\n",
      "(for 1 minibatch) Training loss 0.0144820 | PSNR training 38.1420860\n",
      "\n",
      " epoch 759 of 2000\n",
      "5/5 [==============================] - 0s 920us/step\n",
      "11/11 [==============================] - 0s 836us/step\n",
      "(for 1 minibatch) Training loss 0.0142265 | PSNR training 38.1299896\n",
      "\n",
      " epoch 760 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 841us/step\n",
      "(for 1 minibatch) Training loss 0.0137254 | PSNR training 38.5390434\n",
      "\n",
      " epoch 761 of 2000\n",
      "5/5 [==============================] - 0s 947us/step\n",
      "11/11 [==============================] - 0s 815us/step\n",
      "(for 1 minibatch) Training loss 0.0133291 | PSNR training 38.7964592\n",
      "\n",
      " epoch 762 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 918us/step\n",
      "(for 1 minibatch) Training loss 0.0131671 | PSNR training 38.7486191\n",
      "\n",
      " epoch 763 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 845us/step\n",
      "(for 1 minibatch) Training loss 0.0131008 | PSNR training 38.8432846\n",
      "\n",
      " epoch 764 of 2000\n",
      "5/5 [==============================] - 0s 968us/step\n",
      "11/11 [==============================] - 0s 821us/step\n",
      "(for 1 minibatch) Training loss 0.0135857 | PSNR training 38.7420006\n",
      "\n",
      " epoch 765 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 872us/step\n",
      "(for 1 minibatch) Training loss 0.0132258 | PSNR training 38.7549820\n",
      "\n",
      " epoch 766 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 859us/step\n",
      "(for 1 minibatch) Training loss 0.0137537 | PSNR training 38.5744858\n",
      "\n",
      " epoch 767 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 859us/step\n",
      "(for 1 minibatch) Training loss 0.0139378 | PSNR training 38.4333611\n",
      "\n",
      " epoch 768 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 848us/step\n",
      "(for 1 minibatch) Training loss 0.0140988 | PSNR training 38.3338394\n",
      "\n",
      " epoch 769 of 2000\n",
      "5/5 [==============================] - 0s 950us/step\n",
      "11/11 [==============================] - 0s 932us/step\n",
      "(for 1 minibatch) Training loss 0.0141603 | PSNR training 38.1863098\n",
      "\n",
      " epoch 770 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 925us/step\n",
      "(for 1 minibatch) Training loss 0.0141938 | PSNR training 38.3542557\n",
      "\n",
      " epoch 771 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 897us/step\n",
      "(for 1 minibatch) Training loss 0.0141928 | PSNR training 38.2705574\n",
      "\n",
      " epoch 772 of 2000\n",
      "5/5 [==============================] - 0s 979us/step\n",
      "11/11 [==============================] - 0s 871us/step\n",
      "(for 1 minibatch) Training loss 0.0137181 | PSNR training 38.6335831\n",
      "\n",
      " epoch 773 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 817us/step\n",
      "(for 1 minibatch) Training loss 0.0135789 | PSNR training 38.4640846\n",
      "\n",
      " epoch 774 of 2000\n",
      "5/5 [==============================] - 0s 937us/step\n",
      "11/11 [==============================] - 0s 824us/step\n",
      "(for 1 minibatch) Training loss 0.0134813 | PSNR training 38.5705605\n",
      "\n",
      " epoch 775 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 889us/step\n",
      "(for 1 minibatch) Training loss 0.0138152 | PSNR training 38.6773987\n",
      "\n",
      " epoch 776 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 1ms/step\n",
      "(for 1 minibatch) Training loss 0.0134408 | PSNR training 38.9165649\n",
      "\n",
      " epoch 777 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 940us/step\n",
      "(for 1 minibatch) Training loss 0.0138206 | PSNR training 38.2280579\n",
      "\n",
      " epoch 778 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 861us/step\n",
      "(for 1 minibatch) Training loss 0.0138218 | PSNR training 38.2579842\n",
      "\n",
      " epoch 779 of 2000\n",
      "5/5 [==============================] - 0s 974us/step\n",
      "11/11 [==============================] - 0s 1ms/step\n",
      "(for 1 minibatch) Training loss 0.0137801 | PSNR training 38.6027832\n",
      "\n",
      " epoch 780 of 2000\n",
      "5/5 [==============================] - 0s 999us/step\n",
      "11/11 [==============================] - 0s 955us/step\n",
      "(for 1 minibatch) Training loss 0.0139610 | PSNR training 38.2848549\n",
      "\n",
      " epoch 781 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 976us/step\n",
      "(for 1 minibatch) Training loss 0.0137277 | PSNR training 38.5202637\n",
      "\n",
      " epoch 782 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 857us/step\n",
      "(for 1 minibatch) Training loss 0.0136780 | PSNR training 38.5412407\n",
      "\n",
      " epoch 783 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 857us/step\n",
      "(for 1 minibatch) Training loss 0.0137686 | PSNR training 38.2895432\n",
      "\n",
      " epoch 784 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 852us/step\n",
      "(for 1 minibatch) Training loss 0.0138167 | PSNR training 38.3682327\n",
      "\n",
      " epoch 785 of 2000\n",
      "5/5 [==============================] - 0s 954us/step\n",
      "11/11 [==============================] - 0s 852us/step\n",
      "(for 1 minibatch) Training loss 0.0135791 | PSNR training 38.3954811\n",
      "\n",
      " epoch 786 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 830us/step\n",
      "(for 1 minibatch) Training loss 0.0136598 | PSNR training 38.6506233\n",
      "\n",
      " epoch 787 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 875us/step\n",
      "(for 1 minibatch) Training loss 0.0136656 | PSNR training 38.3069954\n",
      "\n",
      " epoch 788 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 867us/step\n",
      "(for 1 minibatch) Training loss 0.0132071 | PSNR training 38.8033104\n",
      "\n",
      " epoch 789 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 1ms/step\n",
      "(for 1 minibatch) Training loss 0.0131249 | PSNR training 38.9584961\n",
      "\n",
      " epoch 790 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 919us/step\n",
      "(for 1 minibatch) Training loss 0.0134596 | PSNR training 38.4885483\n",
      "\n",
      " epoch 791 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 980us/step\n",
      "(for 1 minibatch) Training loss 0.0133691 | PSNR training 38.7588959\n",
      "\n",
      " epoch 792 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 897us/step\n",
      "(for 1 minibatch) Training loss 0.0137740 | PSNR training 38.6047897\n",
      "\n",
      " epoch 793 of 2000\n",
      "5/5 [==============================] - 0s 944us/step\n",
      "11/11 [==============================] - 0s 850us/step\n",
      "(for 1 minibatch) Training loss 0.0136955 | PSNR training 38.6265221\n",
      "\n",
      " epoch 794 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 847us/step\n",
      "(for 1 minibatch) Training loss 0.0135574 | PSNR training 38.7341347\n",
      "\n",
      " epoch 795 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 888us/step\n",
      "(for 1 minibatch) Training loss 0.0133396 | PSNR training 38.7352943\n",
      "\n",
      " epoch 796 of 2000\n",
      "5/5 [==============================] - 0s 997us/step\n",
      "11/11 [==============================] - 0s 853us/step\n",
      "(for 1 minibatch) Training loss 0.0133093 | PSNR training 38.7214890\n",
      "\n",
      " epoch 797 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 860us/step\n",
      "(for 1 minibatch) Training loss 0.0139302 | PSNR training 38.4546242\n",
      "\n",
      " epoch 798 of 2000\n",
      "5/5 [==============================] - 0s 926us/step\n",
      "11/11 [==============================] - 0s 865us/step\n",
      "(for 1 minibatch) Training loss 0.0153881 | PSNR training 37.6021042\n",
      "\n",
      " epoch 799 of 2000\n",
      "5/5 [==============================] - 0s 939us/step\n",
      "11/11 [==============================] - 0s 818us/step\n",
      "(for 1 minibatch) Training loss 0.0146186 | PSNR training 38.0660973\n",
      "\n",
      " epoch 800 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 839us/step\n",
      "(for 1 minibatch) Training loss 0.0135257 | PSNR training 38.4908905\n",
      "\n",
      " epoch 801 of 2000\n",
      "5/5 [==============================] - 0s 916us/step\n",
      "11/11 [==============================] - 0s 865us/step\n",
      "(for 1 minibatch) Training loss 0.0134678 | PSNR training 38.5349045\n",
      "\n",
      " epoch 802 of 2000\n",
      "5/5 [==============================] - 0s 930us/step\n",
      "11/11 [==============================] - 0s 834us/step\n",
      "(for 1 minibatch) Training loss 0.0134873 | PSNR training 38.6613884\n",
      "\n",
      " epoch 803 of 2000\n",
      "5/5 [==============================] - 0s 938us/step\n",
      "11/11 [==============================] - 0s 858us/step\n",
      "(for 1 minibatch) Training loss 0.0132167 | PSNR training 38.7672958\n",
      "\n",
      " epoch 804 of 2000\n",
      "5/5 [==============================] - 0s 932us/step\n",
      "11/11 [==============================] - 0s 855us/step\n",
      "(for 1 minibatch) Training loss 0.0132360 | PSNR training 38.8251762\n",
      "\n",
      " epoch 805 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 887us/step\n",
      "(for 1 minibatch) Training loss 0.0128092 | PSNR training 39.0423012\n",
      "\n",
      " epoch 806 of 2000\n",
      "5/5 [==============================] - 0s 957us/step\n",
      "11/11 [==============================] - 0s 886us/step\n",
      "(for 1 minibatch) Training loss 0.0129482 | PSNR training 39.0198212\n",
      "\n",
      " epoch 807 of 2000\n",
      "5/5 [==============================] - 0s 921us/step\n",
      "11/11 [==============================] - 0s 885us/step\n",
      "(for 1 minibatch) Training loss 0.0128645 | PSNR training 38.9613724\n",
      "\n",
      " epoch 808 of 2000\n",
      "5/5 [==============================] - 0s 943us/step\n",
      "11/11 [==============================] - 0s 868us/step\n",
      "(for 1 minibatch) Training loss 0.0130243 | PSNR training 38.7138557\n",
      "\n",
      " epoch 809 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 902us/step\n",
      "(for 1 minibatch) Training loss 0.0129436 | PSNR training 38.9343948\n",
      "\n",
      " epoch 810 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 840us/step\n",
      "(for 1 minibatch) Training loss 0.0134978 | PSNR training 38.8323631\n",
      "\n",
      " epoch 811 of 2000\n",
      "5/5 [==============================] - 0s 966us/step\n",
      "11/11 [==============================] - 0s 832us/step\n",
      "(for 1 minibatch) Training loss 0.0137224 | PSNR training 38.5405693\n",
      "\n",
      " epoch 812 of 2000\n",
      "5/5 [==============================] - 0s 923us/step\n",
      "11/11 [==============================] - 0s 932us/step\n",
      "(for 1 minibatch) Training loss 0.0138312 | PSNR training 38.3654137\n",
      "\n",
      " epoch 813 of 2000\n",
      "5/5 [==============================] - 0s 921us/step\n",
      "11/11 [==============================] - 0s 849us/step\n",
      "(for 1 minibatch) Training loss 0.0148656 | PSNR training 37.9860916\n",
      "\n",
      " epoch 814 of 2000\n",
      "5/5 [==============================] - 0s 976us/step\n",
      "11/11 [==============================] - 0s 842us/step\n",
      "(for 1 minibatch) Training loss 0.0140353 | PSNR training 38.2701378\n",
      "\n",
      " epoch 815 of 2000\n",
      "5/5 [==============================] - 0s 987us/step\n",
      "11/11 [==============================] - 0s 845us/step\n",
      "(for 1 minibatch) Training loss 0.0136437 | PSNR training 38.5641022\n",
      "\n",
      " epoch 816 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 817us/step\n",
      "(for 1 minibatch) Training loss 0.0136845 | PSNR training 38.5762138\n",
      "\n",
      " epoch 817 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 949us/step\n",
      "(for 1 minibatch) Training loss 0.0133069 | PSNR training 38.8129234\n",
      "\n",
      " epoch 818 of 2000\n",
      "5/5 [==============================] - 0s 963us/step\n",
      "11/11 [==============================] - 0s 819us/step\n",
      "(for 1 minibatch) Training loss 0.0130523 | PSNR training 38.8283882\n",
      "\n",
      " epoch 819 of 2000\n",
      "5/5 [==============================] - 0s 957us/step\n",
      "11/11 [==============================] - 0s 847us/step\n",
      "(for 1 minibatch) Training loss 0.0131611 | PSNR training 38.9379730\n",
      "\n",
      " epoch 820 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 865us/step\n",
      "(for 1 minibatch) Training loss 0.0133434 | PSNR training 38.6846504\n",
      "\n",
      " epoch 821 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 867us/step\n",
      "(for 1 minibatch) Training loss 0.0126669 | PSNR training 39.0070457\n",
      "\n",
      " epoch 822 of 2000\n",
      "5/5 [==============================] - 0s 956us/step\n",
      "11/11 [==============================] - 0s 821us/step\n",
      "(for 1 minibatch) Training loss 0.0134777 | PSNR training 38.5874176\n",
      "\n",
      " epoch 823 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 908us/step\n",
      "(for 1 minibatch) Training loss 0.0136619 | PSNR training 38.5232773\n",
      "\n",
      " epoch 824 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 879us/step\n",
      "(for 1 minibatch) Training loss 0.0142265 | PSNR training 38.3055153\n",
      "\n",
      " epoch 825 of 2000\n",
      "5/5 [==============================] - 0s 972us/step\n",
      "11/11 [==============================] - 0s 893us/step\n",
      "(for 1 minibatch) Training loss 0.0138869 | PSNR training 38.4379501\n",
      "\n",
      " epoch 826 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 917us/step\n",
      "(for 1 minibatch) Training loss 0.0138723 | PSNR training 38.4090347\n",
      "\n",
      " epoch 827 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 877us/step\n",
      "(for 1 minibatch) Training loss 0.0140635 | PSNR training 38.3676186\n",
      "\n",
      " epoch 828 of 2000\n",
      "5/5 [==============================] - 0s 995us/step\n",
      "11/11 [==============================] - 0s 822us/step\n",
      "(for 1 minibatch) Training loss 0.0133635 | PSNR training 38.8273163\n",
      "\n",
      " epoch 829 of 2000\n",
      "5/5 [==============================] - 0s 974us/step\n",
      "11/11 [==============================] - 0s 897us/step\n",
      "(for 1 minibatch) Training loss 0.0129325 | PSNR training 39.0451431\n",
      "\n",
      " epoch 830 of 2000\n",
      "5/5 [==============================] - 0s 968us/step\n",
      "11/11 [==============================] - 0s 856us/step\n",
      "(for 1 minibatch) Training loss 0.0133830 | PSNR training 38.8167953\n",
      "\n",
      " epoch 831 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 1ms/step\n",
      "(for 1 minibatch) Training loss 0.0126906 | PSNR training 39.1184769\n",
      "\n",
      " epoch 832 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 897us/step\n",
      "(for 1 minibatch) Training loss 0.0129085 | PSNR training 39.1828308\n",
      "\n",
      " epoch 833 of 2000\n",
      "5/5 [==============================] - 0s 973us/step\n",
      "11/11 [==============================] - 0s 925us/step\n",
      "(for 1 minibatch) Training loss 0.0126237 | PSNR training 39.3434219\n",
      "\n",
      " epoch 834 of 2000\n",
      "5/5 [==============================] - 0s 2ms/step\n",
      "11/11 [==============================] - 0s 922us/step\n",
      "(for 1 minibatch) Training loss 0.0128601 | PSNR training 38.6983376\n",
      "\n",
      " epoch 835 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 864us/step\n",
      "(for 1 minibatch) Training loss 0.0131359 | PSNR training 38.6240883\n",
      "\n",
      " epoch 836 of 2000\n",
      "5/5 [==============================] - 0s 971us/step\n",
      "11/11 [==============================] - 0s 965us/step\n",
      "(for 1 minibatch) Training loss 0.0130045 | PSNR training 38.7848320\n",
      "\n",
      " epoch 837 of 2000\n",
      "5/5 [==============================] - 0s 973us/step\n",
      "11/11 [==============================] - 0s 828us/step\n",
      "(for 1 minibatch) Training loss 0.0136509 | PSNR training 38.4619102\n",
      "\n",
      " epoch 838 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 952us/step\n",
      "(for 1 minibatch) Training loss 0.0143650 | PSNR training 38.1310425\n",
      "\n",
      " epoch 839 of 2000\n",
      "5/5 [==============================] - 0s 944us/step\n",
      "11/11 [==============================] - 0s 924us/step\n",
      "(for 1 minibatch) Training loss 0.0144760 | PSNR training 38.2907143\n",
      "\n",
      " epoch 840 of 2000\n",
      "5/5 [==============================] - 0s 999us/step\n",
      "11/11 [==============================] - 0s 1ms/step\n",
      "(for 1 minibatch) Training loss 0.0137346 | PSNR training 38.4567566\n",
      "\n",
      " epoch 841 of 2000\n",
      "5/5 [==============================] - 0s 976us/step\n",
      "11/11 [==============================] - 0s 912us/step\n",
      "(for 1 minibatch) Training loss 0.0130404 | PSNR training 38.5928726\n",
      "\n",
      " epoch 842 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 840us/step\n",
      "(for 1 minibatch) Training loss 0.0129791 | PSNR training 38.8357658\n",
      "\n",
      " epoch 843 of 2000\n",
      "5/5 [==============================] - 0s 948us/step\n",
      "11/11 [==============================] - 0s 813us/step\n",
      "(for 1 minibatch) Training loss 0.0124804 | PSNR training 39.1562195\n",
      "\n",
      " epoch 844 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 886us/step\n",
      "(for 1 minibatch) Training loss 0.0131086 | PSNR training 38.9490013\n",
      "\n",
      " epoch 845 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 867us/step\n",
      "(for 1 minibatch) Training loss 0.0131393 | PSNR training 38.9751968\n",
      "\n",
      " epoch 846 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 849us/step\n",
      "(for 1 minibatch) Training loss 0.0131525 | PSNR training 38.6041718\n",
      "\n",
      " epoch 847 of 2000\n",
      "5/5 [==============================] - 0s 981us/step\n",
      "11/11 [==============================] - 0s 804us/step\n",
      "(for 1 minibatch) Training loss 0.0129693 | PSNR training 38.8032379\n",
      "\n",
      " epoch 848 of 2000\n",
      "5/5 [==============================] - 0s 955us/step\n",
      "11/11 [==============================] - 0s 853us/step\n",
      "(for 1 minibatch) Training loss 0.0126784 | PSNR training 38.9909706\n",
      "\n",
      " epoch 849 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 848us/step\n",
      "(for 1 minibatch) Training loss 0.0125459 | PSNR training 39.0608978\n",
      "\n",
      " epoch 850 of 2000\n",
      "5/5 [==============================] - 0s 979us/step\n",
      "11/11 [==============================] - 0s 896us/step\n",
      "(for 1 minibatch) Training loss 0.0131462 | PSNR training 38.9264717\n",
      "\n",
      " epoch 851 of 2000\n",
      "5/5 [==============================] - 0s 991us/step\n",
      "11/11 [==============================] - 0s 853us/step\n",
      "(for 1 minibatch) Training loss 0.0132059 | PSNR training 38.7072067\n",
      "\n",
      " epoch 852 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 1ms/step\n",
      "(for 1 minibatch) Training loss 0.0139325 | PSNR training 38.3771782\n",
      "\n",
      " epoch 853 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 912us/step\n",
      "(for 1 minibatch) Training loss 0.0140042 | PSNR training 38.3263931\n",
      "\n",
      " epoch 854 of 2000\n",
      "5/5 [==============================] - 0s 987us/step\n",
      "11/11 [==============================] - 0s 880us/step\n",
      "(for 1 minibatch) Training loss 0.0137671 | PSNR training 38.6266251\n",
      "\n",
      " epoch 855 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 901us/step\n",
      "(for 1 minibatch) Training loss 0.0134004 | PSNR training 38.6375771\n",
      "\n",
      " epoch 856 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 902us/step\n",
      "(for 1 minibatch) Training loss 0.0130911 | PSNR training 38.9745102\n",
      "\n",
      " epoch 857 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 844us/step\n",
      "(for 1 minibatch) Training loss 0.0126150 | PSNR training 39.0299454\n",
      "\n",
      " epoch 858 of 2000\n",
      "5/5 [==============================] - 0s 996us/step\n",
      "11/11 [==============================] - 0s 816us/step\n",
      "(for 1 minibatch) Training loss 0.0130849 | PSNR training 38.8187141\n",
      "\n",
      " epoch 859 of 2000\n",
      "5/5 [==============================] - 0s 987us/step\n",
      "11/11 [==============================] - 0s 931us/step\n",
      "(for 1 minibatch) Training loss 0.0126807 | PSNR training 38.9659424\n",
      "\n",
      " epoch 860 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 834us/step\n",
      "(for 1 minibatch) Training loss 0.0131256 | PSNR training 38.9070854\n",
      "\n",
      " epoch 861 of 2000\n",
      "5/5 [==============================] - 0s 984us/step\n",
      "11/11 [==============================] - 0s 850us/step\n",
      "(for 1 minibatch) Training loss 0.0136539 | PSNR training 38.5691643\n",
      "\n",
      " epoch 862 of 2000\n",
      "5/5 [==============================] - 0s 995us/step\n",
      "11/11 [==============================] - 0s 875us/step\n",
      "(for 1 minibatch) Training loss 0.0146852 | PSNR training 37.8917503\n",
      "\n",
      " epoch 863 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 841us/step\n",
      "(for 1 minibatch) Training loss 0.0134916 | PSNR training 38.7940369\n",
      "\n",
      " epoch 864 of 2000\n",
      "5/5 [==============================] - 0s 969us/step\n",
      "11/11 [==============================] - 0s 858us/step\n",
      "(for 1 minibatch) Training loss 0.0124114 | PSNR training 39.2003670\n",
      "\n",
      " epoch 865 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 886us/step\n",
      "(for 1 minibatch) Training loss 0.0124877 | PSNR training 39.3005714\n",
      "\n",
      " epoch 866 of 2000\n",
      "5/5 [==============================] - 0s 976us/step\n",
      "11/11 [==============================] - 0s 892us/step\n",
      "(for 1 minibatch) Training loss 0.0130430 | PSNR training 39.0461121\n",
      "\n",
      " epoch 867 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 917us/step\n",
      "(for 1 minibatch) Training loss 0.0128481 | PSNR training 38.9847336\n",
      "\n",
      " epoch 868 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 812us/step\n",
      "(for 1 minibatch) Training loss 0.0136231 | PSNR training 38.6764984\n",
      "\n",
      " epoch 869 of 2000\n",
      "5/5 [==============================] - 0s 965us/step\n",
      "11/11 [==============================] - 0s 853us/step\n",
      "(for 1 minibatch) Training loss 0.0144701 | PSNR training 38.2329292\n",
      "\n",
      " epoch 870 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 913us/step\n",
      "(for 1 minibatch) Training loss 0.0147660 | PSNR training 38.0702858\n",
      "\n",
      " epoch 871 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 824us/step\n",
      "(for 1 minibatch) Training loss 0.0140205 | PSNR training 38.5557518\n",
      "\n",
      " epoch 872 of 2000\n",
      "5/5 [==============================] - 0s 984us/step\n",
      "11/11 [==============================] - 0s 973us/step\n",
      "(for 1 minibatch) Training loss 0.0130391 | PSNR training 39.0046349\n",
      "\n",
      " epoch 873 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 853us/step\n",
      "(for 1 minibatch) Training loss 0.0127799 | PSNR training 39.0313301\n",
      "\n",
      " epoch 874 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 824us/step\n",
      "(for 1 minibatch) Training loss 0.0134321 | PSNR training 38.9026794\n",
      "\n",
      " epoch 875 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 820us/step\n",
      "(for 1 minibatch) Training loss 0.0138386 | PSNR training 38.6015091\n",
      "\n",
      " epoch 876 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 827us/step\n",
      "(for 1 minibatch) Training loss 0.0143576 | PSNR training 38.3326569\n",
      "\n",
      " epoch 877 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 847us/step\n",
      "(for 1 minibatch) Training loss 0.0145922 | PSNR training 38.0535698\n",
      "\n",
      " epoch 878 of 2000\n",
      "5/5 [==============================] - 0s 998us/step\n",
      "11/11 [==============================] - 0s 859us/step\n",
      "(for 1 minibatch) Training loss 0.0139865 | PSNR training 38.4150276\n",
      "\n",
      " epoch 879 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 879us/step\n",
      "(for 1 minibatch) Training loss 0.0132940 | PSNR training 38.9547501\n",
      "\n",
      " epoch 880 of 2000\n",
      "5/5 [==============================] - 0s 979us/step\n",
      "11/11 [==============================] - 0s 897us/step\n",
      "(for 1 minibatch) Training loss 0.0132640 | PSNR training 38.8678322\n",
      "\n",
      " epoch 881 of 2000\n",
      "5/5 [==============================] - 0s 946us/step\n",
      "11/11 [==============================] - 0s 905us/step\n",
      "(for 1 minibatch) Training loss 0.0132977 | PSNR training 39.0985756\n",
      "\n",
      " epoch 882 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 850us/step\n",
      "(for 1 minibatch) Training loss 0.0132052 | PSNR training 39.1578484\n",
      "\n",
      " epoch 883 of 2000\n",
      "5/5 [==============================] - 0s 937us/step\n",
      "11/11 [==============================] - 0s 859us/step\n",
      "(for 1 minibatch) Training loss 0.0140165 | PSNR training 38.4466629\n",
      "\n",
      " epoch 884 of 2000\n",
      "5/5 [==============================] - 0s 972us/step\n",
      "11/11 [==============================] - 0s 855us/step\n",
      "(for 1 minibatch) Training loss 0.0139021 | PSNR training 38.6428719\n",
      "\n",
      " epoch 885 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 848us/step\n",
      "(for 1 minibatch) Training loss 0.0131512 | PSNR training 38.8343811\n",
      "\n",
      " epoch 886 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 839us/step\n",
      "(for 1 minibatch) Training loss 0.0129243 | PSNR training 38.8387756\n",
      "\n",
      " epoch 887 of 2000\n",
      "5/5 [==============================] - 0s 977us/step\n",
      "11/11 [==============================] - 0s 847us/step\n",
      "(for 1 minibatch) Training loss 0.0137215 | PSNR training 38.4720306\n",
      "\n",
      " epoch 888 of 2000\n",
      "5/5 [==============================] - 0s 985us/step\n",
      "11/11 [==============================] - 0s 838us/step\n",
      "(for 1 minibatch) Training loss 0.0127968 | PSNR training 39.0393906\n",
      "\n",
      " epoch 889 of 2000\n",
      "5/5 [==============================] - 0s 980us/step\n",
      "11/11 [==============================] - 0s 980us/step\n",
      "(for 1 minibatch) Training loss 0.0126757 | PSNR training 39.3228302\n",
      "\n",
      " epoch 890 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 856us/step\n",
      "(for 1 minibatch) Training loss 0.0125092 | PSNR training 39.2572327\n",
      "\n",
      " epoch 891 of 2000\n",
      "5/5 [==============================] - 0s 931us/step\n",
      "11/11 [==============================] - 0s 817us/step\n",
      "(for 1 minibatch) Training loss 0.0130187 | PSNR training 38.8538208\n",
      "\n",
      " epoch 892 of 2000\n",
      "5/5 [==============================] - 0s 958us/step\n",
      "11/11 [==============================] - 0s 911us/step\n",
      "(for 1 minibatch) Training loss 0.0142249 | PSNR training 38.1750259\n",
      "\n",
      " epoch 893 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 818us/step\n",
      "(for 1 minibatch) Training loss 0.0136651 | PSNR training 38.5581627\n",
      "\n",
      " epoch 894 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 811us/step\n",
      "(for 1 minibatch) Training loss 0.0134096 | PSNR training 38.7471161\n",
      "\n",
      " epoch 895 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 863us/step\n",
      "(for 1 minibatch) Training loss 0.0130438 | PSNR training 38.5823669\n",
      "\n",
      " epoch 896 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 854us/step\n",
      "(for 1 minibatch) Training loss 0.0128775 | PSNR training 39.1107559\n",
      "\n",
      " epoch 897 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 873us/step\n",
      "(for 1 minibatch) Training loss 0.0127863 | PSNR training 38.9719315\n",
      "\n",
      " epoch 898 of 2000\n",
      "5/5 [==============================] - 0s 969us/step\n",
      "11/11 [==============================] - 0s 829us/step\n",
      "(for 1 minibatch) Training loss 0.0133354 | PSNR training 38.9841690\n",
      "\n",
      " epoch 899 of 2000\n",
      "5/5 [==============================] - 0s 999us/step\n",
      "11/11 [==============================] - 0s 871us/step\n",
      "(for 1 minibatch) Training loss 0.0142618 | PSNR training 38.2176514\n",
      "\n",
      " epoch 900 of 2000\n",
      "5/5 [==============================] - 0s 985us/step\n",
      "11/11 [==============================] - 0s 871us/step\n",
      "(for 1 minibatch) Training loss 0.0131097 | PSNR training 38.8112450\n",
      "\n",
      " epoch 901 of 2000\n",
      "5/5 [==============================] - 0s 968us/step\n",
      "11/11 [==============================] - 0s 812us/step\n",
      "(for 1 minibatch) Training loss 0.0128520 | PSNR training 39.1691246\n",
      "\n",
      " epoch 902 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 844us/step\n",
      "(for 1 minibatch) Training loss 0.0123028 | PSNR training 39.3583527\n",
      "\n",
      " epoch 903 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 909us/step\n",
      "(for 1 minibatch) Training loss 0.0125873 | PSNR training 39.3330994\n",
      "\n",
      " epoch 904 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 966us/step\n",
      "(for 1 minibatch) Training loss 0.0124136 | PSNR training 39.3715057\n",
      "\n",
      " epoch 905 of 2000\n",
      "5/5 [==============================] - 0s 986us/step\n",
      "11/11 [==============================] - 0s 836us/step\n",
      "(for 1 minibatch) Training loss 0.0121647 | PSNR training 39.5219650\n",
      "\n",
      " epoch 906 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 848us/step\n",
      "(for 1 minibatch) Training loss 0.0121498 | PSNR training 39.5458260\n",
      "\n",
      " epoch 907 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 835us/step\n",
      "(for 1 minibatch) Training loss 0.0123461 | PSNR training 39.3273773\n",
      "\n",
      " epoch 908 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 833us/step\n",
      "(for 1 minibatch) Training loss 0.0120674 | PSNR training 39.2347298\n",
      "\n",
      " epoch 909 of 2000\n",
      "5/5 [==============================] - 0s 978us/step\n",
      "11/11 [==============================] - 0s 935us/step\n",
      "(for 1 minibatch) Training loss 0.0124605 | PSNR training 38.8945847\n",
      "\n",
      " epoch 910 of 2000\n",
      "5/5 [==============================] - 0s 984us/step\n",
      "11/11 [==============================] - 0s 911us/step\n",
      "(for 1 minibatch) Training loss 0.0124182 | PSNR training 39.1798439\n",
      "\n",
      " epoch 911 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 887us/step\n",
      "(for 1 minibatch) Training loss 0.0121107 | PSNR training 39.4859734\n",
      "\n",
      " epoch 912 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 885us/step\n",
      "(for 1 minibatch) Training loss 0.0123428 | PSNR training 39.2697792\n",
      "\n",
      " epoch 913 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 820us/step\n",
      "(for 1 minibatch) Training loss 0.0121626 | PSNR training 39.4726753\n",
      "\n",
      " epoch 914 of 2000\n",
      "5/5 [==============================] - 0s 978us/step\n",
      "11/11 [==============================] - 0s 796us/step\n",
      "(for 1 minibatch) Training loss 0.0121477 | PSNR training 39.5056953\n",
      "\n",
      " epoch 915 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 808us/step\n",
      "(for 1 minibatch) Training loss 0.0122006 | PSNR training 39.5359039\n",
      "\n",
      " epoch 916 of 2000\n",
      "5/5 [==============================] - 0s 997us/step\n",
      "11/11 [==============================] - 0s 833us/step\n",
      "(for 1 minibatch) Training loss 0.0121344 | PSNR training 39.4021912\n",
      "\n",
      " epoch 917 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 780us/step\n",
      "(for 1 minibatch) Training loss 0.0122146 | PSNR training 39.3780785\n",
      "\n",
      " epoch 918 of 2000\n",
      "5/5 [==============================] - 0s 953us/step\n",
      "11/11 [==============================] - 0s 859us/step\n",
      "(for 1 minibatch) Training loss 0.0122102 | PSNR training 39.2691879\n",
      "\n",
      " epoch 919 of 2000\n",
      "5/5 [==============================] - 0s 975us/step\n",
      "11/11 [==============================] - 0s 847us/step\n",
      "(for 1 minibatch) Training loss 0.0121270 | PSNR training 39.5597649\n",
      "\n",
      " epoch 920 of 2000\n",
      "5/5 [==============================] - 0s 975us/step\n",
      "11/11 [==============================] - 0s 867us/step\n",
      "(for 1 minibatch) Training loss 0.0120720 | PSNR training 39.3743019\n",
      "\n",
      " epoch 921 of 2000\n",
      "5/5 [==============================] - 0s 955us/step\n",
      "11/11 [==============================] - 0s 905us/step\n",
      "(for 1 minibatch) Training loss 0.0118238 | PSNR training 39.4731102\n",
      "\n",
      " epoch 922 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 832us/step\n",
      "(for 1 minibatch) Training loss 0.0123366 | PSNR training 39.4889221\n",
      "\n",
      " epoch 923 of 2000\n",
      "5/5 [==============================] - 0s 914us/step\n",
      "11/11 [==============================] - 0s 878us/step\n",
      "(for 1 minibatch) Training loss 0.0118684 | PSNR training 39.4987450\n",
      "\n",
      " epoch 924 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 834us/step\n",
      "(for 1 minibatch) Training loss 0.0123149 | PSNR training 39.5536804\n",
      "\n",
      " epoch 925 of 2000\n",
      "5/5 [==============================] - 0s 969us/step\n",
      "11/11 [==============================] - 0s 841us/step\n",
      "(for 1 minibatch) Training loss 0.0123925 | PSNR training 39.2285461\n",
      "\n",
      " epoch 926 of 2000\n",
      "5/5 [==============================] - 0s 991us/step\n",
      "11/11 [==============================] - 0s 806us/step\n",
      "(for 1 minibatch) Training loss 0.0123610 | PSNR training 39.2876358\n",
      "\n",
      " epoch 927 of 2000\n",
      "5/5 [==============================] - 0s 976us/step\n",
      "11/11 [==============================] - 0s 975us/step\n",
      "(for 1 minibatch) Training loss 0.0119683 | PSNR training 39.4801064\n",
      "\n",
      " epoch 928 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 848us/step\n",
      "(for 1 minibatch) Training loss 0.0121297 | PSNR training 39.4355431\n",
      "\n",
      " epoch 929 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 828us/step\n",
      "(for 1 minibatch) Training loss 0.0121512 | PSNR training 39.4153061\n",
      "\n",
      " epoch 930 of 2000\n",
      "5/5 [==============================] - 0s 969us/step\n",
      "11/11 [==============================] - 0s 851us/step\n",
      "(for 1 minibatch) Training loss 0.0122512 | PSNR training 39.4370880\n",
      "\n",
      " epoch 931 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 822us/step\n",
      "(for 1 minibatch) Training loss 0.0120924 | PSNR training 39.5360603\n",
      "\n",
      " epoch 932 of 2000\n",
      "5/5 [==============================] - 0s 959us/step\n",
      "11/11 [==============================] - 0s 871us/step\n",
      "(for 1 minibatch) Training loss 0.0121863 | PSNR training 39.5079803\n",
      "\n",
      " epoch 933 of 2000\n",
      "5/5 [==============================] - 0s 941us/step\n",
      "11/11 [==============================] - 0s 827us/step\n",
      "(for 1 minibatch) Training loss 0.0121200 | PSNR training 39.5204773\n",
      "\n",
      " epoch 934 of 2000\n",
      "5/5 [==============================] - 0s 974us/step\n",
      "11/11 [==============================] - 0s 815us/step\n",
      "(for 1 minibatch) Training loss 0.0120676 | PSNR training 39.4821510\n",
      "\n",
      " epoch 935 of 2000\n",
      "5/5 [==============================] - 0s 953us/step\n",
      "11/11 [==============================] - 0s 805us/step\n",
      "(for 1 minibatch) Training loss 0.0122608 | PSNR training 39.3148651\n",
      "\n",
      " epoch 936 of 2000\n",
      "5/5 [==============================] - 0s 989us/step\n",
      "11/11 [==============================] - 0s 804us/step\n",
      "(for 1 minibatch) Training loss 0.0117292 | PSNR training 39.6380081\n",
      "\n",
      " epoch 937 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 833us/step\n",
      "(for 1 minibatch) Training loss 0.0122602 | PSNR training 39.5261345\n",
      "\n",
      " epoch 938 of 2000\n",
      "5/5 [==============================] - 0s 938us/step\n",
      "11/11 [==============================] - 0s 811us/step\n",
      "(for 1 minibatch) Training loss 0.0119554 | PSNR training 39.6086922\n",
      "\n",
      " epoch 939 of 2000\n",
      "5/5 [==============================] - 0s 973us/step\n",
      "11/11 [==============================] - 0s 813us/step\n",
      "(for 1 minibatch) Training loss 0.0122847 | PSNR training 39.4667358\n",
      "\n",
      " epoch 940 of 2000\n",
      "5/5 [==============================] - 0s 939us/step\n",
      "11/11 [==============================] - 0s 831us/step\n",
      "(for 1 minibatch) Training loss 0.0121291 | PSNR training 39.3926239\n",
      "\n",
      " epoch 941 of 2000\n",
      "5/5 [==============================] - 0s 902us/step\n",
      "11/11 [==============================] - 0s 896us/step\n",
      "(for 1 minibatch) Training loss 0.0120408 | PSNR training 39.5178986\n",
      "\n",
      " epoch 942 of 2000\n",
      "5/5 [==============================] - 0s 969us/step\n",
      "11/11 [==============================] - 0s 797us/step\n",
      "(for 1 minibatch) Training loss 0.0121403 | PSNR training 39.3940849\n",
      "\n",
      " epoch 943 of 2000\n",
      "5/5 [==============================] - 0s 927us/step\n",
      "11/11 [==============================] - 0s 831us/step\n",
      "(for 1 minibatch) Training loss 0.0125996 | PSNR training 39.3003464\n",
      "\n",
      " epoch 944 of 2000\n",
      "5/5 [==============================] - 0s 943us/step\n",
      "11/11 [==============================] - 0s 825us/step\n",
      "(for 1 minibatch) Training loss 0.0123854 | PSNR training 39.4815521\n",
      "\n",
      " epoch 945 of 2000\n",
      "5/5 [==============================] - 0s 978us/step\n",
      "11/11 [==============================] - 0s 803us/step\n",
      "(for 1 minibatch) Training loss 0.0120627 | PSNR training 39.6110878\n",
      "\n",
      " epoch 946 of 2000\n",
      "5/5 [==============================] - 0s 910us/step\n",
      "11/11 [==============================] - 0s 815us/step\n",
      "(for 1 minibatch) Training loss 0.0117617 | PSNR training 39.5391808\n",
      "\n",
      " epoch 947 of 2000\n",
      "5/5 [==============================] - 0s 947us/step\n",
      "11/11 [==============================] - 0s 822us/step\n",
      "(for 1 minibatch) Training loss 0.0122006 | PSNR training 39.4325485\n",
      "\n",
      " epoch 948 of 2000\n",
      "5/5 [==============================] - 0s 982us/step\n",
      "11/11 [==============================] - 0s 828us/step\n",
      "(for 1 minibatch) Training loss 0.0119019 | PSNR training 39.7135963\n",
      "\n",
      " epoch 949 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 822us/step\n",
      "(for 1 minibatch) Training loss 0.0122889 | PSNR training 39.1848946\n",
      "\n",
      " epoch 950 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 790us/step\n",
      "(for 1 minibatch) Training loss 0.0117768 | PSNR training 39.6964111\n",
      "\n",
      " epoch 951 of 2000\n",
      "5/5 [==============================] - 0s 936us/step\n",
      "11/11 [==============================] - 0s 820us/step\n",
      "(for 1 minibatch) Training loss 0.0123582 | PSNR training 39.2060661\n",
      "\n",
      " epoch 952 of 2000\n",
      "5/5 [==============================] - 0s 897us/step\n",
      "11/11 [==============================] - 0s 842us/step\n",
      "(for 1 minibatch) Training loss 0.0120314 | PSNR training 39.3518677\n",
      "\n",
      " epoch 953 of 2000\n",
      "5/5 [==============================] - 0s 931us/step\n",
      "11/11 [==============================] - 0s 781us/step\n",
      "(for 1 minibatch) Training loss 0.0121158 | PSNR training 39.4813004\n",
      "\n",
      " epoch 954 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 847us/step\n",
      "(for 1 minibatch) Training loss 0.0123715 | PSNR training 39.5661812\n",
      "\n",
      " epoch 955 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 810us/step\n",
      "(for 1 minibatch) Training loss 0.0121320 | PSNR training 39.3542519\n",
      "\n",
      " epoch 956 of 2000\n",
      "5/5 [==============================] - 0s 984us/step\n",
      "11/11 [==============================] - 0s 833us/step\n",
      "(for 1 minibatch) Training loss 0.0121003 | PSNR training 39.3913994\n",
      "\n",
      " epoch 957 of 2000\n",
      "5/5 [==============================] - 0s 950us/step\n",
      "11/11 [==============================] - 0s 841us/step\n",
      "(for 1 minibatch) Training loss 0.0119221 | PSNR training 39.4354134\n",
      "\n",
      " epoch 958 of 2000\n",
      "5/5 [==============================] - 0s 947us/step\n",
      "11/11 [==============================] - 0s 800us/step\n",
      "(for 1 minibatch) Training loss 0.0121023 | PSNR training 39.3214264\n",
      "\n",
      " epoch 959 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 858us/step\n",
      "(for 1 minibatch) Training loss 0.0117532 | PSNR training 39.6296310\n",
      "\n",
      " epoch 960 of 2000\n",
      "5/5 [==============================] - 0s 963us/step\n",
      "11/11 [==============================] - 0s 794us/step\n",
      "(for 1 minibatch) Training loss 0.0119985 | PSNR training 39.8502197\n",
      "\n",
      " epoch 961 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 847us/step\n",
      "(for 1 minibatch) Training loss 0.0116126 | PSNR training 39.5736084\n",
      "\n",
      " epoch 962 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 904us/step\n",
      "(for 1 minibatch) Training loss 0.0116784 | PSNR training 39.8526306\n",
      "\n",
      " epoch 963 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 809us/step\n",
      "(for 1 minibatch) Training loss 0.0121664 | PSNR training 39.3840218\n",
      "\n",
      " epoch 964 of 2000\n",
      "5/5 [==============================] - 0s 994us/step\n",
      "11/11 [==============================] - 0s 808us/step\n",
      "(for 1 minibatch) Training loss 0.0117568 | PSNR training 39.6582184\n",
      "\n",
      " epoch 965 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 779us/step\n",
      "(for 1 minibatch) Training loss 0.0117193 | PSNR training 39.6457939\n",
      "\n",
      " epoch 966 of 2000\n",
      "5/5 [==============================] - 0s 942us/step\n",
      "11/11 [==============================] - 0s 791us/step\n",
      "(for 1 minibatch) Training loss 0.0119681 | PSNR training 39.7036972\n",
      "\n",
      " epoch 967 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 862us/step\n",
      "(for 1 minibatch) Training loss 0.0118697 | PSNR training 39.4748001\n",
      "\n",
      " epoch 968 of 2000\n",
      "5/5 [==============================] - 0s 984us/step\n",
      "11/11 [==============================] - 0s 833us/step\n",
      "(for 1 minibatch) Training loss 0.0121167 | PSNR training 39.7529831\n",
      "\n",
      " epoch 969 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 798us/step\n",
      "(for 1 minibatch) Training loss 0.0118654 | PSNR training 39.5848923\n",
      "\n",
      " epoch 970 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 784us/step\n",
      "(for 1 minibatch) Training loss 0.0121767 | PSNR training 39.7250481\n",
      "\n",
      " epoch 971 of 2000\n",
      "5/5 [==============================] - 0s 996us/step\n",
      "11/11 [==============================] - 0s 794us/step\n",
      "(for 1 minibatch) Training loss 0.0120117 | PSNR training 39.6198158\n",
      "\n",
      " epoch 972 of 2000\n",
      "5/5 [==============================] - 0s 959us/step\n",
      "11/11 [==============================] - 0s 816us/step\n",
      "(for 1 minibatch) Training loss 0.0117247 | PSNR training 39.8600464\n",
      "\n",
      " epoch 973 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 835us/step\n",
      "(for 1 minibatch) Training loss 0.0121759 | PSNR training 39.3723183\n",
      "\n",
      " epoch 974 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 786us/step\n",
      "(for 1 minibatch) Training loss 0.0119715 | PSNR training 39.5859528\n",
      "\n",
      " epoch 975 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 829us/step\n",
      "(for 1 minibatch) Training loss 0.0117571 | PSNR training 39.6640434\n",
      "\n",
      " epoch 976 of 2000\n",
      "5/5 [==============================] - 0s 922us/step\n",
      "11/11 [==============================] - 0s 808us/step\n",
      "(for 1 minibatch) Training loss 0.0115820 | PSNR training 39.6439285\n",
      "\n",
      " epoch 977 of 2000\n",
      "5/5 [==============================] - 0s 928us/step\n",
      "11/11 [==============================] - 0s 824us/step\n",
      "(for 1 minibatch) Training loss 0.0117420 | PSNR training 39.7962646\n",
      "\n",
      " epoch 978 of 2000\n",
      "5/5 [==============================] - 0s 972us/step\n",
      "11/11 [==============================] - 0s 822us/step\n",
      "(for 1 minibatch) Training loss 0.0122486 | PSNR training 39.5013847\n",
      "\n",
      " epoch 979 of 2000\n",
      "5/5 [==============================] - 0s 995us/step\n",
      "11/11 [==============================] - 0s 837us/step\n",
      "(for 1 minibatch) Training loss 0.0118066 | PSNR training 39.6968765\n",
      "\n",
      " epoch 980 of 2000\n",
      "5/5 [==============================] - 0s 980us/step\n",
      "11/11 [==============================] - 0s 800us/step\n",
      "(for 1 minibatch) Training loss 0.0120984 | PSNR training 39.5783119\n",
      "\n",
      " epoch 981 of 2000\n",
      "5/5 [==============================] - 0s 972us/step\n",
      "11/11 [==============================] - 0s 791us/step\n",
      "(for 1 minibatch) Training loss 0.0119470 | PSNR training 39.6034584\n",
      "\n",
      " epoch 982 of 2000\n",
      "5/5 [==============================] - 0s 931us/step\n",
      "11/11 [==============================] - 0s 824us/step\n",
      "(for 1 minibatch) Training loss 0.0121256 | PSNR training 39.4426041\n",
      "\n",
      " epoch 983 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 870us/step\n",
      "(for 1 minibatch) Training loss 0.0120038 | PSNR training 39.6438065\n",
      "\n",
      " epoch 984 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 810us/step\n",
      "(for 1 minibatch) Training loss 0.0118586 | PSNR training 39.4766769\n",
      "\n",
      " epoch 985 of 2000\n",
      "5/5 [==============================] - 0s 999us/step\n",
      "11/11 [==============================] - 0s 795us/step\n",
      "(for 1 minibatch) Training loss 0.0121129 | PSNR training 39.5618057\n",
      "\n",
      " epoch 986 of 2000\n",
      "5/5 [==============================] - 0s 932us/step\n",
      "11/11 [==============================] - 0s 819us/step\n",
      "(for 1 minibatch) Training loss 0.0117127 | PSNR training 39.6306534\n",
      "\n",
      " epoch 987 of 2000\n",
      "5/5 [==============================] - 0s 965us/step\n",
      "11/11 [==============================] - 0s 803us/step\n",
      "(for 1 minibatch) Training loss 0.0116302 | PSNR training 39.6654358\n",
      "\n",
      " epoch 988 of 2000\n",
      "5/5 [==============================] - 0s 953us/step\n",
      "11/11 [==============================] - 0s 800us/step\n",
      "(for 1 minibatch) Training loss 0.0115877 | PSNR training 39.6542587\n",
      "\n",
      " epoch 989 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 876us/step\n",
      "(for 1 minibatch) Training loss 0.0118743 | PSNR training 39.7086296\n",
      "\n",
      " epoch 990 of 2000\n",
      "5/5 [==============================] - 0s 960us/step\n",
      "11/11 [==============================] - 0s 829us/step\n",
      "(for 1 minibatch) Training loss 0.0117028 | PSNR training 39.6899567\n",
      "\n",
      " epoch 991 of 2000\n",
      "5/5 [==============================] - 0s 937us/step\n",
      "11/11 [==============================] - 0s 817us/step\n",
      "(for 1 minibatch) Training loss 0.0121985 | PSNR training 39.5039368\n",
      "\n",
      " epoch 992 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 831us/step\n",
      "(for 1 minibatch) Training loss 0.0116809 | PSNR training 39.6982117\n",
      "\n",
      " epoch 993 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 825us/step\n",
      "(for 1 minibatch) Training loss 0.0116202 | PSNR training 39.8199234\n",
      "\n",
      " epoch 994 of 2000\n",
      "5/5 [==============================] - 0s 992us/step\n",
      "11/11 [==============================] - 0s 803us/step\n",
      "(for 1 minibatch) Training loss 0.0120245 | PSNR training 39.5456505\n",
      "\n",
      " epoch 995 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 789us/step\n",
      "(for 1 minibatch) Training loss 0.0120982 | PSNR training 39.4876633\n",
      "\n",
      " epoch 996 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 816us/step\n",
      "(for 1 minibatch) Training loss 0.0122812 | PSNR training 39.5406189\n",
      "\n",
      " epoch 997 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 817us/step\n",
      "(for 1 minibatch) Training loss 0.0116053 | PSNR training 39.8055992\n",
      "\n",
      " epoch 998 of 2000\n",
      "5/5 [==============================] - 0s 954us/step\n",
      "11/11 [==============================] - 0s 826us/step\n",
      "(for 1 minibatch) Training loss 0.0114154 | PSNR training 39.7268562\n",
      "\n",
      " epoch 999 of 2000\n",
      "5/5 [==============================] - 0s 959us/step\n",
      "11/11 [==============================] - 0s 817us/step\n",
      "(for 1 minibatch) Training loss 0.0120605 | PSNR training 39.6940384\n",
      "\n",
      " epoch 1000 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 807us/step\n",
      "(for 1 minibatch) Training loss 0.0120922 | PSNR training 39.5483475\n",
      "\n",
      " epoch 1001 of 2000\n",
      "5/5 [==============================] - 0s 922us/step\n",
      "11/11 [==============================] - 0s 827us/step\n",
      "(for 1 minibatch) Training loss 0.0115337 | PSNR training 39.5183105\n",
      "\n",
      " epoch 1002 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 827us/step\n",
      "(for 1 minibatch) Training loss 0.0118631 | PSNR training 39.6616440\n",
      "\n",
      " epoch 1003 of 2000\n",
      "5/5 [==============================] - 0s 956us/step\n",
      "11/11 [==============================] - 0s 824us/step\n",
      "(for 1 minibatch) Training loss 0.0117498 | PSNR training 39.7550812\n",
      "\n",
      " epoch 1004 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 821us/step\n",
      "(for 1 minibatch) Training loss 0.0120036 | PSNR training 39.5931664\n",
      "\n",
      " epoch 1005 of 2000\n",
      "5/5 [==============================] - 0s 948us/step\n",
      "11/11 [==============================] - 0s 827us/step\n",
      "(for 1 minibatch) Training loss 0.0117853 | PSNR training 39.8255501\n",
      "\n",
      " epoch 1006 of 2000\n",
      "5/5 [==============================] - 0s 982us/step\n",
      "11/11 [==============================] - 0s 770us/step\n",
      "(for 1 minibatch) Training loss 0.0120109 | PSNR training 39.7016792\n",
      "\n",
      " epoch 1007 of 2000\n",
      "5/5 [==============================] - 0s 942us/step\n",
      "11/11 [==============================] - 0s 965us/step\n",
      "(for 1 minibatch) Training loss 0.0118408 | PSNR training 39.6273346\n",
      "\n",
      " epoch 1008 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 912us/step\n",
      "(for 1 minibatch) Training loss 0.0121331 | PSNR training 39.6787872\n",
      "\n",
      " epoch 1009 of 2000\n",
      "5/5 [==============================] - 0s 979us/step\n",
      "11/11 [==============================] - 0s 871us/step\n",
      "(for 1 minibatch) Training loss 0.0115885 | PSNR training 39.6697121\n",
      "\n",
      " epoch 1010 of 2000\n",
      "5/5 [==============================] - 0s 951us/step\n",
      "11/11 [==============================] - 0s 850us/step\n",
      "(for 1 minibatch) Training loss 0.0121121 | PSNR training 39.6729202\n",
      "\n",
      " epoch 1011 of 2000\n",
      "5/5 [==============================] - 0s 929us/step\n",
      "11/11 [==============================] - 0s 857us/step\n",
      "(for 1 minibatch) Training loss 0.0119021 | PSNR training 39.6276436\n",
      "\n",
      " epoch 1012 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 833us/step\n",
      "(for 1 minibatch) Training loss 0.0121766 | PSNR training 39.5857162\n",
      "\n",
      " epoch 1013 of 2000\n",
      "5/5 [==============================] - 0s 948us/step\n",
      "11/11 [==============================] - 0s 787us/step\n",
      "(for 1 minibatch) Training loss 0.0118068 | PSNR training 39.8476639\n",
      "\n",
      " epoch 1014 of 2000\n",
      "5/5 [==============================] - 0s 977us/step\n",
      "11/11 [==============================] - 0s 856us/step\n",
      "(for 1 minibatch) Training loss 0.0115068 | PSNR training 39.7934380\n",
      "\n",
      " epoch 1015 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 867us/step\n",
      "(for 1 minibatch) Training loss 0.0119119 | PSNR training 39.6186104\n",
      "\n",
      " epoch 1016 of 2000\n",
      "5/5 [==============================] - 0s 941us/step\n",
      "11/11 [==============================] - 0s 839us/step\n",
      "(for 1 minibatch) Training loss 0.0118694 | PSNR training 39.6596985\n",
      "\n",
      " epoch 1017 of 2000\n",
      "5/5 [==============================] - 0s 908us/step\n",
      "11/11 [==============================] - 0s 856us/step\n",
      "(for 1 minibatch) Training loss 0.0116491 | PSNR training 39.8295135\n",
      "\n",
      " epoch 1018 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 842us/step\n",
      "(for 1 minibatch) Training loss 0.0118327 | PSNR training 39.8393784\n",
      "\n",
      " epoch 1019 of 2000\n",
      "5/5 [==============================] - 0s 943us/step\n",
      "11/11 [==============================] - 0s 798us/step\n",
      "(for 1 minibatch) Training loss 0.0119396 | PSNR training 39.6793060\n",
      "\n",
      " epoch 1020 of 2000\n",
      "5/5 [==============================] - 0s 945us/step\n",
      "11/11 [==============================] - 0s 983us/step\n",
      "(for 1 minibatch) Training loss 0.0119600 | PSNR training 39.4417953\n",
      "\n",
      " epoch 1021 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 880us/step\n",
      "(for 1 minibatch) Training loss 0.0118013 | PSNR training 39.5889015\n",
      "\n",
      " epoch 1022 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 934us/step\n",
      "(for 1 minibatch) Training loss 0.0118905 | PSNR training 39.7266808\n",
      "\n",
      " epoch 1023 of 2000\n",
      "5/5 [==============================] - 0s 951us/step\n",
      "11/11 [==============================] - 0s 892us/step\n",
      "(for 1 minibatch) Training loss 0.0118145 | PSNR training 39.7483406\n",
      "\n",
      " epoch 1024 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 816us/step\n",
      "(for 1 minibatch) Training loss 0.0117356 | PSNR training 39.6635361\n",
      "\n",
      " epoch 1025 of 2000\n",
      "5/5 [==============================] - 0s 932us/step\n",
      "11/11 [==============================] - 0s 871us/step\n",
      "(for 1 minibatch) Training loss 0.0120404 | PSNR training 39.4843330\n",
      "\n",
      " epoch 1026 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 840us/step\n",
      "(for 1 minibatch) Training loss 0.0121613 | PSNR training 39.7849960\n",
      "\n",
      " epoch 1027 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 808us/step\n",
      "(for 1 minibatch) Training loss 0.0118167 | PSNR training 39.7322044\n",
      "\n",
      " epoch 1028 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 802us/step\n",
      "(for 1 minibatch) Training loss 0.0118021 | PSNR training 39.8965645\n",
      "\n",
      " epoch 1029 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 885us/step\n",
      "(for 1 minibatch) Training loss 0.0120773 | PSNR training 39.6439133\n",
      "\n",
      " epoch 1030 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 872us/step\n",
      "(for 1 minibatch) Training loss 0.0119593 | PSNR training 39.6897011\n",
      "\n",
      " epoch 1031 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 899us/step\n",
      "(for 1 minibatch) Training loss 0.0119077 | PSNR training 39.6873703\n",
      "\n",
      " epoch 1032 of 2000\n",
      "5/5 [==============================] - 0s 980us/step\n",
      "11/11 [==============================] - 0s 805us/step\n",
      "(for 1 minibatch) Training loss 0.0116518 | PSNR training 39.8392067\n",
      "\n",
      " epoch 1033 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 834us/step\n",
      "(for 1 minibatch) Training loss 0.0118276 | PSNR training 39.7438660\n",
      "\n",
      " epoch 1034 of 2000\n",
      "5/5 [==============================] - 0s 976us/step\n",
      "11/11 [==============================] - 0s 823us/step\n",
      "(for 1 minibatch) Training loss 0.0119663 | PSNR training 39.6378174\n",
      "\n",
      " epoch 1035 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 847us/step\n",
      "(for 1 minibatch) Training loss 0.0118479 | PSNR training 39.7436676\n",
      "\n",
      " epoch 1036 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 835us/step\n",
      "(for 1 minibatch) Training loss 0.0114371 | PSNR training 39.8865318\n",
      "\n",
      " epoch 1037 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 808us/step\n",
      "(for 1 minibatch) Training loss 0.0114974 | PSNR training 39.8522682\n",
      "\n",
      " epoch 1038 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 935us/step\n",
      "(for 1 minibatch) Training loss 0.0118363 | PSNR training 39.8133736\n",
      "\n",
      " epoch 1039 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 901us/step\n",
      "(for 1 minibatch) Training loss 0.0121751 | PSNR training 39.5788879\n",
      "\n",
      " epoch 1040 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 839us/step\n",
      "(for 1 minibatch) Training loss 0.0117180 | PSNR training 39.6805496\n",
      "\n",
      " epoch 1041 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 820us/step\n",
      "(for 1 minibatch) Training loss 0.0117682 | PSNR training 39.7234344\n",
      "\n",
      " epoch 1042 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 861us/step\n",
      "(for 1 minibatch) Training loss 0.0118623 | PSNR training 39.7643890\n",
      "\n",
      " epoch 1043 of 2000\n",
      "5/5 [==============================] - 0s 915us/step\n",
      "11/11 [==============================] - 0s 843us/step\n",
      "(for 1 minibatch) Training loss 0.0114970 | PSNR training 39.9421234\n",
      "\n",
      " epoch 1044 of 2000\n",
      "5/5 [==============================] - 0s 922us/step\n",
      "11/11 [==============================] - 0s 962us/step\n",
      "(for 1 minibatch) Training loss 0.0119254 | PSNR training 39.6622276\n",
      "\n",
      " epoch 1045 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 858us/step\n",
      "(for 1 minibatch) Training loss 0.0118304 | PSNR training 39.8406448\n",
      "\n",
      " epoch 1046 of 2000\n",
      "5/5 [==============================] - 0s 940us/step\n",
      "11/11 [==============================] - 0s 924us/step\n",
      "(for 1 minibatch) Training loss 0.0118799 | PSNR training 39.6146812\n",
      "\n",
      " epoch 1047 of 2000\n",
      "5/5 [==============================] - 0s 999us/step\n",
      "11/11 [==============================] - 0s 867us/step\n",
      "(for 1 minibatch) Training loss 0.0118240 | PSNR training 39.5427132\n",
      "\n",
      " epoch 1048 of 2000\n",
      "5/5 [==============================] - 0s 940us/step\n",
      "11/11 [==============================] - 0s 822us/step\n",
      "(for 1 minibatch) Training loss 0.0119068 | PSNR training 39.3891411\n",
      "\n",
      " epoch 1049 of 2000\n",
      "5/5 [==============================] - 0s 989us/step\n",
      "11/11 [==============================] - 0s 836us/step\n",
      "(for 1 minibatch) Training loss 0.0117210 | PSNR training 39.7794838\n",
      "\n",
      " epoch 1050 of 2000\n",
      "5/5 [==============================] - 0s 957us/step\n",
      "11/11 [==============================] - 0s 971us/step\n",
      "(for 1 minibatch) Training loss 0.0119087 | PSNR training 39.4752808\n",
      "\n",
      " epoch 1051 of 2000\n",
      "5/5 [==============================] - 0s 968us/step\n",
      "11/11 [==============================] - 0s 817us/step\n",
      "(for 1 minibatch) Training loss 0.0119837 | PSNR training 39.7214508\n",
      "\n",
      " epoch 1052 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 895us/step\n",
      "(for 1 minibatch) Training loss 0.0121143 | PSNR training 39.7058640\n",
      "\n",
      " epoch 1053 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 827us/step\n",
      "(for 1 minibatch) Training loss 0.0118248 | PSNR training 39.6077271\n",
      "\n",
      " epoch 1054 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 801us/step\n",
      "(for 1 minibatch) Training loss 0.0116932 | PSNR training 39.6434631\n",
      "\n",
      " epoch 1055 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 898us/step\n",
      "(for 1 minibatch) Training loss 0.0117338 | PSNR training 39.7845993\n",
      "\n",
      " epoch 1056 of 2000\n",
      "5/5 [==============================] - 0s 942us/step\n",
      "11/11 [==============================] - 0s 848us/step\n",
      "(for 1 minibatch) Training loss 0.0120649 | PSNR training 39.5189209\n",
      "\n",
      " epoch 1057 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 800us/step\n",
      "(for 1 minibatch) Training loss 0.0118292 | PSNR training 39.4928894\n",
      "\n",
      " epoch 1058 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 838us/step\n",
      "(for 1 minibatch) Training loss 0.0121687 | PSNR training 39.5176926\n",
      "\n",
      " epoch 1059 of 2000\n",
      "5/5 [==============================] - 0s 959us/step\n",
      "11/11 [==============================] - 0s 836us/step\n",
      "(for 1 minibatch) Training loss 0.0117451 | PSNR training 39.6755104\n",
      "\n",
      " epoch 1060 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 885us/step\n",
      "(for 1 minibatch) Training loss 0.0117245 | PSNR training 39.8767815\n",
      "\n",
      " epoch 1061 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 835us/step\n",
      "(for 1 minibatch) Training loss 0.0115845 | PSNR training 39.8779182\n",
      "\n",
      " epoch 1062 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 852us/step\n",
      "(for 1 minibatch) Training loss 0.0114291 | PSNR training 39.9590645\n",
      "\n",
      " epoch 1063 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 826us/step\n",
      "(for 1 minibatch) Training loss 0.0116261 | PSNR training 39.7959328\n",
      "\n",
      " epoch 1064 of 2000\n",
      "5/5 [==============================] - 0s 997us/step\n",
      "11/11 [==============================] - 0s 825us/step\n",
      "(for 1 minibatch) Training loss 0.0114993 | PSNR training 39.8999825\n",
      "\n",
      " epoch 1065 of 2000\n",
      "5/5 [==============================] - 0s 928us/step\n",
      "11/11 [==============================] - 0s 919us/step\n",
      "(for 1 minibatch) Training loss 0.0114578 | PSNR training 39.8642197\n",
      "\n",
      " epoch 1066 of 2000\n",
      "5/5 [==============================] - 0s 968us/step\n",
      "11/11 [==============================] - 0s 889us/step\n",
      "(for 1 minibatch) Training loss 0.0115213 | PSNR training 39.8437576\n",
      "\n",
      " epoch 1067 of 2000\n",
      "5/5 [==============================] - 0s 980us/step\n",
      "11/11 [==============================] - 0s 833us/step\n",
      "(for 1 minibatch) Training loss 0.0117970 | PSNR training 39.6680908\n",
      "\n",
      " epoch 1068 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 854us/step\n",
      "(for 1 minibatch) Training loss 0.0115861 | PSNR training 39.8246460\n",
      "\n",
      " epoch 1069 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 810us/step\n",
      "(for 1 minibatch) Training loss 0.0115187 | PSNR training 39.9141388\n",
      "\n",
      " epoch 1070 of 2000\n",
      "5/5 [==============================] - 0s 927us/step\n",
      "11/11 [==============================] - 0s 834us/step\n",
      "(for 1 minibatch) Training loss 0.0119663 | PSNR training 39.6856575\n",
      "\n",
      " epoch 1071 of 2000\n",
      "5/5 [==============================] - 0s 993us/step\n",
      "11/11 [==============================] - 0s 849us/step\n",
      "(for 1 minibatch) Training loss 0.0116029 | PSNR training 39.7338104\n",
      "\n",
      " epoch 1072 of 2000\n",
      "5/5 [==============================] - 0s 995us/step\n",
      "11/11 [==============================] - 0s 858us/step\n",
      "(for 1 minibatch) Training loss 0.0116746 | PSNR training 39.6566620\n",
      "\n",
      " epoch 1073 of 2000\n",
      "5/5 [==============================] - 0s 925us/step\n",
      "11/11 [==============================] - 0s 929us/step\n",
      "(for 1 minibatch) Training loss 0.0115692 | PSNR training 40.0886765\n",
      "\n",
      " epoch 1074 of 2000\n",
      "5/5 [==============================] - 0s 995us/step\n",
      "11/11 [==============================] - 0s 821us/step\n",
      "(for 1 minibatch) Training loss 0.0117589 | PSNR training 39.8706779\n",
      "\n",
      " epoch 1075 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 929us/step\n",
      "(for 1 minibatch) Training loss 0.0117378 | PSNR training 39.5674057\n",
      "\n",
      " epoch 1076 of 2000\n",
      "5/5 [==============================] - 0s 876us/step\n",
      "11/11 [==============================] - 0s 870us/step\n",
      "(for 1 minibatch) Training loss 0.0119259 | PSNR training 39.6987991\n",
      "\n",
      " epoch 1077 of 2000\n",
      "5/5 [==============================] - 0s 939us/step\n",
      "11/11 [==============================] - 0s 832us/step\n",
      "(for 1 minibatch) Training loss 0.0117623 | PSNR training 39.8516960\n",
      "\n",
      " epoch 1078 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 844us/step\n",
      "(for 1 minibatch) Training loss 0.0117419 | PSNR training 39.7838745\n",
      "\n",
      " epoch 1079 of 2000\n",
      "5/5 [==============================] - 0s 989us/step\n",
      "11/11 [==============================] - 0s 823us/step\n",
      "(for 1 minibatch) Training loss 0.0117589 | PSNR training 39.6576805\n",
      "\n",
      " epoch 1080 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 857us/step\n",
      "(for 1 minibatch) Training loss 0.0122166 | PSNR training 39.8407631\n",
      "\n",
      " epoch 1081 of 2000\n",
      "5/5 [==============================] - 0s 945us/step\n",
      "11/11 [==============================] - 0s 925us/step\n",
      "(for 1 minibatch) Training loss 0.0117868 | PSNR training 39.5774574\n",
      "\n",
      " epoch 1082 of 2000\n",
      "5/5 [==============================] - 0s 984us/step\n",
      "11/11 [==============================] - 0s 879us/step\n",
      "(for 1 minibatch) Training loss 0.0119081 | PSNR training 39.9016151\n",
      "\n",
      " epoch 1083 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 860us/step\n",
      "(for 1 minibatch) Training loss 0.0115012 | PSNR training 39.8497849\n",
      "\n",
      " epoch 1084 of 2000\n",
      "5/5 [==============================] - 0s 951us/step\n",
      "11/11 [==============================] - 0s 839us/step\n",
      "(for 1 minibatch) Training loss 0.0115215 | PSNR training 39.8522873\n",
      "\n",
      " epoch 1085 of 2000\n",
      "5/5 [==============================] - 0s 966us/step\n",
      "11/11 [==============================] - 0s 922us/step\n",
      "(for 1 minibatch) Training loss 0.0114036 | PSNR training 39.9956970\n",
      "\n",
      " epoch 1086 of 2000\n",
      "5/5 [==============================] - 0s 962us/step\n",
      "11/11 [==============================] - 0s 817us/step\n",
      "(for 1 minibatch) Training loss 0.0117508 | PSNR training 39.7165833\n",
      "\n",
      " epoch 1087 of 2000\n",
      "5/5 [==============================] - 0s 981us/step\n",
      "11/11 [==============================] - 0s 872us/step\n",
      "(for 1 minibatch) Training loss 0.0113288 | PSNR training 40.0112381\n",
      "\n",
      " epoch 1088 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 1ms/step\n",
      "(for 1 minibatch) Training loss 0.0114115 | PSNR training 39.8054047\n",
      "\n",
      " epoch 1089 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 837us/step\n",
      "(for 1 minibatch) Training loss 0.0115151 | PSNR training 39.8896751\n",
      "\n",
      " epoch 1090 of 2000\n",
      "5/5 [==============================] - 0s 960us/step\n",
      "11/11 [==============================] - 0s 812us/step\n",
      "(for 1 minibatch) Training loss 0.0115694 | PSNR training 39.7783089\n",
      "\n",
      " epoch 1091 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 866us/step\n",
      "(for 1 minibatch) Training loss 0.0119014 | PSNR training 39.7886238\n",
      "\n",
      " epoch 1092 of 2000\n",
      "5/5 [==============================] - 0s 904us/step\n",
      "11/11 [==============================] - 0s 820us/step\n",
      "(for 1 minibatch) Training loss 0.0113790 | PSNR training 39.9259911\n",
      "\n",
      " epoch 1093 of 2000\n",
      "5/5 [==============================] - 0s 998us/step\n",
      "11/11 [==============================] - 0s 877us/step\n",
      "(for 1 minibatch) Training loss 0.0114402 | PSNR training 39.9228630\n",
      "\n",
      " epoch 1094 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 874us/step\n",
      "(for 1 minibatch) Training loss 0.0115330 | PSNR training 39.9077263\n",
      "\n",
      " epoch 1095 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 886us/step\n",
      "(for 1 minibatch) Training loss 0.0115825 | PSNR training 39.9107704\n",
      "\n",
      " epoch 1096 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 907us/step\n",
      "(for 1 minibatch) Training loss 0.0115467 | PSNR training 39.7962265\n",
      "\n",
      " epoch 1097 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 942us/step\n",
      "(for 1 minibatch) Training loss 0.0117629 | PSNR training 39.8067436\n",
      "\n",
      " epoch 1098 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 845us/step\n",
      "(for 1 minibatch) Training loss 0.0116376 | PSNR training 39.9899902\n",
      "\n",
      " epoch 1099 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 874us/step\n",
      "(for 1 minibatch) Training loss 0.0114363 | PSNR training 40.0758286\n",
      "\n",
      " epoch 1100 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 828us/step\n",
      "(for 1 minibatch) Training loss 0.0113661 | PSNR training 40.0451965\n",
      "\n",
      " epoch 1101 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 836us/step\n",
      "(for 1 minibatch) Training loss 0.0116504 | PSNR training 40.0004387\n",
      "\n",
      " epoch 1102 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 833us/step\n",
      "(for 1 minibatch) Training loss 0.0118588 | PSNR training 39.8873024\n",
      "\n",
      " epoch 1103 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 854us/step\n",
      "(for 1 minibatch) Training loss 0.0113785 | PSNR training 39.8773422\n",
      "\n",
      " epoch 1104 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 915us/step\n",
      "(for 1 minibatch) Training loss 0.0116686 | PSNR training 39.7051010\n",
      "\n",
      " epoch 1105 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 941us/step\n",
      "(for 1 minibatch) Training loss 0.0116272 | PSNR training 39.9068413\n",
      "\n",
      " epoch 1106 of 2000\n",
      "5/5 [==============================] - 0s 945us/step\n",
      "11/11 [==============================] - 0s 870us/step\n",
      "(for 1 minibatch) Training loss 0.0114801 | PSNR training 40.0449677\n",
      "\n",
      " epoch 1107 of 2000\n",
      "5/5 [==============================] - 0s 928us/step\n",
      "11/11 [==============================] - 0s 835us/step\n",
      "(for 1 minibatch) Training loss 0.0118210 | PSNR training 39.6587219\n",
      "\n",
      " epoch 1108 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 850us/step\n",
      "(for 1 minibatch) Training loss 0.0113513 | PSNR training 40.0096054\n",
      "\n",
      " epoch 1109 of 2000\n",
      "5/5 [==============================] - 0s 902us/step\n",
      "11/11 [==============================] - 0s 870us/step\n",
      "(for 1 minibatch) Training loss 0.0115392 | PSNR training 39.8663750\n",
      "\n",
      " epoch 1110 of 2000\n",
      "5/5 [==============================] - 0s 936us/step\n",
      "11/11 [==============================] - 0s 870us/step\n",
      "(for 1 minibatch) Training loss 0.0117109 | PSNR training 39.6653633\n",
      "\n",
      " epoch 1111 of 2000\n",
      "5/5 [==============================] - 0s 928us/step\n",
      "11/11 [==============================] - 0s 872us/step\n",
      "(for 1 minibatch) Training loss 0.0116530 | PSNR training 39.8051720\n",
      "\n",
      " epoch 1112 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 1ms/step\n",
      "(for 1 minibatch) Training loss 0.0115598 | PSNR training 40.0811462\n",
      "\n",
      " epoch 1113 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 877us/step\n",
      "(for 1 minibatch) Training loss 0.0114172 | PSNR training 40.0251350\n",
      "\n",
      " epoch 1114 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 895us/step\n",
      "(for 1 minibatch) Training loss 0.0114746 | PSNR training 40.0489082\n",
      "\n",
      " epoch 1115 of 2000\n",
      "5/5 [==============================] - 0s 998us/step\n",
      "11/11 [==============================] - 0s 863us/step\n",
      "(for 1 minibatch) Training loss 0.0119193 | PSNR training 39.7572441\n",
      "\n",
      " epoch 1116 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 985us/step\n",
      "(for 1 minibatch) Training loss 0.0114400 | PSNR training 39.9567490\n",
      "\n",
      " epoch 1117 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 1ms/step\n",
      "(for 1 minibatch) Training loss 0.0114592 | PSNR training 39.9882698\n",
      "\n",
      " epoch 1118 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 1ms/step\n",
      "(for 1 minibatch) Training loss 0.0115291 | PSNR training 40.0682411\n",
      "\n",
      " epoch 1119 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 890us/step\n",
      "(for 1 minibatch) Training loss 0.0113322 | PSNR training 40.2187767\n",
      "\n",
      " epoch 1120 of 2000\n",
      "5/5 [==============================] - 0s 975us/step\n",
      "11/11 [==============================] - 0s 848us/step\n",
      "(for 1 minibatch) Training loss 0.0114567 | PSNR training 39.9610176\n",
      "\n",
      " epoch 1121 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 861us/step\n",
      "(for 1 minibatch) Training loss 0.0115441 | PSNR training 39.8222504\n",
      "\n",
      " epoch 1122 of 2000\n",
      "5/5 [==============================] - 0s 995us/step\n",
      "11/11 [==============================] - 0s 847us/step\n",
      "(for 1 minibatch) Training loss 0.0115547 | PSNR training 39.8064499\n",
      "\n",
      " epoch 1123 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 851us/step\n",
      "(for 1 minibatch) Training loss 0.0112789 | PSNR training 39.8792648\n",
      "\n",
      " epoch 1124 of 2000\n",
      "5/5 [==============================] - 0s 934us/step\n",
      "11/11 [==============================] - 0s 853us/step\n",
      "(for 1 minibatch) Training loss 0.0112656 | PSNR training 39.7841606\n",
      "\n",
      " epoch 1125 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 824us/step\n",
      "(for 1 minibatch) Training loss 0.0114737 | PSNR training 39.9943886\n",
      "\n",
      " epoch 1126 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 841us/step\n",
      "(for 1 minibatch) Training loss 0.0114387 | PSNR training 39.8858414\n",
      "\n",
      " epoch 1127 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 827us/step\n",
      "(for 1 minibatch) Training loss 0.0113150 | PSNR training 40.0949287\n",
      "\n",
      " epoch 1128 of 2000\n",
      "5/5 [==============================] - 0s 936us/step\n",
      "11/11 [==============================] - 0s 827us/step\n",
      "(for 1 minibatch) Training loss 0.0112427 | PSNR training 40.0445366\n",
      "\n",
      " epoch 1129 of 2000\n",
      "5/5 [==============================] - 0s 953us/step\n",
      "11/11 [==============================] - 0s 835us/step\n",
      "(for 1 minibatch) Training loss 0.0116557 | PSNR training 39.9362526\n",
      "\n",
      " epoch 1130 of 2000\n",
      "5/5 [==============================] - 0s 975us/step\n",
      "11/11 [==============================] - 0s 900us/step\n",
      "(for 1 minibatch) Training loss 0.0113009 | PSNR training 40.0436440\n",
      "\n",
      " epoch 1131 of 2000\n",
      "5/5 [==============================] - 0s 969us/step\n",
      "11/11 [==============================] - 0s 839us/step\n",
      "(for 1 minibatch) Training loss 0.0117495 | PSNR training 39.8824959\n",
      "\n",
      " epoch 1132 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 870us/step\n",
      "(for 1 minibatch) Training loss 0.0119717 | PSNR training 39.7807655\n",
      "\n",
      " epoch 1133 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 829us/step\n",
      "(for 1 minibatch) Training loss 0.0115880 | PSNR training 40.0312996\n",
      "\n",
      " epoch 1134 of 2000\n",
      "5/5 [==============================] - 0s 971us/step\n",
      "11/11 [==============================] - 0s 856us/step\n",
      "(for 1 minibatch) Training loss 0.0114126 | PSNR training 39.9051590\n",
      "\n",
      " epoch 1135 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 916us/step\n",
      "(for 1 minibatch) Training loss 0.0113407 | PSNR training 40.0141144\n",
      "\n",
      " epoch 1136 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 899us/step\n",
      "(for 1 minibatch) Training loss 0.0115129 | PSNR training 40.0067291\n",
      "\n",
      " epoch 1137 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 869us/step\n",
      "(for 1 minibatch) Training loss 0.0113925 | PSNR training 39.8685799\n",
      "\n",
      " epoch 1138 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 887us/step\n",
      "(for 1 minibatch) Training loss 0.0113566 | PSNR training 39.9168129\n",
      "\n",
      " epoch 1139 of 2000\n",
      "5/5 [==============================] - 0s 965us/step\n",
      "11/11 [==============================] - 0s 865us/step\n",
      "(for 1 minibatch) Training loss 0.0114655 | PSNR training 39.9094734\n",
      "\n",
      " epoch 1140 of 2000\n",
      "5/5 [==============================] - 0s 996us/step\n",
      "11/11 [==============================] - 0s 923us/step\n",
      "(for 1 minibatch) Training loss 0.0114895 | PSNR training 39.8890915\n",
      "\n",
      " epoch 1141 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 850us/step\n",
      "(for 1 minibatch) Training loss 0.0113752 | PSNR training 40.0260544\n",
      "\n",
      " epoch 1142 of 2000\n",
      "5/5 [==============================] - 0s 987us/step\n",
      "11/11 [==============================] - 0s 887us/step\n",
      "(for 1 minibatch) Training loss 0.0115392 | PSNR training 39.9881401\n",
      "\n",
      " epoch 1143 of 2000\n",
      "5/5 [==============================] - 0s 957us/step\n",
      "11/11 [==============================] - 0s 829us/step\n",
      "(for 1 minibatch) Training loss 0.0113378 | PSNR training 40.1298141\n",
      "\n",
      " epoch 1144 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 884us/step\n",
      "(for 1 minibatch) Training loss 0.0119061 | PSNR training 39.6352730\n",
      "\n",
      " epoch 1145 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 940us/step\n",
      "(for 1 minibatch) Training loss 0.0117897 | PSNR training 39.6648560\n",
      "\n",
      " epoch 1146 of 2000\n",
      "5/5 [==============================] - 0s 991us/step\n",
      "11/11 [==============================] - 0s 986us/step\n",
      "(for 1 minibatch) Training loss 0.0113544 | PSNR training 40.1557388\n",
      "\n",
      " epoch 1147 of 2000\n",
      "5/5 [==============================] - 0s 959us/step\n",
      "11/11 [==============================] - 0s 862us/step\n",
      "(for 1 minibatch) Training loss 0.0114526 | PSNR training 39.8398819\n",
      "\n",
      " epoch 1148 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 834us/step\n",
      "(for 1 minibatch) Training loss 0.0116823 | PSNR training 39.8763008\n",
      "\n",
      " epoch 1149 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 858us/step\n",
      "(for 1 minibatch) Training loss 0.0116550 | PSNR training 39.7224007\n",
      "\n",
      " epoch 1150 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 856us/step\n",
      "(for 1 minibatch) Training loss 0.0118209 | PSNR training 39.9331627\n",
      "\n",
      " epoch 1151 of 2000\n",
      "5/5 [==============================] - 0s 980us/step\n",
      "11/11 [==============================] - 0s 846us/step\n",
      "(for 1 minibatch) Training loss 0.0113373 | PSNR training 40.0014877\n",
      "\n",
      " epoch 1152 of 2000\n",
      "5/5 [==============================] - 0s 963us/step\n",
      "11/11 [==============================] - 0s 910us/step\n",
      "(for 1 minibatch) Training loss 0.0114434 | PSNR training 39.8836594\n",
      "\n",
      " epoch 1153 of 2000\n",
      "5/5 [==============================] - 0s 987us/step\n",
      "11/11 [==============================] - 0s 1ms/step\n",
      "(for 1 minibatch) Training loss 0.0115314 | PSNR training 39.9655266\n",
      "\n",
      " epoch 1154 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 874us/step\n",
      "(for 1 minibatch) Training loss 0.0116221 | PSNR training 39.8240204\n",
      "\n",
      " epoch 1155 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 871us/step\n",
      "(for 1 minibatch) Training loss 0.0118136 | PSNR training 39.7138329\n",
      "\n",
      " epoch 1156 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 987us/step\n",
      "(for 1 minibatch) Training loss 0.0115620 | PSNR training 39.9915161\n",
      "\n",
      " epoch 1157 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 1ms/step\n",
      "(for 1 minibatch) Training loss 0.0115932 | PSNR training 39.7529144\n",
      "\n",
      " epoch 1158 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 1ms/step\n",
      "(for 1 minibatch) Training loss 0.0115058 | PSNR training 39.8541870\n",
      "\n",
      " epoch 1159 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 851us/step\n",
      "(for 1 minibatch) Training loss 0.0117276 | PSNR training 39.7581062\n",
      "\n",
      " epoch 1160 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 1ms/step\n",
      "(for 1 minibatch) Training loss 0.0110823 | PSNR training 40.1783524\n",
      "\n",
      " epoch 1161 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 1ms/step\n",
      "(for 1 minibatch) Training loss 0.0114280 | PSNR training 40.0452766\n",
      "\n",
      " epoch 1162 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 917us/step\n",
      "(for 1 minibatch) Training loss 0.0115768 | PSNR training 40.0291290\n",
      "\n",
      " epoch 1163 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 826us/step\n",
      "(for 1 minibatch) Training loss 0.0113779 | PSNR training 40.0429535\n",
      "\n",
      " epoch 1164 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 1ms/step\n",
      "(for 1 minibatch) Training loss 0.0118760 | PSNR training 39.9676895\n",
      "\n",
      " epoch 1165 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 927us/step\n",
      "(for 1 minibatch) Training loss 0.0112397 | PSNR training 39.8822327\n",
      "\n",
      " epoch 1166 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 866us/step\n",
      "(for 1 minibatch) Training loss 0.0117629 | PSNR training 39.8455086\n",
      "\n",
      " epoch 1167 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 845us/step\n",
      "(for 1 minibatch) Training loss 0.0116807 | PSNR training 39.7784882\n",
      "\n",
      " epoch 1168 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 962us/step\n",
      "(for 1 minibatch) Training loss 0.0115629 | PSNR training 40.0626755\n",
      "\n",
      " epoch 1169 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 968us/step\n",
      "(for 1 minibatch) Training loss 0.0114233 | PSNR training 39.9294510\n",
      "\n",
      " epoch 1170 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 892us/step\n",
      "(for 1 minibatch) Training loss 0.0114082 | PSNR training 40.0061264\n",
      "\n",
      " epoch 1171 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 936us/step\n",
      "(for 1 minibatch) Training loss 0.0116851 | PSNR training 39.9782982\n",
      "\n",
      " epoch 1172 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 1ms/step\n",
      "(for 1 minibatch) Training loss 0.0113412 | PSNR training 40.0225525\n",
      "\n",
      " epoch 1173 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 1ms/step\n",
      "(for 1 minibatch) Training loss 0.0113004 | PSNR training 40.0326080\n",
      "\n",
      " epoch 1174 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 982us/step\n",
      "(for 1 minibatch) Training loss 0.0116223 | PSNR training 39.7390938\n",
      "\n",
      " epoch 1175 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 931us/step\n",
      "(for 1 minibatch) Training loss 0.0115469 | PSNR training 39.9393997\n",
      "\n",
      " epoch 1176 of 2000\n",
      "5/5 [==============================] - 0s 982us/step\n",
      "11/11 [==============================] - 0s 1ms/step\n",
      "(for 1 minibatch) Training loss 0.0113994 | PSNR training 40.0831680\n",
      "\n",
      " epoch 1177 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 1ms/step\n",
      "(for 1 minibatch) Training loss 0.0113161 | PSNR training 40.0408325\n",
      "\n",
      " epoch 1178 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 1ms/step\n",
      "(for 1 minibatch) Training loss 0.0114843 | PSNR training 40.0581970\n",
      "\n",
      " epoch 1179 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 926us/step\n",
      "(for 1 minibatch) Training loss 0.0116286 | PSNR training 39.9718628\n",
      "\n",
      " epoch 1180 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 886us/step\n",
      "(for 1 minibatch) Training loss 0.0109394 | PSNR training 40.2771072\n",
      "\n",
      " epoch 1181 of 2000\n",
      "5/5 [==============================] - 0s 984us/step\n",
      "11/11 [==============================] - 0s 921us/step\n",
      "(for 1 minibatch) Training loss 0.0113920 | PSNR training 40.1530457\n",
      "\n",
      " epoch 1182 of 2000\n",
      "5/5 [==============================] - 0s 957us/step\n",
      "11/11 [==============================] - 0s 886us/step\n",
      "(for 1 minibatch) Training loss 0.0111798 | PSNR training 40.0431213\n",
      "\n",
      " epoch 1183 of 2000\n",
      "5/5 [==============================] - 0s 963us/step\n",
      "11/11 [==============================] - 0s 878us/step\n",
      "(for 1 minibatch) Training loss 0.0114809 | PSNR training 40.1359138\n",
      "\n",
      " epoch 1184 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 874us/step\n",
      "(for 1 minibatch) Training loss 0.0116193 | PSNR training 39.7231865\n",
      "\n",
      " epoch 1185 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 817us/step\n",
      "(for 1 minibatch) Training loss 0.0116084 | PSNR training 40.0147324\n",
      "\n",
      " epoch 1186 of 2000\n",
      "5/5 [==============================] - 0s 950us/step\n",
      "11/11 [==============================] - 0s 1ms/step\n",
      "(for 1 minibatch) Training loss 0.0117621 | PSNR training 39.9274216\n",
      "\n",
      " epoch 1187 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 863us/step\n",
      "(for 1 minibatch) Training loss 0.0113009 | PSNR training 39.8463936\n",
      "\n",
      " epoch 1188 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 862us/step\n",
      "(for 1 minibatch) Training loss 0.0113912 | PSNR training 40.0106430\n",
      "\n",
      " epoch 1189 of 2000\n",
      "5/5 [==============================] - 0s 937us/step\n",
      "11/11 [==============================] - 0s 947us/step\n",
      "(for 1 minibatch) Training loss 0.0112855 | PSNR training 40.0694199\n",
      "\n",
      " epoch 1190 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 849us/step\n",
      "(for 1 minibatch) Training loss 0.0116817 | PSNR training 39.9869652\n",
      "\n",
      " epoch 1191 of 2000\n",
      "5/5 [==============================] - 0s 980us/step\n",
      "11/11 [==============================] - 0s 835us/step\n",
      "(for 1 minibatch) Training loss 0.0113683 | PSNR training 40.0948639\n",
      "\n",
      " epoch 1192 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 850us/step\n",
      "(for 1 minibatch) Training loss 0.0117279 | PSNR training 39.8629837\n",
      "\n",
      " epoch 1193 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 827us/step\n",
      "(for 1 minibatch) Training loss 0.0116319 | PSNR training 39.9157066\n",
      "\n",
      " epoch 1194 of 2000\n",
      "5/5 [==============================] - 0s 954us/step\n",
      "11/11 [==============================] - 0s 861us/step\n",
      "(for 1 minibatch) Training loss 0.0117554 | PSNR training 39.8901291\n",
      "\n",
      " epoch 1195 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 868us/step\n",
      "(for 1 minibatch) Training loss 0.0120848 | PSNR training 39.5317345\n",
      "\n",
      " epoch 1196 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 1ms/step\n",
      "(for 1 minibatch) Training loss 0.0115116 | PSNR training 39.9750175\n",
      "\n",
      " epoch 1197 of 2000\n",
      "5/5 [==============================] - 0s 991us/step\n",
      "11/11 [==============================] - 0s 850us/step\n",
      "(for 1 minibatch) Training loss 0.0115612 | PSNR training 39.8737259\n",
      "\n",
      " epoch 1198 of 2000\n",
      "5/5 [==============================] - 0s 983us/step\n",
      "11/11 [==============================] - 0s 861us/step\n",
      "(for 1 minibatch) Training loss 0.0114430 | PSNR training 40.0427818\n",
      "\n",
      " epoch 1199 of 2000\n",
      "5/5 [==============================] - 0s 978us/step\n",
      "11/11 [==============================] - 0s 859us/step\n",
      "(for 1 minibatch) Training loss 0.0112985 | PSNR training 40.2613373\n",
      "\n",
      " epoch 1200 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 841us/step\n",
      "(for 1 minibatch) Training loss 0.0112416 | PSNR training 40.0718994\n",
      "\n",
      " epoch 1201 of 2000\n",
      "5/5 [==============================] - 0s 932us/step\n",
      "11/11 [==============================] - 0s 866us/step\n",
      "(for 1 minibatch) Training loss 0.0113655 | PSNR training 39.8734131\n",
      "\n",
      " epoch 1202 of 2000\n",
      "5/5 [==============================] - 0s 955us/step\n",
      "11/11 [==============================] - 0s 835us/step\n",
      "(for 1 minibatch) Training loss 0.0114907 | PSNR training 39.9484406\n",
      "\n",
      " epoch 1203 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 847us/step\n",
      "(for 1 minibatch) Training loss 0.0118710 | PSNR training 39.9151344\n",
      "\n",
      " epoch 1204 of 2000\n",
      "5/5 [==============================] - 0s 943us/step\n",
      "11/11 [==============================] - 0s 888us/step\n",
      "(for 1 minibatch) Training loss 0.0116884 | PSNR training 39.7199402\n",
      "\n",
      " epoch 1205 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 882us/step\n",
      "(for 1 minibatch) Training loss 0.0112966 | PSNR training 39.7230797\n",
      "\n",
      " epoch 1206 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 854us/step\n",
      "(for 1 minibatch) Training loss 0.0118167 | PSNR training 39.8560562\n",
      "\n",
      " epoch 1207 of 2000\n",
      "5/5 [==============================] - 0s 978us/step\n",
      "11/11 [==============================] - 0s 854us/step\n",
      "(for 1 minibatch) Training loss 0.0116088 | PSNR training 39.7980728\n",
      "\n",
      " epoch 1208 of 2000\n",
      "5/5 [==============================] - 0s 990us/step\n",
      "11/11 [==============================] - 0s 882us/step\n",
      "(for 1 minibatch) Training loss 0.0111684 | PSNR training 40.1373100\n",
      "\n",
      " epoch 1209 of 2000\n",
      "5/5 [==============================] - 0s 985us/step\n",
      "11/11 [==============================] - 0s 887us/step\n",
      "(for 1 minibatch) Training loss 0.0113534 | PSNR training 40.0488052\n",
      "\n",
      " epoch 1210 of 2000\n",
      "5/5 [==============================] - 0s 994us/step\n",
      "11/11 [==============================] - 0s 896us/step\n",
      "(for 1 minibatch) Training loss 0.0111831 | PSNR training 40.2169952\n",
      "\n",
      " epoch 1211 of 2000\n",
      "5/5 [==============================] - 0s 989us/step\n",
      "11/11 [==============================] - 0s 855us/step\n",
      "(for 1 minibatch) Training loss 0.0111551 | PSNR training 40.2123947\n",
      "\n",
      " epoch 1212 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 865us/step\n",
      "(for 1 minibatch) Training loss 0.0111961 | PSNR training 40.1792297\n",
      "\n",
      " epoch 1213 of 2000\n",
      "5/5 [==============================] - 0s 952us/step\n",
      "11/11 [==============================] - 0s 934us/step\n",
      "(for 1 minibatch) Training loss 0.0113479 | PSNR training 39.9617805\n",
      "\n",
      " epoch 1214 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 859us/step\n",
      "(for 1 minibatch) Training loss 0.0116202 | PSNR training 39.9923935\n",
      "\n",
      " epoch 1215 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 824us/step\n",
      "(for 1 minibatch) Training loss 0.0110258 | PSNR training 40.2018509\n",
      "\n",
      " epoch 1216 of 2000\n",
      "5/5 [==============================] - 0s 992us/step\n",
      "11/11 [==============================] - 0s 865us/step\n",
      "(for 1 minibatch) Training loss 0.0112103 | PSNR training 40.0130653\n",
      "\n",
      " epoch 1217 of 2000\n",
      "5/5 [==============================] - 0s 969us/step\n",
      "11/11 [==============================] - 0s 916us/step\n",
      "(for 1 minibatch) Training loss 0.0114666 | PSNR training 39.8943481\n",
      "\n",
      " epoch 1218 of 2000\n",
      "5/5 [==============================] - 0s 967us/step\n",
      "11/11 [==============================] - 0s 879us/step\n",
      "(for 1 minibatch) Training loss 0.0112307 | PSNR training 40.1219330\n",
      "\n",
      " epoch 1219 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 856us/step\n",
      "(for 1 minibatch) Training loss 0.0115159 | PSNR training 39.9483833\n",
      "\n",
      " epoch 1220 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 954us/step\n",
      "(for 1 minibatch) Training loss 0.0116253 | PSNR training 39.8457565\n",
      "\n",
      " epoch 1221 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 880us/step\n",
      "(for 1 minibatch) Training loss 0.0118400 | PSNR training 39.8397369\n",
      "\n",
      " epoch 1222 of 2000\n",
      "5/5 [==============================] - 0s 963us/step\n",
      "11/11 [==============================] - 0s 1ms/step\n",
      "(for 1 minibatch) Training loss 0.0117715 | PSNR training 39.7407341\n",
      "\n",
      " epoch 1223 of 2000\n",
      "5/5 [==============================] - 0s 990us/step\n",
      "11/11 [==============================] - 0s 878us/step\n",
      "(for 1 minibatch) Training loss 0.0116305 | PSNR training 39.9778976\n",
      "\n",
      " epoch 1224 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 883us/step\n",
      "(for 1 minibatch) Training loss 0.0114495 | PSNR training 40.1577377\n",
      "\n",
      " epoch 1225 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 858us/step\n",
      "(for 1 minibatch) Training loss 0.0115755 | PSNR training 39.8261032\n",
      "\n",
      " epoch 1226 of 2000\n",
      "5/5 [==============================] - 0s 961us/step\n",
      "11/11 [==============================] - 0s 1ms/step\n",
      "(for 1 minibatch) Training loss 0.0113139 | PSNR training 40.0355873\n",
      "\n",
      " epoch 1227 of 2000\n",
      "5/5 [==============================] - 0s 988us/step\n",
      "11/11 [==============================] - 0s 862us/step\n",
      "(for 1 minibatch) Training loss 0.0110521 | PSNR training 40.2697601\n",
      "\n",
      " epoch 1228 of 2000\n",
      "5/5 [==============================] - 0s 972us/step\n",
      "11/11 [==============================] - 0s 889us/step\n",
      "(for 1 minibatch) Training loss 0.0113177 | PSNR training 40.1806259\n",
      "\n",
      " epoch 1229 of 2000\n",
      "5/5 [==============================] - 0s 944us/step\n",
      "11/11 [==============================] - 0s 840us/step\n",
      "(for 1 minibatch) Training loss 0.0112926 | PSNR training 40.0387993\n",
      "\n",
      " epoch 1230 of 2000\n",
      "5/5 [==============================] - 0s 988us/step\n",
      "11/11 [==============================] - 0s 901us/step\n",
      "(for 1 minibatch) Training loss 0.0113834 | PSNR training 40.0495911\n",
      "\n",
      " epoch 1231 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 852us/step\n",
      "(for 1 minibatch) Training loss 0.0118917 | PSNR training 39.9793091\n",
      "\n",
      " epoch 1232 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 846us/step\n",
      "(for 1 minibatch) Training loss 0.0119471 | PSNR training 39.8007126\n",
      "\n",
      " epoch 1233 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 817us/step\n",
      "(for 1 minibatch) Training loss 0.0113351 | PSNR training 40.2418289\n",
      "\n",
      " epoch 1234 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 1ms/step\n",
      "(for 1 minibatch) Training loss 0.0115654 | PSNR training 40.0551643\n",
      "\n",
      " epoch 1235 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 878us/step\n",
      "(for 1 minibatch) Training loss 0.0115678 | PSNR training 40.0939102\n",
      "\n",
      " epoch 1236 of 2000\n",
      "5/5 [==============================] - 0s 976us/step\n",
      "11/11 [==============================] - 0s 1ms/step\n",
      "(for 1 minibatch) Training loss 0.0111517 | PSNR training 39.9318657\n",
      "\n",
      " epoch 1237 of 2000\n",
      "5/5 [==============================] - 0s 968us/step\n",
      "11/11 [==============================] - 0s 858us/step\n",
      "(for 1 minibatch) Training loss 0.0114966 | PSNR training 39.9408951\n",
      "\n",
      " epoch 1238 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 877us/step\n",
      "(for 1 minibatch) Training loss 0.0114868 | PSNR training 39.8838158\n",
      "\n",
      " epoch 1239 of 2000\n",
      "5/5 [==============================] - 0s 975us/step\n",
      "11/11 [==============================] - 0s 897us/step\n",
      "(for 1 minibatch) Training loss 0.0115750 | PSNR training 39.8087540\n",
      "\n",
      " epoch 1240 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 823us/step\n",
      "(for 1 minibatch) Training loss 0.0113155 | PSNR training 40.0946808\n",
      "\n",
      " epoch 1241 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 892us/step\n",
      "(for 1 minibatch) Training loss 0.0116006 | PSNR training 39.5520287\n",
      "\n",
      " epoch 1242 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 891us/step\n",
      "(for 1 minibatch) Training loss 0.0113337 | PSNR training 39.8588982\n",
      "\n",
      " epoch 1243 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 827us/step\n",
      "(for 1 minibatch) Training loss 0.0113289 | PSNR training 40.0738220\n",
      "\n",
      " epoch 1244 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 907us/step\n",
      "(for 1 minibatch) Training loss 0.0112305 | PSNR training 39.8902473\n",
      "\n",
      " epoch 1245 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 940us/step\n",
      "(for 1 minibatch) Training loss 0.0113253 | PSNR training 40.0360413\n",
      "\n",
      " epoch 1246 of 2000\n",
      "5/5 [==============================] - 0s 962us/step\n",
      "11/11 [==============================] - 0s 918us/step\n",
      "(for 1 minibatch) Training loss 0.0113917 | PSNR training 40.0734177\n",
      "\n",
      " epoch 1247 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 836us/step\n",
      "(for 1 minibatch) Training loss 0.0111755 | PSNR training 40.1979141\n",
      "\n",
      " epoch 1248 of 2000\n",
      "5/5 [==============================] - 0s 991us/step\n",
      "11/11 [==============================] - 0s 1ms/step\n",
      "(for 1 minibatch) Training loss 0.0113388 | PSNR training 40.1649475\n",
      "\n",
      " epoch 1249 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 868us/step\n",
      "(for 1 minibatch) Training loss 0.0118187 | PSNR training 39.7863884\n",
      "\n",
      " epoch 1250 of 2000\n",
      "5/5 [==============================] - 0s 994us/step\n",
      "11/11 [==============================] - 0s 895us/step\n",
      "(for 1 minibatch) Training loss 0.0118352 | PSNR training 39.8849335\n",
      "\n",
      " epoch 1251 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 864us/step\n",
      "(for 1 minibatch) Training loss 0.0113223 | PSNR training 39.8960648\n",
      "\n",
      " epoch 1252 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 949us/step\n",
      "(for 1 minibatch) Training loss 0.0114778 | PSNR training 39.8288651\n",
      "\n",
      " epoch 1253 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 862us/step\n",
      "(for 1 minibatch) Training loss 0.0114899 | PSNR training 39.9373322\n",
      "\n",
      " epoch 1254 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 993us/step\n",
      "(for 1 minibatch) Training loss 0.0116568 | PSNR training 39.9683342\n",
      "\n",
      " epoch 1255 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 833us/step\n",
      "(for 1 minibatch) Training loss 0.0112117 | PSNR training 39.9470100\n",
      "\n",
      " epoch 1256 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 830us/step\n",
      "(for 1 minibatch) Training loss 0.0111576 | PSNR training 40.3174210\n",
      "\n",
      " epoch 1257 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 861us/step\n",
      "(for 1 minibatch) Training loss 0.0112876 | PSNR training 40.0967445\n",
      "\n",
      " epoch 1258 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 886us/step\n",
      "(for 1 minibatch) Training loss 0.0112303 | PSNR training 40.2107811\n",
      "\n",
      " epoch 1259 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 850us/step\n",
      "(for 1 minibatch) Training loss 0.0113788 | PSNR training 40.2358551\n",
      "\n",
      " epoch 1260 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 853us/step\n",
      "(for 1 minibatch) Training loss 0.0111267 | PSNR training 40.2317886\n",
      "\n",
      " epoch 1261 of 2000\n",
      "5/5 [==============================] - 0s 992us/step\n",
      "11/11 [==============================] - 0s 845us/step\n",
      "(for 1 minibatch) Training loss 0.0111865 | PSNR training 40.3627357\n",
      "\n",
      " epoch 1262 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 917us/step\n",
      "(for 1 minibatch) Training loss 0.0110847 | PSNR training 40.4050865\n",
      "\n",
      " epoch 1263 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 912us/step\n",
      "(for 1 minibatch) Training loss 0.0114173 | PSNR training 40.2070236\n",
      "\n",
      " epoch 1264 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 891us/step\n",
      "(for 1 minibatch) Training loss 0.0111775 | PSNR training 40.2828674\n",
      "\n",
      " epoch 1265 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 882us/step\n",
      "(for 1 minibatch) Training loss 0.0111783 | PSNR training 40.2240410\n",
      "\n",
      " epoch 1266 of 2000\n",
      "5/5 [==============================] - 0s 988us/step\n",
      "11/11 [==============================] - 0s 1ms/step\n",
      "(for 1 minibatch) Training loss 0.0116650 | PSNR training 39.9263153\n",
      "\n",
      " epoch 1267 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 846us/step\n",
      "(for 1 minibatch) Training loss 0.0120521 | PSNR training 39.7817688\n",
      "\n",
      " epoch 1268 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 847us/step\n",
      "(for 1 minibatch) Training loss 0.0116982 | PSNR training 39.8194046\n",
      "\n",
      " epoch 1269 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 1ms/step\n",
      "(for 1 minibatch) Training loss 0.0115155 | PSNR training 40.0527573\n",
      "\n",
      " epoch 1270 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 1ms/step\n",
      "(for 1 minibatch) Training loss 0.0113263 | PSNR training 40.0919304\n",
      "\n",
      " epoch 1271 of 2000\n",
      "5/5 [==============================] - 0s 989us/step\n",
      "11/11 [==============================] - 0s 896us/step\n",
      "(for 1 minibatch) Training loss 0.0112386 | PSNR training 40.1762733\n",
      "\n",
      " epoch 1272 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 843us/step\n",
      "(for 1 minibatch) Training loss 0.0110353 | PSNR training 40.3188019\n",
      "\n",
      " epoch 1273 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 821us/step\n",
      "(for 1 minibatch) Training loss 0.0111768 | PSNR training 40.1029205\n",
      "\n",
      " epoch 1274 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 828us/step\n",
      "(for 1 minibatch) Training loss 0.0111629 | PSNR training 40.1054726\n",
      "\n",
      " epoch 1275 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 911us/step\n",
      "(for 1 minibatch) Training loss 0.0111660 | PSNR training 40.3101501\n",
      "\n",
      " epoch 1276 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 927us/step\n",
      "(for 1 minibatch) Training loss 0.0115226 | PSNR training 40.0445099\n",
      "\n",
      " epoch 1277 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 910us/step\n",
      "(for 1 minibatch) Training loss 0.0113822 | PSNR training 39.9906082\n",
      "\n",
      " epoch 1278 of 2000\n",
      "5/5 [==============================] - 0s 981us/step\n",
      "11/11 [==============================] - 0s 820us/step\n",
      "(for 1 minibatch) Training loss 0.0114099 | PSNR training 39.8767929\n",
      "\n",
      " epoch 1279 of 2000\n",
      "5/5 [==============================] - 0s 966us/step\n",
      "11/11 [==============================] - 0s 883us/step\n",
      "(for 1 minibatch) Training loss 0.0115671 | PSNR training 39.9550819\n",
      "\n",
      " epoch 1280 of 2000\n",
      "5/5 [==============================] - 0s 941us/step\n",
      "11/11 [==============================] - 0s 839us/step\n",
      "(for 1 minibatch) Training loss 0.0115771 | PSNR training 39.7597809\n",
      "\n",
      " epoch 1281 of 2000\n",
      "5/5 [==============================] - 0s 986us/step\n",
      "11/11 [==============================] - 0s 853us/step\n",
      "(for 1 minibatch) Training loss 0.0115525 | PSNR training 39.5283661\n",
      "\n",
      " epoch 1282 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 841us/step\n",
      "(for 1 minibatch) Training loss 0.0113018 | PSNR training 40.1664276\n",
      "\n",
      " epoch 1283 of 2000\n",
      "5/5 [==============================] - 0s 984us/step\n",
      "11/11 [==============================] - 0s 897us/step\n",
      "(for 1 minibatch) Training loss 0.0113238 | PSNR training 40.1371460\n",
      "\n",
      " epoch 1284 of 2000\n",
      "5/5 [==============================] - 0s 992us/step\n",
      "11/11 [==============================] - 0s 833us/step\n",
      "(for 1 minibatch) Training loss 0.0113899 | PSNR training 40.1327705\n",
      "\n",
      " epoch 1285 of 2000\n",
      "5/5 [==============================] - 0s 955us/step\n",
      "11/11 [==============================] - 0s 880us/step\n",
      "(for 1 minibatch) Training loss 0.0112756 | PSNR training 40.1112709\n",
      "\n",
      " epoch 1286 of 2000\n",
      "5/5 [==============================] - 0s 903us/step\n",
      "11/11 [==============================] - 0s 1ms/step\n",
      "(for 1 minibatch) Training loss 0.0111302 | PSNR training 40.2192268\n",
      "\n",
      " epoch 1287 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 987us/step\n",
      "(for 1 minibatch) Training loss 0.0113736 | PSNR training 40.0571861\n",
      "\n",
      " epoch 1288 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 957us/step\n",
      "(for 1 minibatch) Training loss 0.0114546 | PSNR training 40.0660782\n",
      "\n",
      " epoch 1289 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 875us/step\n",
      "(for 1 minibatch) Training loss 0.0112545 | PSNR training 40.1912537\n",
      "\n",
      " epoch 1290 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 807us/step\n",
      "(for 1 minibatch) Training loss 0.0111220 | PSNR training 40.2328148\n",
      "\n",
      " epoch 1291 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 858us/step\n",
      "(for 1 minibatch) Training loss 0.0111734 | PSNR training 40.2163467\n",
      "\n",
      " epoch 1292 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 1ms/step\n",
      "(for 1 minibatch) Training loss 0.0110043 | PSNR training 40.0933380\n",
      "\n",
      " epoch 1293 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 839us/step\n",
      "(for 1 minibatch) Training loss 0.0109000 | PSNR training 40.1403656\n",
      "\n",
      " epoch 1294 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 848us/step\n",
      "(for 1 minibatch) Training loss 0.0113093 | PSNR training 40.2240677\n",
      "\n",
      " epoch 1295 of 2000\n",
      "5/5 [==============================] - 0s 929us/step\n",
      "11/11 [==============================] - 0s 884us/step\n",
      "(for 1 minibatch) Training loss 0.0111727 | PSNR training 40.1694031\n",
      "\n",
      " epoch 1296 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 965us/step\n",
      "(for 1 minibatch) Training loss 0.0113317 | PSNR training 40.1129112\n",
      "\n",
      " epoch 1297 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 1ms/step\n",
      "(for 1 minibatch) Training loss 0.0118227 | PSNR training 40.0081291\n",
      "\n",
      " epoch 1298 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 880us/step\n",
      "(for 1 minibatch) Training loss 0.0116794 | PSNR training 39.8933487\n",
      "\n",
      " epoch 1299 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 886us/step\n",
      "(for 1 minibatch) Training loss 0.0115203 | PSNR training 39.8563042\n",
      "\n",
      " epoch 1300 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 911us/step\n",
      "(for 1 minibatch) Training loss 0.0115984 | PSNR training 39.9400787\n",
      "\n",
      " epoch 1301 of 2000\n",
      "5/5 [==============================] - 0s 966us/step\n",
      "11/11 [==============================] - 0s 838us/step\n",
      "(for 1 minibatch) Training loss 0.0112588 | PSNR training 40.0666771\n",
      "\n",
      " epoch 1302 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 831us/step\n",
      "(for 1 minibatch) Training loss 0.0113059 | PSNR training 40.1028862\n",
      "\n",
      " epoch 1303 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 837us/step\n",
      "(for 1 minibatch) Training loss 0.0110986 | PSNR training 40.2604294\n",
      "\n",
      " epoch 1304 of 2000\n",
      "5/5 [==============================] - 0s 937us/step\n",
      "11/11 [==============================] - 0s 879us/step\n",
      "(for 1 minibatch) Training loss 0.0113381 | PSNR training 40.1761246\n",
      "\n",
      " epoch 1305 of 2000\n",
      "5/5 [==============================] - 0s 930us/step\n",
      "11/11 [==============================] - 0s 876us/step\n",
      "(for 1 minibatch) Training loss 0.0115082 | PSNR training 40.0316963\n",
      "\n",
      " epoch 1306 of 2000\n",
      "5/5 [==============================] - 0s 975us/step\n",
      "11/11 [==============================] - 0s 852us/step\n",
      "(for 1 minibatch) Training loss 0.0112710 | PSNR training 40.1819572\n",
      "\n",
      " epoch 1307 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 952us/step\n",
      "(for 1 minibatch) Training loss 0.0110201 | PSNR training 40.3334999\n",
      "\n",
      " epoch 1308 of 2000\n",
      "5/5 [==============================] - 0s 935us/step\n",
      "11/11 [==============================] - 0s 909us/step\n",
      "(for 1 minibatch) Training loss 0.0114676 | PSNR training 40.1197662\n",
      "\n",
      " epoch 1309 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 922us/step\n",
      "(for 1 minibatch) Training loss 0.0116264 | PSNR training 39.9858055\n",
      "\n",
      " epoch 1310 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 882us/step\n",
      "(for 1 minibatch) Training loss 0.0114413 | PSNR training 40.0833168\n",
      "\n",
      " epoch 1311 of 2000\n",
      "5/5 [==============================] - 0s 911us/step\n",
      "11/11 [==============================] - 0s 862us/step\n",
      "(for 1 minibatch) Training loss 0.0118750 | PSNR training 39.8537521\n",
      "\n",
      " epoch 1312 of 2000\n",
      "5/5 [==============================] - 0s 989us/step\n",
      "11/11 [==============================] - 0s 832us/step\n",
      "(for 1 minibatch) Training loss 0.0110241 | PSNR training 40.0700531\n",
      "\n",
      " epoch 1313 of 2000\n",
      "5/5 [==============================] - 0s 950us/step\n",
      "11/11 [==============================] - 0s 893us/step\n",
      "(for 1 minibatch) Training loss 0.0109492 | PSNR training 40.2806511\n",
      "\n",
      " epoch 1314 of 2000\n",
      "5/5 [==============================] - 0s 996us/step\n",
      "11/11 [==============================] - 0s 903us/step\n",
      "(for 1 minibatch) Training loss 0.0112199 | PSNR training 40.3174171\n",
      "\n",
      " epoch 1315 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 844us/step\n",
      "(for 1 minibatch) Training loss 0.0114574 | PSNR training 40.1713104\n",
      "\n",
      " epoch 1316 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 959us/step\n",
      "(for 1 minibatch) Training loss 0.0113164 | PSNR training 39.9470139\n",
      "\n",
      " epoch 1317 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 944us/step\n",
      "(for 1 minibatch) Training loss 0.0111409 | PSNR training 40.2437210\n",
      "\n",
      " epoch 1318 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 879us/step\n",
      "(for 1 minibatch) Training loss 0.0117306 | PSNR training 39.9223785\n",
      "\n",
      " epoch 1319 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 904us/step\n",
      "(for 1 minibatch) Training loss 0.0117459 | PSNR training 39.9097519\n",
      "\n",
      " epoch 1320 of 2000\n",
      "5/5 [==============================] - 0s 968us/step\n",
      "11/11 [==============================] - 0s 860us/step\n",
      "(for 1 minibatch) Training loss 0.0111849 | PSNR training 40.1974869\n",
      "\n",
      " epoch 1321 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 847us/step\n",
      "(for 1 minibatch) Training loss 0.0112287 | PSNR training 39.8930702\n",
      "\n",
      " epoch 1322 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 944us/step\n",
      "(for 1 minibatch) Training loss 0.0117775 | PSNR training 39.9361649\n",
      "\n",
      " epoch 1323 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 943us/step\n",
      "(for 1 minibatch) Training loss 0.0111554 | PSNR training 40.1451874\n",
      "\n",
      " epoch 1324 of 2000\n",
      "5/5 [==============================] - 0s 1000us/step\n",
      "11/11 [==============================] - 0s 899us/step\n",
      "(for 1 minibatch) Training loss 0.0110815 | PSNR training 40.2090149\n",
      "\n",
      " epoch 1325 of 2000\n",
      "5/5 [==============================] - 0s 985us/step\n",
      "11/11 [==============================] - 0s 872us/step\n",
      "(for 1 minibatch) Training loss 0.0111484 | PSNR training 40.2733383\n",
      "\n",
      " epoch 1326 of 2000\n",
      "5/5 [==============================] - 0s 966us/step\n",
      "11/11 [==============================] - 0s 831us/step\n",
      "(for 1 minibatch) Training loss 0.0109989 | PSNR training 40.4027214\n",
      "\n",
      " epoch 1327 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 868us/step\n",
      "(for 1 minibatch) Training loss 0.0114145 | PSNR training 40.0054893\n",
      "\n",
      " epoch 1328 of 2000\n",
      "5/5 [==============================] - 0s 936us/step\n",
      "11/11 [==============================] - 0s 850us/step\n",
      "(for 1 minibatch) Training loss 0.0113020 | PSNR training 40.3938675\n",
      "\n",
      " epoch 1329 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 928us/step\n",
      "(for 1 minibatch) Training loss 0.0107578 | PSNR training 39.9688683\n",
      "\n",
      " epoch 1330 of 2000\n",
      "5/5 [==============================] - 0s 985us/step\n",
      "11/11 [==============================] - 0s 878us/step\n",
      "(for 1 minibatch) Training loss 0.0112191 | PSNR training 40.1058502\n",
      "\n",
      " epoch 1331 of 2000\n",
      "5/5 [==============================] - 0s 986us/step\n",
      "11/11 [==============================] - 0s 925us/step\n",
      "(for 1 minibatch) Training loss 0.0110462 | PSNR training 40.3886642\n",
      "\n",
      " epoch 1332 of 2000\n",
      "5/5 [==============================] - 0s 921us/step\n",
      "11/11 [==============================] - 0s 830us/step\n",
      "(for 1 minibatch) Training loss 0.0110475 | PSNR training 40.1277885\n",
      "\n",
      " epoch 1333 of 2000\n",
      "5/5 [==============================] - 0s 937us/step\n",
      "11/11 [==============================] - 0s 833us/step\n",
      "(for 1 minibatch) Training loss 0.0110683 | PSNR training 40.2285919\n",
      "\n",
      " epoch 1334 of 2000\n",
      "5/5 [==============================] - 0s 936us/step\n",
      "11/11 [==============================] - 0s 866us/step\n",
      "(for 1 minibatch) Training loss 0.0109591 | PSNR training 40.0441742\n",
      "\n",
      " epoch 1335 of 2000\n",
      "5/5 [==============================] - 0s 953us/step\n",
      "11/11 [==============================] - 0s 822us/step\n",
      "(for 1 minibatch) Training loss 0.0114728 | PSNR training 40.0963631\n",
      "\n",
      " epoch 1336 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 906us/step\n",
      "(for 1 minibatch) Training loss 0.0116396 | PSNR training 39.8769531\n",
      "\n",
      " epoch 1337 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 853us/step\n",
      "(for 1 minibatch) Training loss 0.0112140 | PSNR training 39.9660568\n",
      "\n",
      " epoch 1338 of 2000\n",
      "5/5 [==============================] - 0s 943us/step\n",
      "11/11 [==============================] - 0s 870us/step\n",
      "(for 1 minibatch) Training loss 0.0110474 | PSNR training 40.2370453\n",
      "\n",
      " epoch 1339 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 874us/step\n",
      "(for 1 minibatch) Training loss 0.0114135 | PSNR training 40.1010895\n",
      "\n",
      " epoch 1340 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 900us/step\n",
      "(for 1 minibatch) Training loss 0.0113885 | PSNR training 40.1753311\n",
      "\n",
      " epoch 1341 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 877us/step\n",
      "(for 1 minibatch) Training loss 0.0111430 | PSNR training 40.1806641\n",
      "\n",
      " epoch 1342 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 865us/step\n",
      "(for 1 minibatch) Training loss 0.0111848 | PSNR training 40.0613785\n",
      "\n",
      " epoch 1343 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 967us/step\n",
      "(for 1 minibatch) Training loss 0.0115951 | PSNR training 40.0126343\n",
      "\n",
      " epoch 1344 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 911us/step\n",
      "(for 1 minibatch) Training loss 0.0115939 | PSNR training 40.2306213\n",
      "\n",
      " epoch 1345 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 874us/step\n",
      "(for 1 minibatch) Training loss 0.0112589 | PSNR training 40.0156212\n",
      "\n",
      " epoch 1346 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 840us/step\n",
      "(for 1 minibatch) Training loss 0.0112316 | PSNR training 40.2865295\n",
      "\n",
      " epoch 1347 of 2000\n",
      "5/5 [==============================] - 0s 998us/step\n",
      "11/11 [==============================] - 0s 871us/step\n",
      "(for 1 minibatch) Training loss 0.0112072 | PSNR training 40.1638985\n",
      "\n",
      " epoch 1348 of 2000\n",
      "5/5 [==============================] - 0s 965us/step\n",
      "11/11 [==============================] - 0s 1ms/step\n",
      "(for 1 minibatch) Training loss 0.0112383 | PSNR training 40.2071686\n",
      "\n",
      " epoch 1349 of 2000\n",
      "5/5 [==============================] - 0s 970us/step\n",
      "11/11 [==============================] - 0s 859us/step\n",
      "(for 1 minibatch) Training loss 0.0111023 | PSNR training 40.1824532\n",
      "\n",
      " epoch 1350 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 973us/step\n",
      "(for 1 minibatch) Training loss 0.0114018 | PSNR training 40.1652489\n",
      "\n",
      " epoch 1351 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 858us/step\n",
      "(for 1 minibatch) Training loss 0.0111263 | PSNR training 40.3050766\n",
      "\n",
      " epoch 1352 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 887us/step\n",
      "(for 1 minibatch) Training loss 0.0109407 | PSNR training 40.3168793\n",
      "\n",
      " epoch 1353 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 955us/step\n",
      "(for 1 minibatch) Training loss 0.0109915 | PSNR training 40.2603607\n",
      "\n",
      " epoch 1354 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 988us/step\n",
      "(for 1 minibatch) Training loss 0.0109761 | PSNR training 40.2868881\n",
      "\n",
      " epoch 1355 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 844us/step\n",
      "(for 1 minibatch) Training loss 0.0111387 | PSNR training 40.2033463\n",
      "\n",
      " epoch 1356 of 2000\n",
      "5/5 [==============================] - 0s 940us/step\n",
      "11/11 [==============================] - 0s 835us/step\n",
      "(for 1 minibatch) Training loss 0.0109831 | PSNR training 40.3049049\n",
      "\n",
      " epoch 1357 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 988us/step\n",
      "(for 1 minibatch) Training loss 0.0108437 | PSNR training 40.4016075\n",
      "\n",
      " epoch 1358 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 870us/step\n",
      "(for 1 minibatch) Training loss 0.0110468 | PSNR training 40.1806717\n",
      "\n",
      " epoch 1359 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 936us/step\n",
      "(for 1 minibatch) Training loss 0.0109866 | PSNR training 40.3086433\n",
      "\n",
      " epoch 1360 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 841us/step\n",
      "(for 1 minibatch) Training loss 0.0105324 | PSNR training 40.6424408\n",
      "\n",
      " epoch 1361 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 832us/step\n",
      "(for 1 minibatch) Training loss 0.0107543 | PSNR training 40.4695358\n",
      "\n",
      " epoch 1362 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 944us/step\n",
      "(for 1 minibatch) Training loss 0.0106949 | PSNR training 40.4642067\n",
      "\n",
      " epoch 1363 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 901us/step\n",
      "(for 1 minibatch) Training loss 0.0106524 | PSNR training 40.5716324\n",
      "\n",
      " epoch 1364 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 851us/step\n",
      "(for 1 minibatch) Training loss 0.0109133 | PSNR training 40.2037621\n",
      "\n",
      " epoch 1365 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 854us/step\n",
      "(for 1 minibatch) Training loss 0.0108478 | PSNR training 40.5082855\n",
      "\n",
      " epoch 1366 of 2000\n",
      "5/5 [==============================] - 0s 958us/step\n",
      "11/11 [==============================] - 0s 856us/step\n",
      "(for 1 minibatch) Training loss 0.0108694 | PSNR training 40.5606689\n",
      "\n",
      " epoch 1367 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 856us/step\n",
      "(for 1 minibatch) Training loss 0.0106676 | PSNR training 40.4687653\n",
      "\n",
      " epoch 1368 of 2000\n",
      "5/5 [==============================] - 0s 949us/step\n",
      "11/11 [==============================] - 0s 908us/step\n",
      "(for 1 minibatch) Training loss 0.0110682 | PSNR training 40.3537598\n",
      "\n",
      " epoch 1369 of 2000\n",
      "5/5 [==============================] - 0s 966us/step\n",
      "11/11 [==============================] - 0s 860us/step\n",
      "(for 1 minibatch) Training loss 0.0106661 | PSNR training 40.4516449\n",
      "\n",
      " epoch 1370 of 2000\n",
      "5/5 [==============================] - 0s 993us/step\n",
      "11/11 [==============================] - 0s 935us/step\n",
      "(for 1 minibatch) Training loss 0.0105831 | PSNR training 40.6054230\n",
      "\n",
      " epoch 1371 of 2000\n",
      "5/5 [==============================] - 0s 995us/step\n",
      "11/11 [==============================] - 0s 883us/step\n",
      "(for 1 minibatch) Training loss 0.0109384 | PSNR training 40.4026108\n",
      "\n",
      " epoch 1372 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 854us/step\n",
      "(for 1 minibatch) Training loss 0.0109838 | PSNR training 40.3366280\n",
      "\n",
      " epoch 1373 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 828us/step\n",
      "(for 1 minibatch) Training loss 0.0109830 | PSNR training 40.5172043\n",
      "\n",
      " epoch 1374 of 2000\n",
      "5/5 [==============================] - 0s 964us/step\n",
      "11/11 [==============================] - 0s 827us/step\n",
      "(for 1 minibatch) Training loss 0.0107843 | PSNR training 40.5737915\n",
      "\n",
      " epoch 1375 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 1ms/step\n",
      "(for 1 minibatch) Training loss 0.0107159 | PSNR training 40.5240211\n",
      "\n",
      " epoch 1376 of 2000\n",
      "5/5 [==============================] - 0s 926us/step\n",
      "11/11 [==============================] - 0s 860us/step\n",
      "(for 1 minibatch) Training loss 0.0107352 | PSNR training 40.4919357\n",
      "\n",
      " epoch 1377 of 2000\n",
      "5/5 [==============================] - 0s 980us/step\n",
      "11/11 [==============================] - 0s 814us/step\n",
      "(for 1 minibatch) Training loss 0.0109523 | PSNR training 40.3805466\n",
      "\n",
      " epoch 1378 of 2000\n",
      "5/5 [==============================] - 0s 948us/step\n",
      "11/11 [==============================] - 0s 853us/step\n",
      "(for 1 minibatch) Training loss 0.0106710 | PSNR training 40.4438705\n",
      "\n",
      " epoch 1379 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 915us/step\n",
      "(for 1 minibatch) Training loss 0.0109141 | PSNR training 40.3511581\n",
      "\n",
      " epoch 1380 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 918us/step\n",
      "(for 1 minibatch) Training loss 0.0106453 | PSNR training 40.5383224\n",
      "\n",
      " epoch 1381 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 932us/step\n",
      "(for 1 minibatch) Training loss 0.0108076 | PSNR training 40.4281921\n",
      "\n",
      " epoch 1382 of 2000\n",
      "5/5 [==============================] - 0s 929us/step\n",
      "11/11 [==============================] - 0s 960us/step\n",
      "(for 1 minibatch) Training loss 0.0108447 | PSNR training 40.4941177\n",
      "\n",
      " epoch 1383 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 842us/step\n",
      "(for 1 minibatch) Training loss 0.0108790 | PSNR training 40.3699837\n",
      "\n",
      " epoch 1384 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 839us/step\n",
      "(for 1 minibatch) Training loss 0.0108059 | PSNR training 40.4938736\n",
      "\n",
      " epoch 1385 of 2000\n",
      "5/5 [==============================] - 0s 971us/step\n",
      "11/11 [==============================] - 0s 842us/step\n",
      "(for 1 minibatch) Training loss 0.0112492 | PSNR training 40.2135277\n",
      "\n",
      " epoch 1386 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 900us/step\n",
      "(for 1 minibatch) Training loss 0.0104827 | PSNR training 40.5942726\n",
      "\n",
      " epoch 1387 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 927us/step\n",
      "(for 1 minibatch) Training loss 0.0106256 | PSNR training 40.6153679\n",
      "\n",
      " epoch 1388 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 851us/step\n",
      "(for 1 minibatch) Training loss 0.0109855 | PSNR training 40.4928513\n",
      "\n",
      " epoch 1389 of 2000\n",
      "5/5 [==============================] - 0s 997us/step\n",
      "11/11 [==============================] - 0s 808us/step\n",
      "(for 1 minibatch) Training loss 0.0108420 | PSNR training 40.4927597\n",
      "\n",
      " epoch 1390 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 862us/step\n",
      "(for 1 minibatch) Training loss 0.0110245 | PSNR training 40.3347588\n",
      "\n",
      " epoch 1391 of 2000\n",
      "5/5 [==============================] - 0s 936us/step\n",
      "11/11 [==============================] - 0s 854us/step\n",
      "(for 1 minibatch) Training loss 0.0109212 | PSNR training 40.4867783\n",
      "\n",
      " epoch 1392 of 2000\n",
      "5/5 [==============================] - 0s 976us/step\n",
      "11/11 [==============================] - 0s 840us/step\n",
      "(for 1 minibatch) Training loss 0.0107356 | PSNR training 40.6151619\n",
      "\n",
      " epoch 1393 of 2000\n",
      "5/5 [==============================] - 0s 997us/step\n",
      "11/11 [==============================] - 0s 857us/step\n",
      "(for 1 minibatch) Training loss 0.0106809 | PSNR training 40.4487228\n",
      "\n",
      " epoch 1394 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 947us/step\n",
      "(for 1 minibatch) Training loss 0.0109023 | PSNR training 40.3914185\n",
      "\n",
      " epoch 1395 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 841us/step\n",
      "(for 1 minibatch) Training loss 0.0104753 | PSNR training 40.7666359\n",
      "\n",
      " epoch 1396 of 2000\n",
      "5/5 [==============================] - 0s 1000us/step\n",
      "11/11 [==============================] - 0s 876us/step\n",
      "(for 1 minibatch) Training loss 0.0109380 | PSNR training 40.2426987\n",
      "\n",
      " epoch 1397 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 844us/step\n",
      "(for 1 minibatch) Training loss 0.0108877 | PSNR training 40.6189575\n",
      "\n",
      " epoch 1398 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 836us/step\n",
      "(for 1 minibatch) Training loss 0.0106192 | PSNR training 40.5400124\n",
      "\n",
      " epoch 1399 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 824us/step\n",
      "(for 1 minibatch) Training loss 0.0105935 | PSNR training 40.6324234\n",
      "\n",
      " epoch 1400 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 893us/step\n",
      "(for 1 minibatch) Training loss 0.0108693 | PSNR training 40.6331100\n",
      "\n",
      " epoch 1401 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 925us/step\n",
      "(for 1 minibatch) Training loss 0.0107121 | PSNR training 40.3315277\n",
      "\n",
      " epoch 1402 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 880us/step\n",
      "(for 1 minibatch) Training loss 0.0107939 | PSNR training 40.4716148\n",
      "\n",
      " epoch 1403 of 2000\n",
      "5/5 [==============================] - 0s 984us/step\n",
      "11/11 [==============================] - 0s 1ms/step\n",
      "(for 1 minibatch) Training loss 0.0106335 | PSNR training 40.4815979\n",
      "\n",
      " epoch 1404 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 986us/step\n",
      "(for 1 minibatch) Training loss 0.0109065 | PSNR training 40.5394211\n",
      "\n",
      " epoch 1405 of 2000\n",
      "5/5 [==============================] - 0s 980us/step\n",
      "11/11 [==============================] - 0s 1ms/step\n",
      "(for 1 minibatch) Training loss 0.0106453 | PSNR training 40.5295715\n",
      "\n",
      " epoch 1406 of 2000\n",
      "5/5 [==============================] - 0s 975us/step\n",
      "11/11 [==============================] - 0s 895us/step\n",
      "(for 1 minibatch) Training loss 0.0107194 | PSNR training 40.4114456\n",
      "\n",
      " epoch 1407 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 924us/step\n",
      "(for 1 minibatch) Training loss 0.0107461 | PSNR training 40.7495728\n",
      "\n",
      " epoch 1408 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 857us/step\n",
      "(for 1 minibatch) Training loss 0.0108235 | PSNR training 40.4466553\n",
      "\n",
      " epoch 1409 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 854us/step\n",
      "(for 1 minibatch) Training loss 0.0107197 | PSNR training 40.6089935\n",
      "\n",
      " epoch 1410 of 2000\n",
      "5/5 [==============================] - 0s 980us/step\n",
      "11/11 [==============================] - 0s 923us/step\n",
      "(for 1 minibatch) Training loss 0.0110168 | PSNR training 40.3049622\n",
      "\n",
      " epoch 1411 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 882us/step\n",
      "(for 1 minibatch) Training loss 0.0104225 | PSNR training 40.5408173\n",
      "\n",
      " epoch 1412 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 883us/step\n",
      "(for 1 minibatch) Training loss 0.0108089 | PSNR training 40.4004745\n",
      "\n",
      " epoch 1413 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 912us/step\n",
      "(for 1 minibatch) Training loss 0.0105723 | PSNR training 40.5249367\n",
      "\n",
      " epoch 1414 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 951us/step\n",
      "(for 1 minibatch) Training loss 0.0106781 | PSNR training 40.5730247\n",
      "\n",
      " epoch 1415 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 905us/step\n",
      "(for 1 minibatch) Training loss 0.0110102 | PSNR training 40.4185829\n",
      "\n",
      " epoch 1416 of 2000\n",
      "5/5 [==============================] - 0s 952us/step\n",
      "11/11 [==============================] - 0s 897us/step\n",
      "(for 1 minibatch) Training loss 0.0107493 | PSNR training 40.6019630\n",
      "\n",
      " epoch 1417 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 873us/step\n",
      "(for 1 minibatch) Training loss 0.0106486 | PSNR training 40.6375160\n",
      "\n",
      " epoch 1418 of 2000\n",
      "5/5 [==============================] - 0s 995us/step\n",
      "11/11 [==============================] - 0s 850us/step\n",
      "(for 1 minibatch) Training loss 0.0108126 | PSNR training 40.5180702\n",
      "\n",
      " epoch 1419 of 2000\n",
      "5/5 [==============================] - 0s 945us/step\n",
      "11/11 [==============================] - 0s 871us/step\n",
      "(for 1 minibatch) Training loss 0.0108258 | PSNR training 40.3615532\n",
      "\n",
      " epoch 1420 of 2000\n",
      "5/5 [==============================] - 0s 971us/step\n",
      "11/11 [==============================] - 0s 865us/step\n",
      "(for 1 minibatch) Training loss 0.0105883 | PSNR training 40.5709991\n",
      "\n",
      " epoch 1421 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 872us/step\n",
      "(for 1 minibatch) Training loss 0.0109329 | PSNR training 40.5174294\n",
      "\n",
      " epoch 1422 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 984us/step\n",
      "(for 1 minibatch) Training loss 0.0108878 | PSNR training 40.4105225\n",
      "\n",
      " epoch 1423 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 875us/step\n",
      "(for 1 minibatch) Training loss 0.0104979 | PSNR training 40.4240456\n",
      "\n",
      " epoch 1424 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 877us/step\n",
      "(for 1 minibatch) Training loss 0.0104524 | PSNR training 40.6937180\n",
      "\n",
      " epoch 1425 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 886us/step\n",
      "(for 1 minibatch) Training loss 0.0109054 | PSNR training 40.4053574\n",
      "\n",
      " epoch 1426 of 2000\n",
      "5/5 [==============================] - 0s 994us/step\n",
      "11/11 [==============================] - 0s 879us/step\n",
      "(for 1 minibatch) Training loss 0.0105957 | PSNR training 40.5420876\n",
      "\n",
      " epoch 1427 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 867us/step\n",
      "(for 1 minibatch) Training loss 0.0104806 | PSNR training 40.6058884\n",
      "\n",
      " epoch 1428 of 2000\n",
      "5/5 [==============================] - 0s 981us/step\n",
      "11/11 [==============================] - 0s 874us/step\n",
      "(for 1 minibatch) Training loss 0.0104039 | PSNR training 40.8331451\n",
      "\n",
      " epoch 1429 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 881us/step\n",
      "(for 1 minibatch) Training loss 0.0107284 | PSNR training 40.5093193\n",
      "\n",
      " epoch 1430 of 2000\n",
      "5/5 [==============================] - 0s 954us/step\n",
      "11/11 [==============================] - 0s 836us/step\n",
      "(for 1 minibatch) Training loss 0.0107976 | PSNR training 40.6281128\n",
      "\n",
      " epoch 1431 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 890us/step\n",
      "(for 1 minibatch) Training loss 0.0104473 | PSNR training 40.7180405\n",
      "\n",
      " epoch 1432 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 882us/step\n",
      "(for 1 minibatch) Training loss 0.0110665 | PSNR training 39.9828796\n",
      "\n",
      " epoch 1433 of 2000\n",
      "5/5 [==============================] - 0s 994us/step\n",
      "11/11 [==============================] - 0s 849us/step\n",
      "(for 1 minibatch) Training loss 0.0106336 | PSNR training 40.7014694\n",
      "\n",
      " epoch 1434 of 2000\n",
      "5/5 [==============================] - 0s 957us/step\n",
      "11/11 [==============================] - 0s 900us/step\n",
      "(for 1 minibatch) Training loss 0.0106946 | PSNR training 40.5102730\n",
      "\n",
      " epoch 1435 of 2000\n",
      "5/5 [==============================] - 0s 991us/step\n",
      "11/11 [==============================] - 0s 855us/step\n",
      "(for 1 minibatch) Training loss 0.0108219 | PSNR training 40.6308289\n",
      "\n",
      " epoch 1436 of 2000\n",
      "5/5 [==============================] - 0s 977us/step\n",
      "11/11 [==============================] - 0s 930us/step\n",
      "(for 1 minibatch) Training loss 0.0107515 | PSNR training 40.5922699\n",
      "\n",
      " epoch 1437 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 862us/step\n",
      "(for 1 minibatch) Training loss 0.0103778 | PSNR training 40.6306877\n",
      "\n",
      " epoch 1438 of 2000\n",
      "5/5 [==============================] - 0s 966us/step\n",
      "11/11 [==============================] - 0s 867us/step\n",
      "(for 1 minibatch) Training loss 0.0105256 | PSNR training 40.5185356\n",
      "\n",
      " epoch 1439 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 879us/step\n",
      "(for 1 minibatch) Training loss 0.0107720 | PSNR training 40.7613945\n",
      "\n",
      " epoch 1440 of 2000\n",
      "5/5 [==============================] - 0s 975us/step\n",
      "11/11 [==============================] - 0s 864us/step\n",
      "(for 1 minibatch) Training loss 0.0106141 | PSNR training 40.4737129\n",
      "\n",
      " epoch 1441 of 2000\n",
      "5/5 [==============================] - 0s 920us/step\n",
      "11/11 [==============================] - 0s 857us/step\n",
      "(for 1 minibatch) Training loss 0.0104084 | PSNR training 40.8725014\n",
      "\n",
      " epoch 1442 of 2000\n",
      "5/5 [==============================] - 0s 988us/step\n",
      "11/11 [==============================] - 0s 1ms/step\n",
      "(for 1 minibatch) Training loss 0.0106675 | PSNR training 40.3618050\n",
      "\n",
      " epoch 1443 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 809us/step\n",
      "(for 1 minibatch) Training loss 0.0105374 | PSNR training 40.5354996\n",
      "\n",
      " epoch 1444 of 2000\n",
      "5/5 [==============================] - 0s 974us/step\n",
      "11/11 [==============================] - 0s 839us/step\n",
      "(for 1 minibatch) Training loss 0.0104536 | PSNR training 40.6885490\n",
      "\n",
      " epoch 1445 of 2000\n",
      "5/5 [==============================] - 0s 976us/step\n",
      "11/11 [==============================] - 0s 895us/step\n",
      "(for 1 minibatch) Training loss 0.0107913 | PSNR training 40.4562836\n",
      "\n",
      " epoch 1446 of 2000\n",
      "5/5 [==============================] - 0s 959us/step\n",
      "11/11 [==============================] - 0s 848us/step\n",
      "(for 1 minibatch) Training loss 0.0103894 | PSNR training 40.5346146\n",
      "\n",
      " epoch 1447 of 2000\n",
      "5/5 [==============================] - 0s 961us/step\n",
      "11/11 [==============================] - 0s 914us/step\n",
      "(for 1 minibatch) Training loss 0.0108268 | PSNR training 40.7274323\n",
      "\n",
      " epoch 1448 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 856us/step\n",
      "(for 1 minibatch) Training loss 0.0106698 | PSNR training 40.6793633\n",
      "\n",
      " epoch 1449 of 2000\n",
      "5/5 [==============================] - 0s 985us/step\n",
      "11/11 [==============================] - 0s 861us/step\n",
      "(for 1 minibatch) Training loss 0.0106638 | PSNR training 40.4646492\n",
      "\n",
      " epoch 1450 of 2000\n",
      "5/5 [==============================] - 0s 987us/step\n",
      "11/11 [==============================] - 0s 839us/step\n",
      "(for 1 minibatch) Training loss 0.0108023 | PSNR training 40.3176460\n",
      "\n",
      " epoch 1451 of 2000\n",
      "5/5 [==============================] - 0s 949us/step\n",
      "11/11 [==============================] - 0s 856us/step\n",
      "(for 1 minibatch) Training loss 0.0106580 | PSNR training 40.5933838\n",
      "\n",
      " epoch 1452 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 834us/step\n",
      "(for 1 minibatch) Training loss 0.0108204 | PSNR training 40.5579605\n",
      "\n",
      " epoch 1453 of 2000\n",
      "5/5 [==============================] - 0s 983us/step\n",
      "11/11 [==============================] - 0s 857us/step\n",
      "(for 1 minibatch) Training loss 0.0105017 | PSNR training 40.5368652\n",
      "\n",
      " epoch 1454 of 2000\n",
      "5/5 [==============================] - 0s 937us/step\n",
      "11/11 [==============================] - 0s 847us/step\n",
      "(for 1 minibatch) Training loss 0.0106913 | PSNR training 40.3472328\n",
      "\n",
      " epoch 1455 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 969us/step\n",
      "(for 1 minibatch) Training loss 0.0109553 | PSNR training 40.3229294\n",
      "\n",
      " epoch 1456 of 2000\n",
      "5/5 [==============================] - 0s 980us/step\n",
      "11/11 [==============================] - 0s 859us/step\n",
      "(for 1 minibatch) Training loss 0.0108929 | PSNR training 40.2684326\n",
      "\n",
      " epoch 1457 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 868us/step\n",
      "(for 1 minibatch) Training loss 0.0105527 | PSNR training 40.7302666\n",
      "\n",
      " epoch 1458 of 2000\n",
      "5/5 [==============================] - 0s 978us/step\n",
      "11/11 [==============================] - 0s 893us/step\n",
      "(for 1 minibatch) Training loss 0.0106508 | PSNR training 40.6757393\n",
      "\n",
      " epoch 1459 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 935us/step\n",
      "(for 1 minibatch) Training loss 0.0106533 | PSNR training 40.6029510\n",
      "\n",
      " epoch 1460 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 904us/step\n",
      "(for 1 minibatch) Training loss 0.0107670 | PSNR training 40.5721893\n",
      "\n",
      " epoch 1461 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 911us/step\n",
      "(for 1 minibatch) Training loss 0.0106966 | PSNR training 40.5254784\n",
      "\n",
      " epoch 1462 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 834us/step\n",
      "(for 1 minibatch) Training loss 0.0107076 | PSNR training 40.4130173\n",
      "\n",
      " epoch 1463 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 899us/step\n",
      "(for 1 minibatch) Training loss 0.0109922 | PSNR training 40.4388657\n",
      "\n",
      " epoch 1464 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 832us/step\n",
      "(for 1 minibatch) Training loss 0.0108271 | PSNR training 40.6370468\n",
      "\n",
      " epoch 1465 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 827us/step\n",
      "(for 1 minibatch) Training loss 0.0107965 | PSNR training 40.5545349\n",
      "\n",
      " epoch 1466 of 2000\n",
      "5/5 [==============================] - 0s 966us/step\n",
      "11/11 [==============================] - 0s 859us/step\n",
      "(for 1 minibatch) Training loss 0.0108331 | PSNR training 40.3853302\n",
      "\n",
      " epoch 1467 of 2000\n",
      "5/5 [==============================] - 0s 940us/step\n",
      "11/11 [==============================] - 0s 863us/step\n",
      "(for 1 minibatch) Training loss 0.0109386 | PSNR training 40.3512993\n",
      "\n",
      " epoch 1468 of 2000\n",
      "5/5 [==============================] - 0s 925us/step\n",
      "11/11 [==============================] - 0s 838us/step\n",
      "(for 1 minibatch) Training loss 0.0105407 | PSNR training 40.6641846\n",
      "\n",
      " epoch 1469 of 2000\n",
      "5/5 [==============================] - 0s 997us/step\n",
      "11/11 [==============================] - 0s 851us/step\n",
      "(for 1 minibatch) Training loss 0.0107471 | PSNR training 40.6908493\n",
      "\n",
      " epoch 1470 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 921us/step\n",
      "(for 1 minibatch) Training loss 0.0106234 | PSNR training 40.5384483\n",
      "\n",
      " epoch 1471 of 2000\n",
      "5/5 [==============================] - 0s 943us/step\n",
      "11/11 [==============================] - 0s 935us/step\n",
      "(for 1 minibatch) Training loss 0.0107431 | PSNR training 40.6327591\n",
      "\n",
      " epoch 1472 of 2000\n",
      "5/5 [==============================] - 0s 957us/step\n",
      "11/11 [==============================] - 0s 845us/step\n",
      "(for 1 minibatch) Training loss 0.0108684 | PSNR training 40.2673187\n",
      "\n",
      " epoch 1473 of 2000\n",
      "5/5 [==============================] - 0s 966us/step\n",
      "11/11 [==============================] - 0s 835us/step\n",
      "(for 1 minibatch) Training loss 0.0104612 | PSNR training 40.7516136\n",
      "\n",
      " epoch 1474 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 874us/step\n",
      "(for 1 minibatch) Training loss 0.0106502 | PSNR training 40.5102882\n",
      "\n",
      " epoch 1475 of 2000\n",
      "5/5 [==============================] - 0s 995us/step\n",
      "11/11 [==============================] - 0s 886us/step\n",
      "(for 1 minibatch) Training loss 0.0108069 | PSNR training 40.6905556\n",
      "\n",
      " epoch 1476 of 2000\n",
      "5/5 [==============================] - 0s 947us/step\n",
      "11/11 [==============================] - 0s 923us/step\n",
      "(for 1 minibatch) Training loss 0.0109644 | PSNR training 40.5191040\n",
      "\n",
      " epoch 1477 of 2000\n",
      "5/5 [==============================] - 0s 986us/step\n",
      "11/11 [==============================] - 0s 914us/step\n",
      "(for 1 minibatch) Training loss 0.0105271 | PSNR training 40.6497040\n",
      "\n",
      " epoch 1478 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 845us/step\n",
      "(for 1 minibatch) Training loss 0.0106286 | PSNR training 40.4576836\n",
      "\n",
      " epoch 1479 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 903us/step\n",
      "(for 1 minibatch) Training loss 0.0105641 | PSNR training 40.6295471\n",
      "\n",
      " epoch 1480 of 2000\n",
      "5/5 [==============================] - 0s 956us/step\n",
      "11/11 [==============================] - 0s 801us/step\n",
      "(for 1 minibatch) Training loss 0.0106908 | PSNR training 40.6711769\n",
      "\n",
      " epoch 1481 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 907us/step\n",
      "(for 1 minibatch) Training loss 0.0106625 | PSNR training 40.5669212\n",
      "\n",
      " epoch 1482 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 875us/step\n",
      "(for 1 minibatch) Training loss 0.0105343 | PSNR training 40.6666756\n",
      "\n",
      " epoch 1483 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 839us/step\n",
      "(for 1 minibatch) Training loss 0.0103144 | PSNR training 40.6139336\n",
      "\n",
      " epoch 1484 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 885us/step\n",
      "(for 1 minibatch) Training loss 0.0104814 | PSNR training 40.7987938\n",
      "\n",
      " epoch 1485 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 840us/step\n",
      "(for 1 minibatch) Training loss 0.0104638 | PSNR training 40.6703491\n",
      "\n",
      " epoch 1486 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 856us/step\n",
      "(for 1 minibatch) Training loss 0.0108309 | PSNR training 40.6737747\n",
      "\n",
      " epoch 1487 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 902us/step\n",
      "(for 1 minibatch) Training loss 0.0106689 | PSNR training 40.4496994\n",
      "\n",
      " epoch 1488 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 885us/step\n",
      "(for 1 minibatch) Training loss 0.0104632 | PSNR training 40.6697617\n",
      "\n",
      " epoch 1489 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 959us/step\n",
      "(for 1 minibatch) Training loss 0.0108745 | PSNR training 40.4754257\n",
      "\n",
      " epoch 1490 of 2000\n",
      "5/5 [==============================] - 0s 921us/step\n",
      "11/11 [==============================] - 0s 1ms/step\n",
      "(for 1 minibatch) Training loss 0.0103640 | PSNR training 40.8202286\n",
      "\n",
      " epoch 1491 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 872us/step\n",
      "(for 1 minibatch) Training loss 0.0106945 | PSNR training 40.5449524\n",
      "\n",
      " epoch 1492 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 909us/step\n",
      "(for 1 minibatch) Training loss 0.0105031 | PSNR training 40.7095757\n",
      "\n",
      " epoch 1493 of 2000\n",
      "5/5 [==============================] - 0s 933us/step\n",
      "11/11 [==============================] - 0s 860us/step\n",
      "(for 1 minibatch) Training loss 0.0103189 | PSNR training 40.7751923\n",
      "\n",
      " epoch 1494 of 2000\n",
      "5/5 [==============================] - 0s 916us/step\n",
      "11/11 [==============================] - 0s 858us/step\n",
      "(for 1 minibatch) Training loss 0.0107055 | PSNR training 40.3976669\n",
      "\n",
      " epoch 1495 of 2000\n",
      "5/5 [==============================] - 0s 964us/step\n",
      "11/11 [==============================] - 0s 917us/step\n",
      "(for 1 minibatch) Training loss 0.0103721 | PSNR training 40.6670341\n",
      "\n",
      " epoch 1496 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 849us/step\n",
      "(for 1 minibatch) Training loss 0.0107720 | PSNR training 40.7847290\n",
      "\n",
      " epoch 1497 of 2000\n",
      "5/5 [==============================] - 0s 897us/step\n",
      "11/11 [==============================] - 0s 909us/step\n",
      "(for 1 minibatch) Training loss 0.0106679 | PSNR training 40.3503189\n",
      "\n",
      " epoch 1498 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 895us/step\n",
      "(for 1 minibatch) Training loss 0.0104415 | PSNR training 40.5881424\n",
      "\n",
      " epoch 1499 of 2000\n",
      "5/5 [==============================] - 0s 927us/step\n",
      "11/11 [==============================] - 0s 881us/step\n",
      "(for 1 minibatch) Training loss 0.0109479 | PSNR training 40.3310547\n",
      "\n",
      " epoch 1500 of 2000\n",
      "5/5 [==============================] - 0s 986us/step\n",
      "11/11 [==============================] - 0s 872us/step\n",
      "(for 1 minibatch) Training loss 0.0103076 | PSNR training 40.8484344\n",
      "\n",
      " epoch 1501 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 876us/step\n",
      "(for 1 minibatch) Training loss 0.0109162 | PSNR training 40.4996147\n",
      "\n",
      " epoch 1502 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 873us/step\n",
      "(for 1 minibatch) Training loss 0.0105079 | PSNR training 40.7129364\n",
      "\n",
      " epoch 1503 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 927us/step\n",
      "(for 1 minibatch) Training loss 0.0106485 | PSNR training 40.4946136\n",
      "\n",
      " epoch 1504 of 2000\n",
      "5/5 [==============================] - 0s 994us/step\n",
      "11/11 [==============================] - 0s 845us/step\n",
      "(for 1 minibatch) Training loss 0.0104964 | PSNR training 40.6899872\n",
      "\n",
      " epoch 1505 of 2000\n",
      "5/5 [==============================] - 0s 936us/step\n",
      "11/11 [==============================] - 0s 906us/step\n",
      "(for 1 minibatch) Training loss 0.0102925 | PSNR training 40.9151001\n",
      "\n",
      " epoch 1506 of 2000\n",
      "5/5 [==============================] - 0s 972us/step\n",
      "11/11 [==============================] - 0s 845us/step\n",
      "(for 1 minibatch) Training loss 0.0106983 | PSNR training 40.5981064\n",
      "\n",
      " epoch 1507 of 2000\n",
      "5/5 [==============================] - 0s 932us/step\n",
      "11/11 [==============================] - 0s 852us/step\n",
      "(for 1 minibatch) Training loss 0.0107000 | PSNR training 40.5564537\n",
      "\n",
      " epoch 1508 of 2000\n",
      "5/5 [==============================] - 0s 998us/step\n",
      "11/11 [==============================] - 0s 930us/step\n",
      "(for 1 minibatch) Training loss 0.0106301 | PSNR training 40.7175827\n",
      "\n",
      " epoch 1509 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 848us/step\n",
      "(for 1 minibatch) Training loss 0.0106582 | PSNR training 40.7153358\n",
      "\n",
      " epoch 1510 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 838us/step\n",
      "(for 1 minibatch) Training loss 0.0105554 | PSNR training 40.6825867\n",
      "\n",
      " epoch 1511 of 2000\n",
      "5/5 [==============================] - 0s 971us/step\n",
      "11/11 [==============================] - 0s 912us/step\n",
      "(for 1 minibatch) Training loss 0.0106125 | PSNR training 40.5882301\n",
      "\n",
      " epoch 1512 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 906us/step\n",
      "(for 1 minibatch) Training loss 0.0108001 | PSNR training 40.5320892\n",
      "\n",
      " epoch 1513 of 2000\n",
      "5/5 [==============================] - 0s 942us/step\n",
      "11/11 [==============================] - 0s 876us/step\n",
      "(for 1 minibatch) Training loss 0.0106441 | PSNR training 40.5770683\n",
      "\n",
      " epoch 1514 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 858us/step\n",
      "(for 1 minibatch) Training loss 0.0106428 | PSNR training 40.5014496\n",
      "\n",
      " epoch 1515 of 2000\n",
      "5/5 [==============================] - 0s 941us/step\n",
      "11/11 [==============================] - 0s 818us/step\n",
      "(for 1 minibatch) Training loss 0.0106553 | PSNR training 40.6386490\n",
      "\n",
      " epoch 1516 of 2000\n",
      "5/5 [==============================] - 0s 927us/step\n",
      "11/11 [==============================] - 0s 835us/step\n",
      "(for 1 minibatch) Training loss 0.0105150 | PSNR training 40.7753334\n",
      "\n",
      " epoch 1517 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 955us/step\n",
      "(for 1 minibatch) Training loss 0.0104878 | PSNR training 40.5848885\n",
      "\n",
      " epoch 1518 of 2000\n",
      "5/5 [==============================] - 0s 980us/step\n",
      "11/11 [==============================] - 0s 884us/step\n",
      "(for 1 minibatch) Training loss 0.0106164 | PSNR training 40.7270088\n",
      "\n",
      " epoch 1519 of 2000\n",
      "5/5 [==============================] - 0s 944us/step\n",
      "11/11 [==============================] - 0s 919us/step\n",
      "(for 1 minibatch) Training loss 0.0104279 | PSNR training 40.5523300\n",
      "\n",
      " epoch 1520 of 2000\n",
      "5/5 [==============================] - 0s 992us/step\n",
      "11/11 [==============================] - 0s 853us/step\n",
      "(for 1 minibatch) Training loss 0.0106897 | PSNR training 40.6677551\n",
      "\n",
      " epoch 1521 of 2000\n",
      "5/5 [==============================] - 0s 942us/step\n",
      "11/11 [==============================] - 0s 850us/step\n",
      "(for 1 minibatch) Training loss 0.0106877 | PSNR training 40.5401649\n",
      "\n",
      " epoch 1522 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 935us/step\n",
      "(for 1 minibatch) Training loss 0.0104852 | PSNR training 40.7692299\n",
      "\n",
      " epoch 1523 of 2000\n",
      "5/5 [==============================] - 0s 959us/step\n",
      "11/11 [==============================] - 0s 929us/step\n",
      "(for 1 minibatch) Training loss 0.0106783 | PSNR training 40.6320496\n",
      "\n",
      " epoch 1524 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 912us/step\n",
      "(for 1 minibatch) Training loss 0.0104588 | PSNR training 40.5142593\n",
      "\n",
      " epoch 1525 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 929us/step\n",
      "(for 1 minibatch) Training loss 0.0107349 | PSNR training 40.7873726\n",
      "\n",
      " epoch 1526 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 913us/step\n",
      "(for 1 minibatch) Training loss 0.0106354 | PSNR training 40.6746063\n",
      "\n",
      " epoch 1527 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 891us/step\n",
      "(for 1 minibatch) Training loss 0.0106295 | PSNR training 40.7201958\n",
      "\n",
      " epoch 1528 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 863us/step\n",
      "(for 1 minibatch) Training loss 0.0101344 | PSNR training 40.8779182\n",
      "\n",
      " epoch 1529 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 914us/step\n",
      "(for 1 minibatch) Training loss 0.0104530 | PSNR training 40.7786560\n",
      "\n",
      " epoch 1530 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 1ms/step\n",
      "(for 1 minibatch) Training loss 0.0104012 | PSNR training 40.7636566\n",
      "\n",
      " epoch 1531 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 967us/step\n",
      "(for 1 minibatch) Training loss 0.0104926 | PSNR training 40.6351280\n",
      "\n",
      " epoch 1532 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 927us/step\n",
      "(for 1 minibatch) Training loss 0.0105782 | PSNR training 40.7809525\n",
      "\n",
      " epoch 1533 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 954us/step\n",
      "(for 1 minibatch) Training loss 0.0103099 | PSNR training 40.8348427\n",
      "\n",
      " epoch 1534 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 980us/step\n",
      "(for 1 minibatch) Training loss 0.0103067 | PSNR training 40.7675514\n",
      "\n",
      " epoch 1535 of 2000\n",
      "5/5 [==============================] - 0s 976us/step\n",
      "11/11 [==============================] - 0s 983us/step\n",
      "(for 1 minibatch) Training loss 0.0104063 | PSNR training 40.5672493\n",
      "\n",
      " epoch 1536 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 957us/step\n",
      "(for 1 minibatch) Training loss 0.0103906 | PSNR training 40.5652008\n",
      "\n",
      " epoch 1537 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 909us/step\n",
      "(for 1 minibatch) Training loss 0.0105350 | PSNR training 40.8947678\n",
      "\n",
      " epoch 1538 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 858us/step\n",
      "(for 1 minibatch) Training loss 0.0107132 | PSNR training 40.2325096\n",
      "\n",
      " epoch 1539 of 2000\n",
      "5/5 [==============================] - 0s 983us/step\n",
      "11/11 [==============================] - 0s 870us/step\n",
      "(for 1 minibatch) Training loss 0.0105433 | PSNR training 40.6504784\n",
      "\n",
      " epoch 1540 of 2000\n",
      "5/5 [==============================] - 0s 981us/step\n",
      "11/11 [==============================] - 0s 838us/step\n",
      "(for 1 minibatch) Training loss 0.0105865 | PSNR training 40.7748108\n",
      "\n",
      " epoch 1541 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 914us/step\n",
      "(for 1 minibatch) Training loss 0.0107309 | PSNR training 40.5771255\n",
      "\n",
      " epoch 1542 of 2000\n",
      "5/5 [==============================] - 0s 947us/step\n",
      "11/11 [==============================] - 0s 927us/step\n",
      "(for 1 minibatch) Training loss 0.0103159 | PSNR training 40.7064095\n",
      "\n",
      " epoch 1543 of 2000\n",
      "5/5 [==============================] - 0s 997us/step\n",
      "11/11 [==============================] - 0s 937us/step\n",
      "(for 1 minibatch) Training loss 0.0104064 | PSNR training 40.7048492\n",
      "\n",
      " epoch 1544 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 903us/step\n",
      "(for 1 minibatch) Training loss 0.0110422 | PSNR training 40.4460678\n",
      "\n",
      " epoch 1545 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 1ms/step\n",
      "(for 1 minibatch) Training loss 0.0104082 | PSNR training 40.6320763\n",
      "\n",
      " epoch 1546 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 1ms/step\n",
      "(for 1 minibatch) Training loss 0.0106439 | PSNR training 40.6317978\n",
      "\n",
      " epoch 1547 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 932us/step\n",
      "(for 1 minibatch) Training loss 0.0103862 | PSNR training 40.8779678\n",
      "\n",
      " epoch 1548 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 923us/step\n",
      "(for 1 minibatch) Training loss 0.0105618 | PSNR training 40.5298080\n",
      "\n",
      " epoch 1549 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 833us/step\n",
      "(for 1 minibatch) Training loss 0.0102209 | PSNR training 40.8006821\n",
      "\n",
      " epoch 1550 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 996us/step\n",
      "(for 1 minibatch) Training loss 0.0106359 | PSNR training 40.6019020\n",
      "\n",
      " epoch 1551 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 947us/step\n",
      "(for 1 minibatch) Training loss 0.0108356 | PSNR training 40.6087036\n",
      "\n",
      " epoch 1552 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 893us/step\n",
      "(for 1 minibatch) Training loss 0.0106140 | PSNR training 40.7094498\n",
      "\n",
      " epoch 1553 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 1ms/step\n",
      "(for 1 minibatch) Training loss 0.0104229 | PSNR training 40.5990639\n",
      "\n",
      " epoch 1554 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 912us/step\n",
      "(for 1 minibatch) Training loss 0.0105984 | PSNR training 40.6089592\n",
      "\n",
      " epoch 1555 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 902us/step\n",
      "(for 1 minibatch) Training loss 0.0105429 | PSNR training 40.4879379\n",
      "\n",
      " epoch 1556 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 855us/step\n",
      "(for 1 minibatch) Training loss 0.0104290 | PSNR training 40.7284546\n",
      "\n",
      " epoch 1557 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 1ms/step\n",
      "(for 1 minibatch) Training loss 0.0103914 | PSNR training 40.7225571\n",
      "\n",
      " epoch 1558 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 938us/step\n",
      "(for 1 minibatch) Training loss 0.0105346 | PSNR training 40.6207657\n",
      "\n",
      " epoch 1559 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 942us/step\n",
      "(for 1 minibatch) Training loss 0.0107670 | PSNR training 40.5562592\n",
      "\n",
      " epoch 1560 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 878us/step\n",
      "(for 1 minibatch) Training loss 0.0107490 | PSNR training 40.5924377\n",
      "\n",
      " epoch 1561 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 889us/step\n",
      "(for 1 minibatch) Training loss 0.0107103 | PSNR training 40.6842766\n",
      "\n",
      " epoch 1562 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 898us/step\n",
      "(for 1 minibatch) Training loss 0.0104601 | PSNR training 40.5066071\n",
      "\n",
      " epoch 1563 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 924us/step\n",
      "(for 1 minibatch) Training loss 0.0106701 | PSNR training 40.7225876\n",
      "\n",
      " epoch 1564 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 886us/step\n",
      "(for 1 minibatch) Training loss 0.0104374 | PSNR training 40.6818581\n",
      "\n",
      " epoch 1565 of 2000\n",
      "5/5 [==============================] - 0s 979us/step\n",
      "11/11 [==============================] - 0s 986us/step\n",
      "(for 1 minibatch) Training loss 0.0103757 | PSNR training 41.0048904\n",
      "\n",
      " epoch 1566 of 2000\n",
      "5/5 [==============================] - 0s 977us/step\n",
      "11/11 [==============================] - 0s 866us/step\n",
      "(for 1 minibatch) Training loss 0.0105683 | PSNR training 40.6824799\n",
      "\n",
      " epoch 1567 of 2000\n",
      "5/5 [==============================] - 0s 964us/step\n",
      "11/11 [==============================] - 0s 854us/step\n",
      "(for 1 minibatch) Training loss 0.0103295 | PSNR training 40.7061958\n",
      "\n",
      " epoch 1568 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 1ms/step\n",
      "(for 1 minibatch) Training loss 0.0104127 | PSNR training 40.7390785\n",
      "\n",
      " epoch 1569 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 879us/step\n",
      "(for 1 minibatch) Training loss 0.0102955 | PSNR training 40.8662186\n",
      "\n",
      " epoch 1570 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 1ms/step\n",
      "(for 1 minibatch) Training loss 0.0104337 | PSNR training 40.9042511\n",
      "\n",
      " epoch 1571 of 2000\n",
      "5/5 [==============================] - 0s 969us/step\n",
      "11/11 [==============================] - 0s 851us/step\n",
      "(for 1 minibatch) Training loss 0.0103024 | PSNR training 40.9205627\n",
      "\n",
      " epoch 1572 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 1ms/step\n",
      "(for 1 minibatch) Training loss 0.0104643 | PSNR training 40.6854935\n",
      "\n",
      " epoch 1573 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 974us/step\n",
      "(for 1 minibatch) Training loss 0.0106380 | PSNR training 40.7744751\n",
      "\n",
      " epoch 1574 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 1ms/step\n",
      "(for 1 minibatch) Training loss 0.0104456 | PSNR training 40.6838837\n",
      "\n",
      " epoch 1575 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 858us/step\n",
      "(for 1 minibatch) Training loss 0.0102819 | PSNR training 40.8711815\n",
      "\n",
      " epoch 1576 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 1ms/step\n",
      "(for 1 minibatch) Training loss 0.0105126 | PSNR training 40.8158035\n",
      "\n",
      " epoch 1577 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 905us/step\n",
      "(for 1 minibatch) Training loss 0.0107118 | PSNR training 40.6802826\n",
      "\n",
      " epoch 1578 of 2000\n",
      "5/5 [==============================] - 0s 993us/step\n",
      "11/11 [==============================] - 0s 863us/step\n",
      "(for 1 minibatch) Training loss 0.0105781 | PSNR training 40.7202301\n",
      "\n",
      " epoch 1579 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 876us/step\n",
      "(for 1 minibatch) Training loss 0.0104241 | PSNR training 40.7293549\n",
      "\n",
      " epoch 1580 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 897us/step\n",
      "(for 1 minibatch) Training loss 0.0104892 | PSNR training 40.6642456\n",
      "\n",
      " epoch 1581 of 2000\n",
      "5/5 [==============================] - 0s 992us/step\n",
      "11/11 [==============================] - 0s 1ms/step\n",
      "(for 1 minibatch) Training loss 0.0105750 | PSNR training 40.7213821\n",
      "\n",
      " epoch 1582 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 823us/step\n",
      "(for 1 minibatch) Training loss 0.0104142 | PSNR training 40.6769943\n",
      "\n",
      " epoch 1583 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 865us/step\n",
      "(for 1 minibatch) Training loss 0.0106222 | PSNR training 40.6506042\n",
      "\n",
      " epoch 1584 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 931us/step\n",
      "(for 1 minibatch) Training loss 0.0105801 | PSNR training 40.6963654\n",
      "\n",
      " epoch 1585 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 970us/step\n",
      "(for 1 minibatch) Training loss 0.0103931 | PSNR training 40.8447838\n",
      "\n",
      " epoch 1586 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 958us/step\n",
      "(for 1 minibatch) Training loss 0.0105928 | PSNR training 40.5895538\n",
      "\n",
      " epoch 1587 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 869us/step\n",
      "(for 1 minibatch) Training loss 0.0102979 | PSNR training 40.7716751\n",
      "\n",
      " epoch 1588 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 877us/step\n",
      "(for 1 minibatch) Training loss 0.0104753 | PSNR training 40.7605896\n",
      "\n",
      " epoch 1589 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 910us/step\n",
      "(for 1 minibatch) Training loss 0.0106953 | PSNR training 40.5807037\n",
      "\n",
      " epoch 1590 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 1ms/step\n",
      "(for 1 minibatch) Training loss 0.0102504 | PSNR training 40.6505623\n",
      "\n",
      " epoch 1591 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 1ms/step\n",
      "(for 1 minibatch) Training loss 0.0105106 | PSNR training 40.7123108\n",
      "\n",
      " epoch 1592 of 2000\n",
      "5/5 [==============================] - 0s 957us/step\n",
      "11/11 [==============================] - 0s 942us/step\n",
      "(for 1 minibatch) Training loss 0.0104359 | PSNR training 40.8756866\n",
      "\n",
      " epoch 1593 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 885us/step\n",
      "(for 1 minibatch) Training loss 0.0108773 | PSNR training 40.3763771\n",
      "\n",
      " epoch 1594 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 890us/step\n",
      "(for 1 minibatch) Training loss 0.0104239 | PSNR training 40.7625809\n",
      "\n",
      " epoch 1595 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 845us/step\n",
      "(for 1 minibatch) Training loss 0.0107290 | PSNR training 40.2773705\n",
      "\n",
      " epoch 1596 of 2000\n",
      "5/5 [==============================] - 0s 961us/step\n",
      "11/11 [==============================] - 0s 948us/step\n",
      "(for 1 minibatch) Training loss 0.0104923 | PSNR training 40.6658478\n",
      "\n",
      " epoch 1597 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 893us/step\n",
      "(for 1 minibatch) Training loss 0.0104650 | PSNR training 40.7285919\n",
      "\n",
      " epoch 1598 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 916us/step\n",
      "(for 1 minibatch) Training loss 0.0107425 | PSNR training 40.2175598\n",
      "\n",
      " epoch 1599 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 1ms/step\n",
      "(for 1 minibatch) Training loss 0.0104027 | PSNR training 40.9020996\n",
      "\n",
      " epoch 1600 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 979us/step\n",
      "(for 1 minibatch) Training loss 0.0104328 | PSNR training 40.9015808\n",
      "\n",
      " epoch 1601 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 859us/step\n",
      "(for 1 minibatch) Training loss 0.0104091 | PSNR training 40.5988770\n",
      "\n",
      " epoch 1602 of 2000\n",
      "5/5 [==============================] - 0s 999us/step\n",
      "11/11 [==============================] - 0s 885us/step\n",
      "(for 1 minibatch) Training loss 0.0106089 | PSNR training 40.6759415\n",
      "\n",
      " epoch 1603 of 2000\n",
      "5/5 [==============================] - 0s 974us/step\n",
      "11/11 [==============================] - 0s 866us/step\n",
      "(for 1 minibatch) Training loss 0.0104623 | PSNR training 40.8262482\n",
      "\n",
      " epoch 1604 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 891us/step\n",
      "(for 1 minibatch) Training loss 0.0104286 | PSNR training 40.5463371\n",
      "\n",
      " epoch 1605 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 861us/step\n",
      "(for 1 minibatch) Training loss 0.0104828 | PSNR training 40.9346352\n",
      "\n",
      " epoch 1606 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 840us/step\n",
      "(for 1 minibatch) Training loss 0.0102984 | PSNR training 40.7657852\n",
      "\n",
      " epoch 1607 of 2000\n",
      "5/5 [==============================] - 0s 963us/step\n",
      "11/11 [==============================] - 0s 889us/step\n",
      "(for 1 minibatch) Training loss 0.0108085 | PSNR training 40.4889908\n",
      "\n",
      " epoch 1608 of 2000\n",
      "5/5 [==============================] - 0s 946us/step\n",
      "11/11 [==============================] - 0s 866us/step\n",
      "(for 1 minibatch) Training loss 0.0103093 | PSNR training 40.9055672\n",
      "\n",
      " epoch 1609 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 898us/step\n",
      "(for 1 minibatch) Training loss 0.0105600 | PSNR training 40.6125946\n",
      "\n",
      " epoch 1610 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 884us/step\n",
      "(for 1 minibatch) Training loss 0.0106390 | PSNR training 40.5892982\n",
      "\n",
      " epoch 1611 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 934us/step\n",
      "(for 1 minibatch) Training loss 0.0104680 | PSNR training 40.7146645\n",
      "\n",
      " epoch 1612 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 875us/step\n",
      "(for 1 minibatch) Training loss 0.0104561 | PSNR training 40.5429077\n",
      "\n",
      " epoch 1613 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 880us/step\n",
      "(for 1 minibatch) Training loss 0.0103877 | PSNR training 40.7678146\n",
      "\n",
      " epoch 1614 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 983us/step\n",
      "(for 1 minibatch) Training loss 0.0101966 | PSNR training 40.8344574\n",
      "\n",
      " epoch 1615 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 1ms/step\n",
      "(for 1 minibatch) Training loss 0.0105793 | PSNR training 40.7593346\n",
      "\n",
      " epoch 1616 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 890us/step\n",
      "(for 1 minibatch) Training loss 0.0103182 | PSNR training 40.7525177\n",
      "\n",
      " epoch 1617 of 2000\n",
      "5/5 [==============================] - 0s 991us/step\n",
      "11/11 [==============================] - 0s 894us/step\n",
      "(for 1 minibatch) Training loss 0.0104178 | PSNR training 40.8130188\n",
      "\n",
      " epoch 1618 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 886us/step\n",
      "(for 1 minibatch) Training loss 0.0106578 | PSNR training 40.7993546\n",
      "\n",
      " epoch 1619 of 2000\n",
      "5/5 [==============================] - 0s 985us/step\n",
      "11/11 [==============================] - 0s 855us/step\n",
      "(for 1 minibatch) Training loss 0.0103769 | PSNR training 40.6914940\n",
      "\n",
      " epoch 1620 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 958us/step\n",
      "(for 1 minibatch) Training loss 0.0104905 | PSNR training 40.8513336\n",
      "\n",
      " epoch 1621 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 975us/step\n",
      "(for 1 minibatch) Training loss 0.0103692 | PSNR training 40.8571854\n",
      "\n",
      " epoch 1622 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 893us/step\n",
      "(for 1 minibatch) Training loss 0.0103394 | PSNR training 40.7271957\n",
      "\n",
      " epoch 1623 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 993us/step\n",
      "(for 1 minibatch) Training loss 0.0105520 | PSNR training 40.6108170\n",
      "\n",
      " epoch 1624 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 854us/step\n",
      "(for 1 minibatch) Training loss 0.0104420 | PSNR training 40.5431290\n",
      "\n",
      " epoch 1625 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 1ms/step\n",
      "(for 1 minibatch) Training loss 0.0103084 | PSNR training 40.7155266\n",
      "\n",
      " epoch 1626 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 889us/step\n",
      "(for 1 minibatch) Training loss 0.0104269 | PSNR training 40.8927231\n",
      "\n",
      " epoch 1627 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 848us/step\n",
      "(for 1 minibatch) Training loss 0.0104982 | PSNR training 40.8794975\n",
      "\n",
      " epoch 1628 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 822us/step\n",
      "(for 1 minibatch) Training loss 0.0105330 | PSNR training 40.6446609\n",
      "\n",
      " epoch 1629 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 919us/step\n",
      "(for 1 minibatch) Training loss 0.0103322 | PSNR training 40.9066010\n",
      "\n",
      " epoch 1630 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 855us/step\n",
      "(for 1 minibatch) Training loss 0.0107020 | PSNR training 40.4417000\n",
      "\n",
      " epoch 1631 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 886us/step\n",
      "(for 1 minibatch) Training loss 0.0103403 | PSNR training 40.7493439\n",
      "\n",
      " epoch 1632 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 868us/step\n",
      "(for 1 minibatch) Training loss 0.0103480 | PSNR training 40.8817825\n",
      "\n",
      " epoch 1633 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 927us/step\n",
      "(for 1 minibatch) Training loss 0.0103461 | PSNR training 40.6267128\n",
      "\n",
      " epoch 1634 of 2000\n",
      "5/5 [==============================] - 0s 976us/step\n",
      "11/11 [==============================] - 0s 981us/step\n",
      "(for 1 minibatch) Training loss 0.0103982 | PSNR training 40.7335663\n",
      "\n",
      " epoch 1635 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 970us/step\n",
      "(for 1 minibatch) Training loss 0.0104262 | PSNR training 40.7908859\n",
      "\n",
      " epoch 1636 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 945us/step\n",
      "(for 1 minibatch) Training loss 0.0099913 | PSNR training 41.0356750\n",
      "\n",
      " epoch 1637 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 940us/step\n",
      "(for 1 minibatch) Training loss 0.0104255 | PSNR training 40.6753502\n",
      "\n",
      " epoch 1638 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 858us/step\n",
      "(for 1 minibatch) Training loss 0.0104564 | PSNR training 40.7155266\n",
      "\n",
      " epoch 1639 of 2000\n",
      "5/5 [==============================] - 0s 974us/step\n",
      "11/11 [==============================] - 0s 941us/step\n",
      "(for 1 minibatch) Training loss 0.0107582 | PSNR training 40.7386093\n",
      "\n",
      " epoch 1640 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 970us/step\n",
      "(for 1 minibatch) Training loss 0.0105284 | PSNR training 40.6134415\n",
      "\n",
      " epoch 1641 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 1ms/step\n",
      "(for 1 minibatch) Training loss 0.0101973 | PSNR training 40.9003105\n",
      "\n",
      " epoch 1642 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 915us/step\n",
      "(for 1 minibatch) Training loss 0.0105937 | PSNR training 40.7055092\n",
      "\n",
      " epoch 1643 of 2000\n",
      "5/5 [==============================] - 0s 966us/step\n",
      "11/11 [==============================] - 0s 892us/step\n",
      "(for 1 minibatch) Training loss 0.0104465 | PSNR training 40.8857422\n",
      "\n",
      " epoch 1644 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 866us/step\n",
      "(for 1 minibatch) Training loss 0.0106282 | PSNR training 40.6556396\n",
      "\n",
      " epoch 1645 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 905us/step\n",
      "(for 1 minibatch) Training loss 0.0102447 | PSNR training 40.9914589\n",
      "\n",
      " epoch 1646 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 952us/step\n",
      "(for 1 minibatch) Training loss 0.0106451 | PSNR training 40.8369026\n",
      "\n",
      " epoch 1647 of 2000\n",
      "5/5 [==============================] - 0s 990us/step\n",
      "11/11 [==============================] - 0s 957us/step\n",
      "(for 1 minibatch) Training loss 0.0105332 | PSNR training 40.7924042\n",
      "\n",
      " epoch 1648 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 815us/step\n",
      "(for 1 minibatch) Training loss 0.0102715 | PSNR training 40.9823837\n",
      "\n",
      " epoch 1649 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 875us/step\n",
      "(for 1 minibatch) Training loss 0.0100008 | PSNR training 41.0933723\n",
      "\n",
      " epoch 1650 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 852us/step\n",
      "(for 1 minibatch) Training loss 0.0101808 | PSNR training 40.6138420\n",
      "\n",
      " epoch 1651 of 2000\n",
      "5/5 [==============================] - 0s 968us/step\n",
      "11/11 [==============================] - 0s 924us/step\n",
      "(for 1 minibatch) Training loss 0.0103422 | PSNR training 40.7824173\n",
      "\n",
      " epoch 1652 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 864us/step\n",
      "(for 1 minibatch) Training loss 0.0104425 | PSNR training 40.6232452\n",
      "\n",
      " epoch 1653 of 2000\n",
      "5/5 [==============================] - 0s 999us/step\n",
      "11/11 [==============================] - 0s 863us/step\n",
      "(for 1 minibatch) Training loss 0.0105839 | PSNR training 40.8480072\n",
      "\n",
      " epoch 1654 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 900us/step\n",
      "(for 1 minibatch) Training loss 0.0104950 | PSNR training 40.7438278\n",
      "\n",
      " epoch 1655 of 2000\n",
      "5/5 [==============================] - 0s 902us/step\n",
      "11/11 [==============================] - 0s 868us/step\n",
      "(for 1 minibatch) Training loss 0.0103823 | PSNR training 40.8256569\n",
      "\n",
      " epoch 1656 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 896us/step\n",
      "(for 1 minibatch) Training loss 0.0104672 | PSNR training 40.8072701\n",
      "\n",
      " epoch 1657 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 870us/step\n",
      "(for 1 minibatch) Training loss 0.0106853 | PSNR training 40.4751358\n",
      "\n",
      " epoch 1658 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 853us/step\n",
      "(for 1 minibatch) Training loss 0.0101796 | PSNR training 40.7665939\n",
      "\n",
      " epoch 1659 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 901us/step\n",
      "(for 1 minibatch) Training loss 0.0104353 | PSNR training 40.8438187\n",
      "\n",
      " epoch 1660 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 921us/step\n",
      "(for 1 minibatch) Training loss 0.0102786 | PSNR training 40.7460632\n",
      "\n",
      " epoch 1661 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 933us/step\n",
      "(for 1 minibatch) Training loss 0.0105219 | PSNR training 40.6963043\n",
      "\n",
      " epoch 1662 of 2000\n",
      "5/5 [==============================] - 0s 979us/step\n",
      "11/11 [==============================] - 0s 911us/step\n",
      "(for 1 minibatch) Training loss 0.0103608 | PSNR training 40.7540741\n",
      "\n",
      " epoch 1663 of 2000\n",
      "5/5 [==============================] - 0s 972us/step\n",
      "11/11 [==============================] - 0s 859us/step\n",
      "(for 1 minibatch) Training loss 0.0102089 | PSNR training 40.9616089\n",
      "\n",
      " epoch 1664 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 902us/step\n",
      "(for 1 minibatch) Training loss 0.0105574 | PSNR training 40.8524933\n",
      "\n",
      " epoch 1665 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 983us/step\n",
      "(for 1 minibatch) Training loss 0.0104555 | PSNR training 40.7195740\n",
      "\n",
      " epoch 1666 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 891us/step\n",
      "(for 1 minibatch) Training loss 0.0102229 | PSNR training 40.7724380\n",
      "\n",
      " epoch 1667 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 925us/step\n",
      "(for 1 minibatch) Training loss 0.0106211 | PSNR training 40.6473999\n",
      "\n",
      " epoch 1668 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 967us/step\n",
      "(for 1 minibatch) Training loss 0.0106331 | PSNR training 40.7126160\n",
      "\n",
      " epoch 1669 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 901us/step\n",
      "(for 1 minibatch) Training loss 0.0105730 | PSNR training 40.6923828\n",
      "\n",
      " epoch 1670 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 849us/step\n",
      "(for 1 minibatch) Training loss 0.0102161 | PSNR training 40.8059235\n",
      "\n",
      " epoch 1671 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 849us/step\n",
      "(for 1 minibatch) Training loss 0.0102299 | PSNR training 40.8294716\n",
      "\n",
      " epoch 1672 of 2000\n",
      "5/5 [==============================] - 0s 955us/step\n",
      "11/11 [==============================] - 0s 892us/step\n",
      "(for 1 minibatch) Training loss 0.0107474 | PSNR training 40.6103210\n",
      "\n",
      " epoch 1673 of 2000\n",
      "5/5 [==============================] - 0s 993us/step\n",
      "11/11 [==============================] - 0s 908us/step\n",
      "(for 1 minibatch) Training loss 0.0102512 | PSNR training 41.0361061\n",
      "\n",
      " epoch 1674 of 2000\n",
      "5/5 [==============================] - 0s 992us/step\n",
      "11/11 [==============================] - 0s 903us/step\n",
      "(for 1 minibatch) Training loss 0.0105291 | PSNR training 40.4013748\n",
      "\n",
      " epoch 1675 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 861us/step\n",
      "(for 1 minibatch) Training loss 0.0103169 | PSNR training 40.9596939\n",
      "\n",
      " epoch 1676 of 2000\n",
      "5/5 [==============================] - 0s 931us/step\n",
      "11/11 [==============================] - 0s 881us/step\n",
      "(for 1 minibatch) Training loss 0.0100898 | PSNR training 40.7855835\n",
      "\n",
      " epoch 1677 of 2000\n",
      "5/5 [==============================] - 0s 974us/step\n",
      "11/11 [==============================] - 0s 903us/step\n",
      "(for 1 minibatch) Training loss 0.0105636 | PSNR training 40.7661095\n",
      "\n",
      " epoch 1678 of 2000\n",
      "5/5 [==============================] - 0s 957us/step\n",
      "11/11 [==============================] - 0s 939us/step\n",
      "(for 1 minibatch) Training loss 0.0105730 | PSNR training 40.6916847\n",
      "\n",
      " epoch 1679 of 2000\n",
      "5/5 [==============================] - 0s 973us/step\n",
      "11/11 [==============================] - 0s 893us/step\n",
      "(for 1 minibatch) Training loss 0.0104671 | PSNR training 40.6918755\n",
      "\n",
      " epoch 1680 of 2000\n",
      "5/5 [==============================] - 0s 960us/step\n",
      "11/11 [==============================] - 0s 897us/step\n",
      "(for 1 minibatch) Training loss 0.0105360 | PSNR training 40.7122879\n",
      "\n",
      " epoch 1681 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 880us/step\n",
      "(for 1 minibatch) Training loss 0.0106105 | PSNR training 40.7734718\n",
      "\n",
      " epoch 1682 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 832us/step\n",
      "(for 1 minibatch) Training loss 0.0102674 | PSNR training 40.7413940\n",
      "\n",
      " epoch 1683 of 2000\n",
      "5/5 [==============================] - 0s 2ms/step\n",
      "11/11 [==============================] - 0s 1ms/step\n",
      "(for 1 minibatch) Training loss 0.0104720 | PSNR training 40.7573891\n",
      "\n",
      " epoch 1684 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 879us/step\n",
      "(for 1 minibatch) Training loss 0.0106935 | PSNR training 40.6376114\n",
      "\n",
      " epoch 1685 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 996us/step\n",
      "(for 1 minibatch) Training loss 0.0105729 | PSNR training 40.7372856\n",
      "\n",
      " epoch 1686 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 909us/step\n",
      "(for 1 minibatch) Training loss 0.0104582 | PSNR training 40.7093811\n",
      "\n",
      " epoch 1687 of 2000\n",
      "5/5 [==============================] - 0s 944us/step\n",
      "11/11 [==============================] - 0s 893us/step\n",
      "(for 1 minibatch) Training loss 0.0102533 | PSNR training 40.9252129\n",
      "\n",
      " epoch 1688 of 2000\n",
      "5/5 [==============================] - 0s 994us/step\n",
      "11/11 [==============================] - 0s 926us/step\n",
      "(for 1 minibatch) Training loss 0.0105829 | PSNR training 40.5112648\n",
      "\n",
      " epoch 1689 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 899us/step\n",
      "(for 1 minibatch) Training loss 0.0103571 | PSNR training 40.8713493\n",
      "\n",
      " epoch 1690 of 2000\n",
      "5/5 [==============================] - 0s 955us/step\n",
      "11/11 [==============================] - 0s 855us/step\n",
      "(for 1 minibatch) Training loss 0.0102894 | PSNR training 40.9679337\n",
      "\n",
      " epoch 1691 of 2000\n",
      "5/5 [==============================] - 0s 975us/step\n",
      "11/11 [==============================] - 0s 919us/step\n",
      "(for 1 minibatch) Training loss 0.0105212 | PSNR training 40.8083115\n",
      "\n",
      " epoch 1692 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 879us/step\n",
      "(for 1 minibatch) Training loss 0.0105251 | PSNR training 40.8045006\n",
      "\n",
      " epoch 1693 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 894us/step\n",
      "(for 1 minibatch) Training loss 0.0103632 | PSNR training 40.8692093\n",
      "\n",
      " epoch 1694 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 993us/step\n",
      "(for 1 minibatch) Training loss 0.0103444 | PSNR training 40.8702126\n",
      "\n",
      " epoch 1695 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 954us/step\n",
      "(for 1 minibatch) Training loss 0.0102694 | PSNR training 40.8378067\n",
      "\n",
      " epoch 1696 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 864us/step\n",
      "(for 1 minibatch) Training loss 0.0102375 | PSNR training 40.9410896\n",
      "\n",
      " epoch 1697 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 903us/step\n",
      "(for 1 minibatch) Training loss 0.0104934 | PSNR training 40.6648560\n",
      "\n",
      " epoch 1698 of 2000\n",
      "5/5 [==============================] - 0s 979us/step\n",
      "11/11 [==============================] - 0s 958us/step\n",
      "(for 1 minibatch) Training loss 0.0101463 | PSNR training 40.9538727\n",
      "\n",
      " epoch 1699 of 2000\n",
      "5/5 [==============================] - 0s 980us/step\n",
      "11/11 [==============================] - 0s 866us/step\n",
      "(for 1 minibatch) Training loss 0.0100532 | PSNR training 41.0796051\n",
      "\n",
      " epoch 1700 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 910us/step\n",
      "(for 1 minibatch) Training loss 0.0103121 | PSNR training 40.8786316\n",
      "\n",
      " epoch 1701 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 920us/step\n",
      "(for 1 minibatch) Training loss 0.0106953 | PSNR training 40.6551170\n",
      "\n",
      " epoch 1702 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 992us/step\n",
      "(for 1 minibatch) Training loss 0.0099708 | PSNR training 41.0376129\n",
      "\n",
      " epoch 1703 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 891us/step\n",
      "(for 1 minibatch) Training loss 0.0106273 | PSNR training 40.6300774\n",
      "\n",
      " epoch 1704 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 850us/step\n",
      "(for 1 minibatch) Training loss 0.0102047 | PSNR training 40.9881935\n",
      "\n",
      " epoch 1705 of 2000\n",
      "5/5 [==============================] - 0s 935us/step\n",
      "11/11 [==============================] - 0s 931us/step\n",
      "(for 1 minibatch) Training loss 0.0103655 | PSNR training 40.8086967\n",
      "\n",
      " epoch 1706 of 2000\n",
      "5/5 [==============================] - 0s 1000us/step\n",
      "11/11 [==============================] - 0s 837us/step\n",
      "(for 1 minibatch) Training loss 0.0102515 | PSNR training 40.9092255\n",
      "\n",
      " epoch 1707 of 2000\n",
      "5/5 [==============================] - 0s 928us/step\n",
      "11/11 [==============================] - 0s 870us/step\n",
      "(for 1 minibatch) Training loss 0.0106411 | PSNR training 40.5566978\n",
      "\n",
      " epoch 1708 of 2000\n",
      "5/5 [==============================] - 0s 907us/step\n",
      "11/11 [==============================] - 0s 883us/step\n",
      "(for 1 minibatch) Training loss 0.0102295 | PSNR training 40.8760147\n",
      "\n",
      " epoch 1709 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 848us/step\n",
      "(for 1 minibatch) Training loss 0.0104304 | PSNR training 40.9452400\n",
      "\n",
      " epoch 1710 of 2000\n",
      "5/5 [==============================] - 0s 947us/step\n",
      "11/11 [==============================] - 0s 836us/step\n",
      "(for 1 minibatch) Training loss 0.0104045 | PSNR training 40.8531761\n",
      "\n",
      " epoch 1711 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 903us/step\n",
      "(for 1 minibatch) Training loss 0.0104008 | PSNR training 40.9779129\n",
      "\n",
      " epoch 1712 of 2000\n",
      "5/5 [==============================] - 0s 930us/step\n",
      "11/11 [==============================] - 0s 878us/step\n",
      "(for 1 minibatch) Training loss 0.0104088 | PSNR training 40.5536118\n",
      "\n",
      " epoch 1713 of 2000\n",
      "5/5 [==============================] - 0s 956us/step\n",
      "11/11 [==============================] - 0s 925us/step\n",
      "(for 1 minibatch) Training loss 0.0104613 | PSNR training 40.8695564\n",
      "\n",
      " epoch 1714 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 859us/step\n",
      "(for 1 minibatch) Training loss 0.0102088 | PSNR training 41.0604057\n",
      "\n",
      " epoch 1715 of 2000\n",
      "5/5 [==============================] - 0s 970us/step\n",
      "11/11 [==============================] - 0s 875us/step\n",
      "(for 1 minibatch) Training loss 0.0102600 | PSNR training 40.8629570\n",
      "\n",
      " epoch 1716 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 899us/step\n",
      "(for 1 minibatch) Training loss 0.0104609 | PSNR training 40.8729935\n",
      "\n",
      " epoch 1717 of 2000\n",
      "5/5 [==============================] - 0s 958us/step\n",
      "11/11 [==============================] - 0s 851us/step\n",
      "(for 1 minibatch) Training loss 0.0103885 | PSNR training 40.8206711\n",
      "\n",
      " epoch 1718 of 2000\n",
      "5/5 [==============================] - 0s 954us/step\n",
      "11/11 [==============================] - 0s 888us/step\n",
      "(for 1 minibatch) Training loss 0.0102589 | PSNR training 40.9450073\n",
      "\n",
      " epoch 1719 of 2000\n",
      "5/5 [==============================] - 0s 918us/step\n",
      "11/11 [==============================] - 0s 886us/step\n",
      "(for 1 minibatch) Training loss 0.0105897 | PSNR training 40.6605492\n",
      "\n",
      " epoch 1720 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 879us/step\n",
      "(for 1 minibatch) Training loss 0.0106417 | PSNR training 40.5460434\n",
      "\n",
      " epoch 1721 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 916us/step\n",
      "(for 1 minibatch) Training loss 0.0102853 | PSNR training 40.9529724\n",
      "\n",
      " epoch 1722 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 900us/step\n",
      "(for 1 minibatch) Training loss 0.0101754 | PSNR training 40.6404839\n",
      "\n",
      " epoch 1723 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 875us/step\n",
      "(for 1 minibatch) Training loss 0.0098889 | PSNR training 41.1868858\n",
      "\n",
      " epoch 1724 of 2000\n",
      "5/5 [==============================] - 0s 977us/step\n",
      "11/11 [==============================] - 0s 959us/step\n",
      "(for 1 minibatch) Training loss 0.0102502 | PSNR training 40.7239151\n",
      "\n",
      " epoch 1725 of 2000\n",
      "5/5 [==============================] - 0s 976us/step\n",
      "11/11 [==============================] - 0s 878us/step\n",
      "(for 1 minibatch) Training loss 0.0104622 | PSNR training 40.6718826\n",
      "\n",
      " epoch 1726 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 844us/step\n",
      "(for 1 minibatch) Training loss 0.0101507 | PSNR training 41.1383514\n",
      "\n",
      " epoch 1727 of 2000\n",
      "5/5 [==============================] - 0s 950us/step\n",
      "11/11 [==============================] - 0s 851us/step\n",
      "(for 1 minibatch) Training loss 0.0102799 | PSNR training 41.0784988\n",
      "\n",
      " epoch 1728 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 904us/step\n",
      "(for 1 minibatch) Training loss 0.0100693 | PSNR training 40.8706093\n",
      "\n",
      " epoch 1729 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 905us/step\n",
      "(for 1 minibatch) Training loss 0.0105589 | PSNR training 40.8046646\n",
      "\n",
      " epoch 1730 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 872us/step\n",
      "(for 1 minibatch) Training loss 0.0102530 | PSNR training 40.6817970\n",
      "\n",
      " epoch 1731 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 864us/step\n",
      "(for 1 minibatch) Training loss 0.0105609 | PSNR training 40.6250725\n",
      "\n",
      " epoch 1732 of 2000\n",
      "5/5 [==============================] - 0s 921us/step\n",
      "11/11 [==============================] - 0s 878us/step\n",
      "(for 1 minibatch) Training loss 0.0103740 | PSNR training 40.7697105\n",
      "\n",
      " epoch 1733 of 2000\n",
      "5/5 [==============================] - 0s 940us/step\n",
      "11/11 [==============================] - 0s 877us/step\n",
      "(for 1 minibatch) Training loss 0.0103931 | PSNR training 40.7482605\n",
      "\n",
      " epoch 1734 of 2000\n",
      "5/5 [==============================] - 0s 991us/step\n",
      "11/11 [==============================] - 0s 884us/step\n",
      "(for 1 minibatch) Training loss 0.0101514 | PSNR training 40.7537155\n",
      "\n",
      " epoch 1735 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 1ms/step\n",
      "(for 1 minibatch) Training loss 0.0105937 | PSNR training 40.7578621\n",
      "\n",
      " epoch 1736 of 2000\n",
      "5/5 [==============================] - 0s 971us/step\n",
      "11/11 [==============================] - 0s 924us/step\n",
      "(for 1 minibatch) Training loss 0.0103557 | PSNR training 40.8713493\n",
      "\n",
      " epoch 1737 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 910us/step\n",
      "(for 1 minibatch) Training loss 0.0106706 | PSNR training 40.6716118\n",
      "\n",
      " epoch 1738 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 906us/step\n",
      "(for 1 minibatch) Training loss 0.0102996 | PSNR training 40.8298531\n",
      "\n",
      " epoch 1739 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 907us/step\n",
      "(for 1 minibatch) Training loss 0.0103693 | PSNR training 40.6801796\n",
      "\n",
      " epoch 1740 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 857us/step\n",
      "(for 1 minibatch) Training loss 0.0104315 | PSNR training 40.8519173\n",
      "\n",
      " epoch 1741 of 2000\n",
      "5/5 [==============================] - 0s 979us/step\n",
      "11/11 [==============================] - 0s 898us/step\n",
      "(for 1 minibatch) Training loss 0.0105134 | PSNR training 40.7710724\n",
      "\n",
      " epoch 1742 of 2000\n",
      "5/5 [==============================] - 0s 967us/step\n",
      "11/11 [==============================] - 0s 911us/step\n",
      "(for 1 minibatch) Training loss 0.0105169 | PSNR training 40.8185959\n",
      "\n",
      " epoch 1743 of 2000\n",
      "5/5 [==============================] - 0s 912us/step\n",
      "11/11 [==============================] - 0s 835us/step\n",
      "(for 1 minibatch) Training loss 0.0102722 | PSNR training 40.7433281\n",
      "\n",
      " epoch 1744 of 2000\n",
      "5/5 [==============================] - 0s 969us/step\n",
      "11/11 [==============================] - 0s 939us/step\n",
      "(for 1 minibatch) Training loss 0.0105349 | PSNR training 40.7316971\n",
      "\n",
      " epoch 1745 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 908us/step\n",
      "(for 1 minibatch) Training loss 0.0102811 | PSNR training 40.8722992\n",
      "\n",
      " epoch 1746 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 848us/step\n",
      "(for 1 minibatch) Training loss 0.0102004 | PSNR training 40.9563675\n",
      "\n",
      " epoch 1747 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 857us/step\n",
      "(for 1 minibatch) Training loss 0.0101981 | PSNR training 40.7809601\n",
      "\n",
      " epoch 1748 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 834us/step\n",
      "(for 1 minibatch) Training loss 0.0105887 | PSNR training 40.6529655\n",
      "\n",
      " epoch 1749 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 862us/step\n",
      "(for 1 minibatch) Training loss 0.0102264 | PSNR training 40.7300034\n",
      "\n",
      " epoch 1750 of 2000\n",
      "5/5 [==============================] - 0s 893us/step\n",
      "11/11 [==============================] - 0s 851us/step\n",
      "(for 1 minibatch) Training loss 0.0103565 | PSNR training 40.8127365\n",
      "\n",
      " epoch 1751 of 2000\n",
      "5/5 [==============================] - 0s 939us/step\n",
      "11/11 [==============================] - 0s 824us/step\n",
      "(for 1 minibatch) Training loss 0.0103604 | PSNR training 40.8220749\n",
      "\n",
      " epoch 1752 of 2000\n",
      "5/5 [==============================] - 0s 954us/step\n",
      "11/11 [==============================] - 0s 906us/step\n",
      "(for 1 minibatch) Training loss 0.0101973 | PSNR training 40.8076439\n",
      "\n",
      " epoch 1753 of 2000\n",
      "5/5 [==============================] - 0s 978us/step\n",
      "11/11 [==============================] - 0s 861us/step\n",
      "(for 1 minibatch) Training loss 0.0102654 | PSNR training 40.9490776\n",
      "\n",
      " epoch 1754 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 834us/step\n",
      "(for 1 minibatch) Training loss 0.0104761 | PSNR training 40.6196938\n",
      "\n",
      " epoch 1755 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 865us/step\n",
      "(for 1 minibatch) Training loss 0.0101480 | PSNR training 41.0347214\n",
      "\n",
      " epoch 1756 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 875us/step\n",
      "(for 1 minibatch) Training loss 0.0105293 | PSNR training 40.6270447\n",
      "\n",
      " epoch 1757 of 2000\n",
      "5/5 [==============================] - 0s 987us/step\n",
      "11/11 [==============================] - 0s 827us/step\n",
      "(for 1 minibatch) Training loss 0.0103716 | PSNR training 40.5865402\n",
      "\n",
      " epoch 1758 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 853us/step\n",
      "(for 1 minibatch) Training loss 0.0105296 | PSNR training 40.8509674\n",
      "\n",
      " epoch 1759 of 2000\n",
      "5/5 [==============================] - 0s 980us/step\n",
      "11/11 [==============================] - 0s 843us/step\n",
      "(for 1 minibatch) Training loss 0.0102043 | PSNR training 40.9750595\n",
      "\n",
      " epoch 1760 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 925us/step\n",
      "(for 1 minibatch) Training loss 0.0101151 | PSNR training 41.0238724\n",
      "\n",
      " epoch 1761 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 947us/step\n",
      "(for 1 minibatch) Training loss 0.0101058 | PSNR training 40.8451424\n",
      "\n",
      " epoch 1762 of 2000\n",
      "5/5 [==============================] - 0s 965us/step\n",
      "11/11 [==============================] - 0s 888us/step\n",
      "(for 1 minibatch) Training loss 0.0102068 | PSNR training 41.0278549\n",
      "\n",
      " epoch 1763 of 2000\n",
      "5/5 [==============================] - 0s 961us/step\n",
      "11/11 [==============================] - 0s 869us/step\n",
      "(for 1 minibatch) Training loss 0.0102420 | PSNR training 40.7833672\n",
      "\n",
      " epoch 1764 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 846us/step\n",
      "(for 1 minibatch) Training loss 0.0103096 | PSNR training 40.9439468\n",
      "\n",
      " epoch 1765 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 884us/step\n",
      "(for 1 minibatch) Training loss 0.0105108 | PSNR training 40.5561752\n",
      "\n",
      " epoch 1766 of 2000\n",
      "5/5 [==============================] - 0s 950us/step\n",
      "11/11 [==============================] - 0s 908us/step\n",
      "(for 1 minibatch) Training loss 0.0104011 | PSNR training 40.8537216\n",
      "\n",
      " epoch 1767 of 2000\n",
      "5/5 [==============================] - 0s 950us/step\n",
      "11/11 [==============================] - 0s 876us/step\n",
      "(for 1 minibatch) Training loss 0.0099450 | PSNR training 41.0378685\n",
      "\n",
      " epoch 1768 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 832us/step\n",
      "(for 1 minibatch) Training loss 0.0107323 | PSNR training 40.7887802\n",
      "\n",
      " epoch 1769 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 797us/step\n",
      "(for 1 minibatch) Training loss 0.0107304 | PSNR training 40.7685776\n",
      "\n",
      " epoch 1770 of 2000\n",
      "5/5 [==============================] - 0s 979us/step\n",
      "11/11 [==============================] - 0s 845us/step\n",
      "(for 1 minibatch) Training loss 0.0105683 | PSNR training 40.7290688\n",
      "\n",
      " epoch 1771 of 2000\n",
      "5/5 [==============================] - 0s 978us/step\n",
      "11/11 [==============================] - 0s 890us/step\n",
      "(for 1 minibatch) Training loss 0.0103694 | PSNR training 40.6368866\n",
      "\n",
      " epoch 1772 of 2000\n",
      "5/5 [==============================] - 0s 992us/step\n",
      "11/11 [==============================] - 0s 863us/step\n",
      "(for 1 minibatch) Training loss 0.0105222 | PSNR training 40.7838593\n",
      "\n",
      " epoch 1773 of 2000\n",
      "5/5 [==============================] - 0s 983us/step\n",
      "11/11 [==============================] - 0s 866us/step\n",
      "(for 1 minibatch) Training loss 0.0102289 | PSNR training 40.7766380\n",
      "\n",
      " epoch 1774 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 915us/step\n",
      "(for 1 minibatch) Training loss 0.0105206 | PSNR training 40.7880211\n",
      "\n",
      " epoch 1775 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 859us/step\n",
      "(for 1 minibatch) Training loss 0.0105140 | PSNR training 40.7661209\n",
      "\n",
      " epoch 1776 of 2000\n",
      "5/5 [==============================] - 0s 931us/step\n",
      "11/11 [==============================] - 0s 879us/step\n",
      "(for 1 minibatch) Training loss 0.0105232 | PSNR training 40.7261047\n",
      "\n",
      " epoch 1777 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 907us/step\n",
      "(for 1 minibatch) Training loss 0.0104929 | PSNR training 40.5550003\n",
      "\n",
      " epoch 1778 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 1ms/step\n",
      "(for 1 minibatch) Training loss 0.0101359 | PSNR training 40.9199944\n",
      "\n",
      " epoch 1779 of 2000\n",
      "5/5 [==============================] - 0s 937us/step\n",
      "11/11 [==============================] - 0s 862us/step\n",
      "(for 1 minibatch) Training loss 0.0102492 | PSNR training 40.9821091\n",
      "\n",
      " epoch 1780 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 856us/step\n",
      "(for 1 minibatch) Training loss 0.0100413 | PSNR training 40.9801178\n",
      "\n",
      " epoch 1781 of 2000\n",
      "5/5 [==============================] - 0s 992us/step\n",
      "11/11 [==============================] - 0s 906us/step\n",
      "(for 1 minibatch) Training loss 0.0101220 | PSNR training 40.8775291\n",
      "\n",
      " epoch 1782 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 887us/step\n",
      "(for 1 minibatch) Training loss 0.0103401 | PSNR training 40.6537628\n",
      "\n",
      " epoch 1783 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 837us/step\n",
      "(for 1 minibatch) Training loss 0.0105196 | PSNR training 40.8731804\n",
      "\n",
      " epoch 1784 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 966us/step\n",
      "(for 1 minibatch) Training loss 0.0106692 | PSNR training 40.5343971\n",
      "\n",
      " epoch 1785 of 2000\n",
      "5/5 [==============================] - 0s 985us/step\n",
      "11/11 [==============================] - 0s 943us/step\n",
      "(for 1 minibatch) Training loss 0.0103555 | PSNR training 41.0106239\n",
      "\n",
      " epoch 1786 of 2000\n",
      "5/5 [==============================] - 0s 949us/step\n",
      "11/11 [==============================] - 0s 848us/step\n",
      "(for 1 minibatch) Training loss 0.0101219 | PSNR training 40.8846321\n",
      "\n",
      " epoch 1787 of 2000\n",
      "5/5 [==============================] - 0s 972us/step\n",
      "11/11 [==============================] - 0s 901us/step\n",
      "(for 1 minibatch) Training loss 0.0102510 | PSNR training 40.8023376\n",
      "\n",
      " epoch 1788 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 867us/step\n",
      "(for 1 minibatch) Training loss 0.0102630 | PSNR training 40.8506012\n",
      "\n",
      " epoch 1789 of 2000\n",
      "5/5 [==============================] - 0s 994us/step\n",
      "11/11 [==============================] - 0s 891us/step\n",
      "(for 1 minibatch) Training loss 0.0100066 | PSNR training 41.0238304\n",
      "\n",
      " epoch 1790 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 866us/step\n",
      "(for 1 minibatch) Training loss 0.0104187 | PSNR training 40.8541794\n",
      "\n",
      " epoch 1791 of 2000\n",
      "5/5 [==============================] - 0s 999us/step\n",
      "11/11 [==============================] - 0s 983us/step\n",
      "(for 1 minibatch) Training loss 0.0106982 | PSNR training 40.9280815\n",
      "\n",
      " epoch 1792 of 2000\n",
      "5/5 [==============================] - 0s 969us/step\n",
      "11/11 [==============================] - 0s 961us/step\n",
      "(for 1 minibatch) Training loss 0.0105354 | PSNR training 40.6573410\n",
      "\n",
      " epoch 1793 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 861us/step\n",
      "(for 1 minibatch) Training loss 0.0104465 | PSNR training 40.7342949\n",
      "\n",
      " epoch 1794 of 2000\n",
      "5/5 [==============================] - 0s 977us/step\n",
      "11/11 [==============================] - 0s 864us/step\n",
      "(for 1 minibatch) Training loss 0.0103577 | PSNR training 40.6581573\n",
      "\n",
      " epoch 1795 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 853us/step\n",
      "(for 1 minibatch) Training loss 0.0102693 | PSNR training 40.9467773\n",
      "\n",
      " epoch 1796 of 2000\n",
      "5/5 [==============================] - 0s 992us/step\n",
      "11/11 [==============================] - 0s 930us/step\n",
      "(for 1 minibatch) Training loss 0.0101558 | PSNR training 41.0537186\n",
      "\n",
      " epoch 1797 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 878us/step\n",
      "(for 1 minibatch) Training loss 0.0098645 | PSNR training 41.0139656\n",
      "\n",
      " epoch 1798 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 897us/step\n",
      "(for 1 minibatch) Training loss 0.0105425 | PSNR training 40.7134285\n",
      "\n",
      " epoch 1799 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 1ms/step\n",
      "(for 1 minibatch) Training loss 0.0102377 | PSNR training 40.8891449\n",
      "\n",
      " epoch 1800 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 842us/step\n",
      "(for 1 minibatch) Training loss 0.0103422 | PSNR training 40.6756248\n",
      "\n",
      " epoch 1801 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 874us/step\n",
      "(for 1 minibatch) Training loss 0.0101963 | PSNR training 40.8001671\n",
      "\n",
      " epoch 1802 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 945us/step\n",
      "(for 1 minibatch) Training loss 0.0101604 | PSNR training 40.9766998\n",
      "\n",
      " epoch 1803 of 2000\n",
      "5/5 [==============================] - 0s 993us/step\n",
      "11/11 [==============================] - 0s 887us/step\n",
      "(for 1 minibatch) Training loss 0.0102413 | PSNR training 40.7256622\n",
      "\n",
      " epoch 1804 of 2000\n",
      "5/5 [==============================] - 0s 942us/step\n",
      "11/11 [==============================] - 0s 822us/step\n",
      "(for 1 minibatch) Training loss 0.0100329 | PSNR training 41.1018982\n",
      "\n",
      " epoch 1805 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 818us/step\n",
      "(for 1 minibatch) Training loss 0.0104089 | PSNR training 40.8028030\n",
      "\n",
      " epoch 1806 of 2000\n",
      "5/5 [==============================] - 0s 963us/step\n",
      "11/11 [==============================] - 0s 834us/step\n",
      "(for 1 minibatch) Training loss 0.0105246 | PSNR training 40.8491325\n",
      "\n",
      " epoch 1807 of 2000\n",
      "5/5 [==============================] - 0s 994us/step\n",
      "11/11 [==============================] - 0s 876us/step\n",
      "(for 1 minibatch) Training loss 0.0105046 | PSNR training 40.7583389\n",
      "\n",
      " epoch 1808 of 2000\n",
      "5/5 [==============================] - 0s 965us/step\n",
      "11/11 [==============================] - 0s 842us/step\n",
      "(for 1 minibatch) Training loss 0.0100874 | PSNR training 41.0841408\n",
      "\n",
      " epoch 1809 of 2000\n",
      "5/5 [==============================] - 0s 958us/step\n",
      "11/11 [==============================] - 0s 870us/step\n",
      "(for 1 minibatch) Training loss 0.0102110 | PSNR training 40.8920479\n",
      "\n",
      " epoch 1810 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 944us/step\n",
      "(for 1 minibatch) Training loss 0.0103152 | PSNR training 40.9532890\n",
      "\n",
      " epoch 1811 of 2000\n",
      "5/5 [==============================] - 0s 925us/step\n",
      "11/11 [==============================] - 0s 898us/step\n",
      "(for 1 minibatch) Training loss 0.0100186 | PSNR training 41.1391449\n",
      "\n",
      " epoch 1812 of 2000\n",
      "5/5 [==============================] - 0s 996us/step\n",
      "11/11 [==============================] - 0s 829us/step\n",
      "(for 1 minibatch) Training loss 0.0102285 | PSNR training 40.8594055\n",
      "\n",
      " epoch 1813 of 2000\n",
      "5/5 [==============================] - 0s 942us/step\n",
      "11/11 [==============================] - 0s 899us/step\n",
      "(for 1 minibatch) Training loss 0.0100130 | PSNR training 41.1482544\n",
      "\n",
      " epoch 1814 of 2000\n",
      "5/5 [==============================] - 0s 933us/step\n",
      "11/11 [==============================] - 0s 894us/step\n",
      "(for 1 minibatch) Training loss 0.0100967 | PSNR training 41.0159874\n",
      "\n",
      " epoch 1815 of 2000\n",
      "5/5 [==============================] - 0s 986us/step\n",
      "11/11 [==============================] - 0s 866us/step\n",
      "(for 1 minibatch) Training loss 0.0100022 | PSNR training 41.0011597\n",
      "\n",
      " epoch 1816 of 2000\n",
      "5/5 [==============================] - 0s 911us/step\n",
      "11/11 [==============================] - 0s 928us/step\n",
      "(for 1 minibatch) Training loss 0.0102376 | PSNR training 40.9995728\n",
      "\n",
      " epoch 1817 of 2000\n",
      "5/5 [==============================] - 0s 975us/step\n",
      "11/11 [==============================] - 0s 889us/step\n",
      "(for 1 minibatch) Training loss 0.0098569 | PSNR training 40.9327393\n",
      "\n",
      " epoch 1818 of 2000\n",
      "5/5 [==============================] - 0s 967us/step\n",
      "11/11 [==============================] - 0s 869us/step\n",
      "(for 1 minibatch) Training loss 0.0104683 | PSNR training 40.8884010\n",
      "\n",
      " epoch 1819 of 2000\n",
      "5/5 [==============================] - 0s 984us/step\n",
      "11/11 [==============================] - 0s 952us/step\n",
      "(for 1 minibatch) Training loss 0.0100278 | PSNR training 40.9989853\n",
      "\n",
      " epoch 1820 of 2000\n",
      "5/5 [==============================] - 0s 908us/step\n",
      "11/11 [==============================] - 0s 873us/step\n",
      "(for 1 minibatch) Training loss 0.0101871 | PSNR training 40.9515076\n",
      "\n",
      " epoch 1821 of 2000\n",
      "5/5 [==============================] - 0s 946us/step\n",
      "11/11 [==============================] - 0s 896us/step\n",
      "(for 1 minibatch) Training loss 0.0102239 | PSNR training 41.2173767\n",
      "\n",
      " epoch 1822 of 2000\n",
      "5/5 [==============================] - 0s 997us/step\n",
      "11/11 [==============================] - 0s 865us/step\n",
      "(for 1 minibatch) Training loss 0.0101129 | PSNR training 40.8577423\n",
      "\n",
      " epoch 1823 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 928us/step\n",
      "(for 1 minibatch) Training loss 0.0101162 | PSNR training 40.9940796\n",
      "\n",
      " epoch 1824 of 2000\n",
      "5/5 [==============================] - 0s 945us/step\n",
      "11/11 [==============================] - 0s 895us/step\n",
      "(for 1 minibatch) Training loss 0.0101048 | PSNR training 40.9495163\n",
      "\n",
      " epoch 1825 of 2000\n",
      "5/5 [==============================] - 0s 964us/step\n",
      "11/11 [==============================] - 0s 898us/step\n",
      "(for 1 minibatch) Training loss 0.0102274 | PSNR training 40.8171234\n",
      "\n",
      " epoch 1826 of 2000\n",
      "5/5 [==============================] - 0s 966us/step\n",
      "11/11 [==============================] - 0s 873us/step\n",
      "(for 1 minibatch) Training loss 0.0101351 | PSNR training 41.0178223\n",
      "\n",
      " epoch 1827 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 864us/step\n",
      "(for 1 minibatch) Training loss 0.0103121 | PSNR training 41.0222321\n",
      "\n",
      " epoch 1828 of 2000\n",
      "5/5 [==============================] - 0s 877us/step\n",
      "11/11 [==============================] - 0s 890us/step\n",
      "(for 1 minibatch) Training loss 0.0098855 | PSNR training 41.1085739\n",
      "\n",
      " epoch 1829 of 2000\n",
      "5/5 [==============================] - 0s 969us/step\n",
      "11/11 [==============================] - 0s 872us/step\n",
      "(for 1 minibatch) Training loss 0.0102485 | PSNR training 40.9925690\n",
      "\n",
      " epoch 1830 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 918us/step\n",
      "(for 1 minibatch) Training loss 0.0099763 | PSNR training 41.0985107\n",
      "\n",
      " epoch 1831 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 1ms/step\n",
      "(for 1 minibatch) Training loss 0.0101545 | PSNR training 41.1353645\n",
      "\n",
      " epoch 1832 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 970us/step\n",
      "(for 1 minibatch) Training loss 0.0103208 | PSNR training 40.9983482\n",
      "\n",
      " epoch 1833 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 861us/step\n",
      "(for 1 minibatch) Training loss 0.0105499 | PSNR training 40.8638458\n",
      "\n",
      " epoch 1834 of 2000\n",
      "5/5 [==============================] - 0s 973us/step\n",
      "11/11 [==============================] - 0s 841us/step\n",
      "(for 1 minibatch) Training loss 0.0102233 | PSNR training 40.8625031\n",
      "\n",
      " epoch 1835 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 979us/step\n",
      "(for 1 minibatch) Training loss 0.0104516 | PSNR training 40.7245712\n",
      "\n",
      " epoch 1836 of 2000\n",
      "5/5 [==============================] - 0s 969us/step\n",
      "11/11 [==============================] - 0s 853us/step\n",
      "(for 1 minibatch) Training loss 0.0105048 | PSNR training 40.7674828\n",
      "\n",
      " epoch 1837 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 868us/step\n",
      "(for 1 minibatch) Training loss 0.0099755 | PSNR training 41.2262955\n",
      "\n",
      " epoch 1838 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 925us/step\n",
      "(for 1 minibatch) Training loss 0.0098770 | PSNR training 41.0517311\n",
      "\n",
      " epoch 1839 of 2000\n",
      "5/5 [==============================] - 0s 970us/step\n",
      "11/11 [==============================] - 0s 868us/step\n",
      "(for 1 minibatch) Training loss 0.0102364 | PSNR training 40.8793564\n",
      "\n",
      " epoch 1840 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 871us/step\n",
      "(for 1 minibatch) Training loss 0.0101384 | PSNR training 40.9810829\n",
      "\n",
      " epoch 1841 of 2000\n",
      "5/5 [==============================] - 0s 986us/step\n",
      "11/11 [==============================] - 0s 842us/step\n",
      "(for 1 minibatch) Training loss 0.0102634 | PSNR training 40.9292793\n",
      "\n",
      " epoch 1842 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 894us/step\n",
      "(for 1 minibatch) Training loss 0.0101024 | PSNR training 41.1218262\n",
      "\n",
      " epoch 1843 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 965us/step\n",
      "(for 1 minibatch) Training loss 0.0104436 | PSNR training 40.9382629\n",
      "\n",
      " epoch 1844 of 2000\n",
      "5/5 [==============================] - 0s 942us/step\n",
      "11/11 [==============================] - 0s 988us/step\n",
      "(for 1 minibatch) Training loss 0.0103810 | PSNR training 40.8483315\n",
      "\n",
      " epoch 1845 of 2000\n",
      "5/5 [==============================] - 0s 989us/step\n",
      "11/11 [==============================] - 0s 903us/step\n",
      "(for 1 minibatch) Training loss 0.0099402 | PSNR training 41.0424576\n",
      "\n",
      " epoch 1846 of 2000\n",
      "5/5 [==============================] - 0s 956us/step\n",
      "11/11 [==============================] - 0s 853us/step\n",
      "(for 1 minibatch) Training loss 0.0101398 | PSNR training 41.0396919\n",
      "\n",
      " epoch 1847 of 2000\n",
      "5/5 [==============================] - 0s 959us/step\n",
      "11/11 [==============================] - 0s 938us/step\n",
      "(for 1 minibatch) Training loss 0.0101533 | PSNR training 40.6305771\n",
      "\n",
      " epoch 1848 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 866us/step\n",
      "(for 1 minibatch) Training loss 0.0098529 | PSNR training 41.3030052\n",
      "\n",
      " epoch 1849 of 2000\n",
      "5/5 [==============================] - 0s 942us/step\n",
      "11/11 [==============================] - 0s 850us/step\n",
      "(for 1 minibatch) Training loss 0.0101311 | PSNR training 40.9894791\n",
      "\n",
      " epoch 1850 of 2000\n",
      "5/5 [==============================] - 0s 948us/step\n",
      "11/11 [==============================] - 0s 914us/step\n",
      "(for 1 minibatch) Training loss 0.0099784 | PSNR training 41.0009995\n",
      "\n",
      " epoch 1851 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 846us/step\n",
      "(for 1 minibatch) Training loss 0.0101138 | PSNR training 41.0388336\n",
      "\n",
      " epoch 1852 of 2000\n",
      "5/5 [==============================] - 0s 916us/step\n",
      "11/11 [==============================] - 0s 890us/step\n",
      "(for 1 minibatch) Training loss 0.0102746 | PSNR training 41.0454979\n",
      "\n",
      " epoch 1853 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 866us/step\n",
      "(for 1 minibatch) Training loss 0.0101316 | PSNR training 40.9952011\n",
      "\n",
      " epoch 1854 of 2000\n",
      "5/5 [==============================] - 0s 988us/step\n",
      "11/11 [==============================] - 0s 853us/step\n",
      "(for 1 minibatch) Training loss 0.0102457 | PSNR training 40.8975906\n",
      "\n",
      " epoch 1855 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 864us/step\n",
      "(for 1 minibatch) Training loss 0.0098731 | PSNR training 41.1964836\n",
      "\n",
      " epoch 1856 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 851us/step\n",
      "(for 1 minibatch) Training loss 0.0099694 | PSNR training 41.0893211\n",
      "\n",
      " epoch 1857 of 2000\n",
      "5/5 [==============================] - 0s 924us/step\n",
      "11/11 [==============================] - 0s 964us/step\n",
      "(for 1 minibatch) Training loss 0.0101438 | PSNR training 41.0343781\n",
      "\n",
      " epoch 1858 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 934us/step\n",
      "(for 1 minibatch) Training loss 0.0100327 | PSNR training 41.0643196\n",
      "\n",
      " epoch 1859 of 2000\n",
      "5/5 [==============================] - 0s 970us/step\n",
      "11/11 [==============================] - 0s 879us/step\n",
      "(for 1 minibatch) Training loss 0.0102201 | PSNR training 40.8296165\n",
      "\n",
      " epoch 1860 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 1ms/step\n",
      "(for 1 minibatch) Training loss 0.0102240 | PSNR training 40.8946800\n",
      "\n",
      " epoch 1861 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 1ms/step\n",
      "(for 1 minibatch) Training loss 0.0100045 | PSNR training 41.2023163\n",
      "\n",
      " epoch 1862 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 926us/step\n",
      "(for 1 minibatch) Training loss 0.0101367 | PSNR training 41.0266342\n",
      "\n",
      " epoch 1863 of 2000\n",
      "5/5 [==============================] - 0s 974us/step\n",
      "11/11 [==============================] - 0s 823us/step\n",
      "(for 1 minibatch) Training loss 0.0102850 | PSNR training 41.0317802\n",
      "\n",
      " epoch 1864 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 890us/step\n",
      "(for 1 minibatch) Training loss 0.0100785 | PSNR training 40.9793930\n",
      "\n",
      " epoch 1865 of 2000\n",
      "5/5 [==============================] - 0s 944us/step\n",
      "11/11 [==============================] - 0s 884us/step\n",
      "(for 1 minibatch) Training loss 0.0102531 | PSNR training 40.9376717\n",
      "\n",
      " epoch 1866 of 2000\n",
      "5/5 [==============================] - 0s 909us/step\n",
      "11/11 [==============================] - 0s 846us/step\n",
      "(for 1 minibatch) Training loss 0.0100349 | PSNR training 41.1996689\n",
      "\n",
      " epoch 1867 of 2000\n",
      "5/5 [==============================] - 0s 972us/step\n",
      "11/11 [==============================] - 0s 863us/step\n",
      "(for 1 minibatch) Training loss 0.0102221 | PSNR training 40.9622383\n",
      "\n",
      " epoch 1868 of 2000\n",
      "5/5 [==============================] - 0s 962us/step\n",
      "11/11 [==============================] - 0s 885us/step\n",
      "(for 1 minibatch) Training loss 0.0102098 | PSNR training 41.1927223\n",
      "\n",
      " epoch 1869 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 838us/step\n",
      "(for 1 minibatch) Training loss 0.0102181 | PSNR training 40.8588333\n",
      "\n",
      " epoch 1870 of 2000\n",
      "5/5 [==============================] - 0s 984us/step\n",
      "11/11 [==============================] - 0s 884us/step\n",
      "(for 1 minibatch) Training loss 0.0101636 | PSNR training 40.9738159\n",
      "\n",
      " epoch 1871 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 843us/step\n",
      "(for 1 minibatch) Training loss 0.0100574 | PSNR training 41.0903625\n",
      "\n",
      " epoch 1872 of 2000\n",
      "5/5 [==============================] - 0s 977us/step\n",
      "11/11 [==============================] - 0s 898us/step\n",
      "(for 1 minibatch) Training loss 0.0099724 | PSNR training 41.2403297\n",
      "\n",
      " epoch 1873 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 1ms/step\n",
      "(for 1 minibatch) Training loss 0.0100657 | PSNR training 40.9693985\n",
      "\n",
      " epoch 1874 of 2000\n",
      "5/5 [==============================] - 0s 975us/step\n",
      "11/11 [==============================] - 0s 946us/step\n",
      "(for 1 minibatch) Training loss 0.0098482 | PSNR training 41.0767899\n",
      "\n",
      " epoch 1875 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 1ms/step\n",
      "(for 1 minibatch) Training loss 0.0100481 | PSNR training 41.0429344\n",
      "\n",
      " epoch 1876 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 949us/step\n",
      "(for 1 minibatch) Training loss 0.0105273 | PSNR training 40.9733353\n",
      "\n",
      " epoch 1877 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 991us/step\n",
      "(for 1 minibatch) Training loss 0.0100029 | PSNR training 40.8862762\n",
      "\n",
      " epoch 1878 of 2000\n",
      "5/5 [==============================] - 0s 974us/step\n",
      "11/11 [==============================] - 0s 886us/step\n",
      "(for 1 minibatch) Training loss 0.0101725 | PSNR training 40.9658394\n",
      "\n",
      " epoch 1879 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 925us/step\n",
      "(for 1 minibatch) Training loss 0.0101209 | PSNR training 40.9068298\n",
      "\n",
      " epoch 1880 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 1ms/step\n",
      "(for 1 minibatch) Training loss 0.0102433 | PSNR training 40.6187744\n",
      "\n",
      " epoch 1881 of 2000\n",
      "5/5 [==============================] - 0s 961us/step\n",
      "11/11 [==============================] - 0s 865us/step\n",
      "(for 1 minibatch) Training loss 0.0100639 | PSNR training 41.0178604\n",
      "\n",
      " epoch 1882 of 2000\n",
      "5/5 [==============================] - 0s 976us/step\n",
      "11/11 [==============================] - 0s 892us/step\n",
      "(for 1 minibatch) Training loss 0.0103410 | PSNR training 40.9678268\n",
      "\n",
      " epoch 1883 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 888us/step\n",
      "(for 1 minibatch) Training loss 0.0101020 | PSNR training 40.9336014\n",
      "\n",
      " epoch 1884 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 949us/step\n",
      "(for 1 minibatch) Training loss 0.0102006 | PSNR training 41.0249290\n",
      "\n",
      " epoch 1885 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 972us/step\n",
      "(for 1 minibatch) Training loss 0.0101345 | PSNR training 40.9388924\n",
      "\n",
      " epoch 1886 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 857us/step\n",
      "(for 1 minibatch) Training loss 0.0100703 | PSNR training 41.1198997\n",
      "\n",
      " epoch 1887 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 960us/step\n",
      "(for 1 minibatch) Training loss 0.0101433 | PSNR training 41.0405350\n",
      "\n",
      " epoch 1888 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 916us/step\n",
      "(for 1 minibatch) Training loss 0.0102150 | PSNR training 41.0426712\n",
      "\n",
      " epoch 1889 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 997us/step\n",
      "(for 1 minibatch) Training loss 0.0100160 | PSNR training 41.1854744\n",
      "\n",
      " epoch 1890 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 865us/step\n",
      "(for 1 minibatch) Training loss 0.0100380 | PSNR training 41.1417694\n",
      "\n",
      " epoch 1891 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 845us/step\n",
      "(for 1 minibatch) Training loss 0.0098754 | PSNR training 41.3150826\n",
      "\n",
      " epoch 1892 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 829us/step\n",
      "(for 1 minibatch) Training loss 0.0100036 | PSNR training 40.8942833\n",
      "\n",
      " epoch 1893 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 887us/step\n",
      "(for 1 minibatch) Training loss 0.0100652 | PSNR training 41.0634804\n",
      "\n",
      " epoch 1894 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 889us/step\n",
      "(for 1 minibatch) Training loss 0.0100779 | PSNR training 41.1833153\n",
      "\n",
      " epoch 1895 of 2000\n",
      "5/5 [==============================] - 0s 914us/step\n",
      "11/11 [==============================] - 0s 922us/step\n",
      "(for 1 minibatch) Training loss 0.0100608 | PSNR training 41.0320282\n",
      "\n",
      " epoch 1896 of 2000\n",
      "5/5 [==============================] - 0s 947us/step\n",
      "11/11 [==============================] - 0s 861us/step\n",
      "(for 1 minibatch) Training loss 0.0098521 | PSNR training 41.2895012\n",
      "\n",
      " epoch 1897 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 905us/step\n",
      "(for 1 minibatch) Training loss 0.0105299 | PSNR training 40.8775902\n",
      "\n",
      " epoch 1898 of 2000\n",
      "5/5 [==============================] - 0s 950us/step\n",
      "11/11 [==============================] - 0s 884us/step\n",
      "(for 1 minibatch) Training loss 0.0100662 | PSNR training 41.0843430\n",
      "\n",
      " epoch 1899 of 2000\n",
      "5/5 [==============================] - 0s 965us/step\n",
      "11/11 [==============================] - 0s 972us/step\n",
      "(for 1 minibatch) Training loss 0.0100736 | PSNR training 41.1083145\n",
      "\n",
      " epoch 1900 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 871us/step\n",
      "(for 1 minibatch) Training loss 0.0099231 | PSNR training 41.1272202\n",
      "\n",
      " epoch 1901 of 2000\n",
      "5/5 [==============================] - 0s 925us/step\n",
      "11/11 [==============================] - 0s 843us/step\n",
      "(for 1 minibatch) Training loss 0.0101908 | PSNR training 40.8231201\n",
      "\n",
      " epoch 1902 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 837us/step\n",
      "(for 1 minibatch) Training loss 0.0098690 | PSNR training 41.1170578\n",
      "\n",
      " epoch 1903 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 903us/step\n",
      "(for 1 minibatch) Training loss 0.0101416 | PSNR training 41.1538734\n",
      "\n",
      " epoch 1904 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 918us/step\n",
      "(for 1 minibatch) Training loss 0.0100624 | PSNR training 41.1166000\n",
      "\n",
      " epoch 1905 of 2000\n",
      "5/5 [==============================] - 0s 988us/step\n",
      "11/11 [==============================] - 0s 937us/step\n",
      "(for 1 minibatch) Training loss 0.0100940 | PSNR training 41.0239716\n",
      "\n",
      " epoch 1906 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 898us/step\n",
      "(for 1 minibatch) Training loss 0.0099102 | PSNR training 41.0865440\n",
      "\n",
      " epoch 1907 of 2000\n",
      "5/5 [==============================] - 0s 948us/step\n",
      "11/11 [==============================] - 0s 880us/step\n",
      "(for 1 minibatch) Training loss 0.0098910 | PSNR training 41.0123291\n",
      "\n",
      " epoch 1908 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 834us/step\n",
      "(for 1 minibatch) Training loss 0.0101162 | PSNR training 41.1183357\n",
      "\n",
      " epoch 1909 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 883us/step\n",
      "(for 1 minibatch) Training loss 0.0099465 | PSNR training 41.3223648\n",
      "\n",
      " epoch 1910 of 2000\n",
      "5/5 [==============================] - 0s 908us/step\n",
      "11/11 [==============================] - 0s 837us/step\n",
      "(for 1 minibatch) Training loss 0.0101081 | PSNR training 41.1949081\n",
      "\n",
      " epoch 1911 of 2000\n",
      "5/5 [==============================] - 0s 929us/step\n",
      "11/11 [==============================] - 0s 843us/step\n",
      "(for 1 minibatch) Training loss 0.0100791 | PSNR training 41.0574417\n",
      "\n",
      " epoch 1912 of 2000\n",
      "5/5 [==============================] - 0s 967us/step\n",
      "11/11 [==============================] - 0s 907us/step\n",
      "(for 1 minibatch) Training loss 0.0101166 | PSNR training 40.9617424\n",
      "\n",
      " epoch 1913 of 2000\n",
      "5/5 [==============================] - 0s 953us/step\n",
      "11/11 [==============================] - 0s 914us/step\n",
      "(for 1 minibatch) Training loss 0.0105179 | PSNR training 40.8069687\n",
      "\n",
      " epoch 1914 of 2000\n",
      "5/5 [==============================] - 0s 996us/step\n",
      "11/11 [==============================] - 0s 944us/step\n",
      "(for 1 minibatch) Training loss 0.0102006 | PSNR training 40.9935417\n",
      "\n",
      " epoch 1915 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 1ms/step\n",
      "(for 1 minibatch) Training loss 0.0098239 | PSNR training 41.2423019\n",
      "\n",
      " epoch 1916 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 957us/step\n",
      "(for 1 minibatch) Training loss 0.0103415 | PSNR training 40.8545914\n",
      "\n",
      " epoch 1917 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 932us/step\n",
      "(for 1 minibatch) Training loss 0.0099903 | PSNR training 41.1289787\n",
      "\n",
      " epoch 1918 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 904us/step\n",
      "(for 1 minibatch) Training loss 0.0100127 | PSNR training 40.9561958\n",
      "\n",
      " epoch 1919 of 2000\n",
      "5/5 [==============================] - 0s 979us/step\n",
      "11/11 [==============================] - 0s 864us/step\n",
      "(for 1 minibatch) Training loss 0.0098805 | PSNR training 41.0222778\n",
      "\n",
      " epoch 1920 of 2000\n",
      "5/5 [==============================] - 0s 948us/step\n",
      "11/11 [==============================] - 0s 877us/step\n",
      "(for 1 minibatch) Training loss 0.0098224 | PSNR training 41.2750626\n",
      "\n",
      " epoch 1921 of 2000\n",
      "5/5 [==============================] - 0s 956us/step\n",
      "11/11 [==============================] - 0s 878us/step\n",
      "(for 1 minibatch) Training loss 0.0101732 | PSNR training 41.1916008\n",
      "\n",
      " epoch 1922 of 2000\n",
      "5/5 [==============================] - 0s 997us/step\n",
      "11/11 [==============================] - 0s 839us/step\n",
      "(for 1 minibatch) Training loss 0.0103865 | PSNR training 40.8198013\n",
      "\n",
      " epoch 1923 of 2000\n",
      "5/5 [==============================] - 0s 961us/step\n",
      "11/11 [==============================] - 0s 833us/step\n",
      "(for 1 minibatch) Training loss 0.0103622 | PSNR training 40.9792786\n",
      "\n",
      " epoch 1924 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 870us/step\n",
      "(for 1 minibatch) Training loss 0.0098414 | PSNR training 41.3309250\n",
      "\n",
      " epoch 1925 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 841us/step\n",
      "(for 1 minibatch) Training loss 0.0099353 | PSNR training 40.9878769\n",
      "\n",
      " epoch 1926 of 2000\n",
      "5/5 [==============================] - 0s 986us/step\n",
      "11/11 [==============================] - 0s 846us/step\n",
      "(for 1 minibatch) Training loss 0.0097495 | PSNR training 41.2848053\n",
      "\n",
      " epoch 1927 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 970us/step\n",
      "(for 1 minibatch) Training loss 0.0101936 | PSNR training 41.1157532\n",
      "\n",
      " epoch 1928 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 833us/step\n",
      "(for 1 minibatch) Training loss 0.0102631 | PSNR training 40.9584007\n",
      "\n",
      " epoch 1929 of 2000\n",
      "5/5 [==============================] - 0s 938us/step\n",
      "11/11 [==============================] - 0s 877us/step\n",
      "(for 1 minibatch) Training loss 0.0101395 | PSNR training 40.9389114\n",
      "\n",
      " epoch 1930 of 2000\n",
      "5/5 [==============================] - 0s 955us/step\n",
      "11/11 [==============================] - 0s 991us/step\n",
      "(for 1 minibatch) Training loss 0.0100988 | PSNR training 40.8784904\n",
      "\n",
      " epoch 1931 of 2000\n",
      "5/5 [==============================] - 0s 955us/step\n",
      "11/11 [==============================] - 0s 835us/step\n",
      "(for 1 minibatch) Training loss 0.0103848 | PSNR training 40.8363495\n",
      "\n",
      " epoch 1932 of 2000\n",
      "5/5 [==============================] - 0s 969us/step\n",
      "11/11 [==============================] - 0s 948us/step\n",
      "(for 1 minibatch) Training loss 0.0098020 | PSNR training 41.1549034\n",
      "\n",
      " epoch 1933 of 2000\n",
      "5/5 [==============================] - 0s 927us/step\n",
      "11/11 [==============================] - 0s 858us/step\n",
      "(for 1 minibatch) Training loss 0.0101029 | PSNR training 41.1977615\n",
      "\n",
      " epoch 1934 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 836us/step\n",
      "(for 1 minibatch) Training loss 0.0101938 | PSNR training 40.9964371\n",
      "\n",
      " epoch 1935 of 2000\n",
      "5/5 [==============================] - 0s 976us/step\n",
      "11/11 [==============================] - 0s 889us/step\n",
      "(for 1 minibatch) Training loss 0.0100837 | PSNR training 41.1639404\n",
      "\n",
      " epoch 1936 of 2000\n",
      "5/5 [==============================] - 0s 946us/step\n",
      "11/11 [==============================] - 0s 827us/step\n",
      "(for 1 minibatch) Training loss 0.0101182 | PSNR training 41.1006775\n",
      "\n",
      " epoch 1937 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 892us/step\n",
      "(for 1 minibatch) Training loss 0.0099990 | PSNR training 40.9704628\n",
      "\n",
      " epoch 1938 of 2000\n",
      "5/5 [==============================] - 0s 947us/step\n",
      "11/11 [==============================] - 0s 851us/step\n",
      "(for 1 minibatch) Training loss 0.0100266 | PSNR training 40.9165039\n",
      "\n",
      " epoch 1939 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 844us/step\n",
      "(for 1 minibatch) Training loss 0.0099915 | PSNR training 41.2893829\n",
      "\n",
      " epoch 1940 of 2000\n",
      "5/5 [==============================] - 0s 917us/step\n",
      "11/11 [==============================] - 0s 885us/step\n",
      "(for 1 minibatch) Training loss 0.0099310 | PSNR training 41.0760574\n",
      "\n",
      " epoch 1941 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 839us/step\n",
      "(for 1 minibatch) Training loss 0.0101123 | PSNR training 41.0888062\n",
      "\n",
      " epoch 1942 of 2000\n",
      "5/5 [==============================] - 0s 987us/step\n",
      "11/11 [==============================] - 0s 837us/step\n",
      "(for 1 minibatch) Training loss 0.0099970 | PSNR training 41.0517349\n",
      "\n",
      " epoch 1943 of 2000\n",
      "5/5 [==============================] - 0s 935us/step\n",
      "11/11 [==============================] - 0s 930us/step\n",
      "(for 1 minibatch) Training loss 0.0100613 | PSNR training 41.1059914\n",
      "\n",
      " epoch 1944 of 2000\n",
      "5/5 [==============================] - 0s 915us/step\n",
      "11/11 [==============================] - 0s 859us/step\n",
      "(for 1 minibatch) Training loss 0.0100759 | PSNR training 41.0725365\n",
      "\n",
      " epoch 1945 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 870us/step\n",
      "(for 1 minibatch) Training loss 0.0098934 | PSNR training 40.9348602\n",
      "\n",
      " epoch 1946 of 2000\n",
      "5/5 [==============================] - 0s 915us/step\n",
      "11/11 [==============================] - 0s 925us/step\n",
      "(for 1 minibatch) Training loss 0.0100207 | PSNR training 41.0420914\n",
      "\n",
      " epoch 1947 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 834us/step\n",
      "(for 1 minibatch) Training loss 0.0100063 | PSNR training 41.0472794\n",
      "\n",
      " epoch 1948 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 826us/step\n",
      "(for 1 minibatch) Training loss 0.0099370 | PSNR training 40.9896507\n",
      "\n",
      " epoch 1949 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 876us/step\n",
      "(for 1 minibatch) Training loss 0.0100543 | PSNR training 41.0137138\n",
      "\n",
      " epoch 1950 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 861us/step\n",
      "(for 1 minibatch) Training loss 0.0098214 | PSNR training 41.1928711\n",
      "\n",
      " epoch 1951 of 2000\n",
      "5/5 [==============================] - 0s 985us/step\n",
      "11/11 [==============================] - 0s 883us/step\n",
      "(for 1 minibatch) Training loss 0.0099517 | PSNR training 41.0893326\n",
      "\n",
      " epoch 1952 of 2000\n",
      "5/5 [==============================] - 0s 948us/step\n",
      "11/11 [==============================] - 0s 908us/step\n",
      "(for 1 minibatch) Training loss 0.0101988 | PSNR training 41.0341072\n",
      "\n",
      " epoch 1953 of 2000\n",
      "5/5 [==============================] - 0s 967us/step\n",
      "11/11 [==============================] - 0s 884us/step\n",
      "(for 1 minibatch) Training loss 0.0100212 | PSNR training 41.2315331\n",
      "\n",
      " epoch 1954 of 2000\n",
      "5/5 [==============================] - 0s 985us/step\n",
      "11/11 [==============================] - 0s 893us/step\n",
      "(for 1 minibatch) Training loss 0.0102707 | PSNR training 40.9529457\n",
      "\n",
      " epoch 1955 of 2000\n",
      "5/5 [==============================] - 0s 943us/step\n",
      "11/11 [==============================] - 0s 891us/step\n",
      "(for 1 minibatch) Training loss 0.0099930 | PSNR training 41.0497971\n",
      "\n",
      " epoch 1956 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 921us/step\n",
      "(for 1 minibatch) Training loss 0.0100724 | PSNR training 41.1752319\n",
      "\n",
      " epoch 1957 of 2000\n",
      "5/5 [==============================] - 0s 929us/step\n",
      "11/11 [==============================] - 0s 837us/step\n",
      "(for 1 minibatch) Training loss 0.0100123 | PSNR training 41.0633087\n",
      "\n",
      " epoch 1958 of 2000\n",
      "5/5 [==============================] - 0s 941us/step\n",
      "11/11 [==============================] - 0s 836us/step\n",
      "(for 1 minibatch) Training loss 0.0099039 | PSNR training 41.1919441\n",
      "\n",
      " epoch 1959 of 2000\n",
      "5/5 [==============================] - 0s 948us/step\n",
      "11/11 [==============================] - 0s 873us/step\n",
      "(for 1 minibatch) Training loss 0.0099964 | PSNR training 41.0500298\n",
      "\n",
      " epoch 1960 of 2000\n",
      "5/5 [==============================] - 0s 976us/step\n",
      "11/11 [==============================] - 0s 828us/step\n",
      "(for 1 minibatch) Training loss 0.0101430 | PSNR training 41.1393585\n",
      "\n",
      " epoch 1961 of 2000\n",
      "5/5 [==============================] - 0s 923us/step\n",
      "11/11 [==============================] - 0s 828us/step\n",
      "(for 1 minibatch) Training loss 0.0103612 | PSNR training 40.8530655\n",
      "\n",
      " epoch 1962 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 897us/step\n",
      "(for 1 minibatch) Training loss 0.0102974 | PSNR training 40.8139381\n",
      "\n",
      " epoch 1963 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 837us/step\n",
      "(for 1 minibatch) Training loss 0.0101268 | PSNR training 41.2521133\n",
      "\n",
      " epoch 1964 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 848us/step\n",
      "(for 1 minibatch) Training loss 0.0099173 | PSNR training 41.2266464\n",
      "\n",
      " epoch 1965 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 882us/step\n",
      "(for 1 minibatch) Training loss 0.0104137 | PSNR training 40.7875671\n",
      "\n",
      " epoch 1966 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 811us/step\n",
      "(for 1 minibatch) Training loss 0.0100245 | PSNR training 41.1534309\n",
      "\n",
      " epoch 1967 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 888us/step\n",
      "(for 1 minibatch) Training loss 0.0100235 | PSNR training 41.0071754\n",
      "\n",
      " epoch 1968 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 893us/step\n",
      "(for 1 minibatch) Training loss 0.0101990 | PSNR training 40.9798622\n",
      "\n",
      " epoch 1969 of 2000\n",
      "5/5 [==============================] - 0s 979us/step\n",
      "11/11 [==============================] - 0s 938us/step\n",
      "(for 1 minibatch) Training loss 0.0100217 | PSNR training 41.0892067\n",
      "\n",
      " epoch 1970 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 834us/step\n",
      "(for 1 minibatch) Training loss 0.0099082 | PSNR training 41.3431854\n",
      "\n",
      " epoch 1971 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 888us/step\n",
      "(for 1 minibatch) Training loss 0.0101608 | PSNR training 41.0069962\n",
      "\n",
      " epoch 1972 of 2000\n",
      "5/5 [==============================] - 0s 951us/step\n",
      "11/11 [==============================] - 0s 882us/step\n",
      "(for 1 minibatch) Training loss 0.0099283 | PSNR training 41.0779419\n",
      "\n",
      " epoch 1973 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 852us/step\n",
      "(for 1 minibatch) Training loss 0.0100964 | PSNR training 41.0388908\n",
      "\n",
      " epoch 1974 of 2000\n",
      "5/5 [==============================] - 0s 982us/step\n",
      "11/11 [==============================] - 0s 909us/step\n",
      "(for 1 minibatch) Training loss 0.0097288 | PSNR training 41.3144455\n",
      "\n",
      " epoch 1975 of 2000\n",
      "5/5 [==============================] - 0s 980us/step\n",
      "11/11 [==============================] - 0s 937us/step\n",
      "(for 1 minibatch) Training loss 0.0100769 | PSNR training 40.9367714\n",
      "\n",
      " epoch 1976 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 859us/step\n",
      "(for 1 minibatch) Training loss 0.0095947 | PSNR training 41.1187325\n",
      "\n",
      " epoch 1977 of 2000\n",
      "5/5 [==============================] - 0s 950us/step\n",
      "11/11 [==============================] - 0s 829us/step\n",
      "(for 1 minibatch) Training loss 0.0103371 | PSNR training 40.7768555\n",
      "\n",
      " epoch 1978 of 2000\n",
      "5/5 [==============================] - 0s 916us/step\n",
      "11/11 [==============================] - 0s 833us/step\n",
      "(for 1 minibatch) Training loss 0.0099121 | PSNR training 41.0322952\n",
      "\n",
      " epoch 1979 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 885us/step\n",
      "(for 1 minibatch) Training loss 0.0103654 | PSNR training 41.0224152\n",
      "\n",
      " epoch 1980 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 881us/step\n",
      "(for 1 minibatch) Training loss 0.0099550 | PSNR training 41.3267822\n",
      "\n",
      " epoch 1981 of 2000\n",
      "5/5 [==============================] - 0s 999us/step\n",
      "11/11 [==============================] - 0s 1ms/step\n",
      "(for 1 minibatch) Training loss 0.0100247 | PSNR training 41.0287819\n",
      "\n",
      " epoch 1982 of 2000\n",
      "5/5 [==============================] - 0s 985us/step\n",
      "11/11 [==============================] - 0s 919us/step\n",
      "(for 1 minibatch) Training loss 0.0100728 | PSNR training 41.0029411\n",
      "\n",
      " epoch 1983 of 2000\n",
      "5/5 [==============================] - 0s 965us/step\n",
      "11/11 [==============================] - 0s 848us/step\n",
      "(for 1 minibatch) Training loss 0.0101982 | PSNR training 40.9927216\n",
      "\n",
      " epoch 1984 of 2000\n",
      "5/5 [==============================] - 0s 993us/step\n",
      "11/11 [==============================] - 0s 832us/step\n",
      "(for 1 minibatch) Training loss 0.0099677 | PSNR training 40.9643860\n",
      "\n",
      " epoch 1985 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 876us/step\n",
      "(for 1 minibatch) Training loss 0.0101778 | PSNR training 41.1111794\n",
      "\n",
      " epoch 1986 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 820us/step\n",
      "(for 1 minibatch) Training loss 0.0099097 | PSNR training 41.2436638\n",
      "\n",
      " epoch 1987 of 2000\n",
      "5/5 [==============================] - 0s 976us/step\n",
      "11/11 [==============================] - 0s 886us/step\n",
      "(for 1 minibatch) Training loss 0.0102654 | PSNR training 41.1748695\n",
      "\n",
      " epoch 1988 of 2000\n",
      "5/5 [==============================] - 0s 961us/step\n",
      "11/11 [==============================] - 0s 879us/step\n",
      "(for 1 minibatch) Training loss 0.0100819 | PSNR training 41.0887566\n",
      "\n",
      " epoch 1989 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 850us/step\n",
      "(for 1 minibatch) Training loss 0.0103780 | PSNR training 40.9845924\n",
      "\n",
      " epoch 1990 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 833us/step\n",
      "(for 1 minibatch) Training loss 0.0099932 | PSNR training 41.2058716\n",
      "\n",
      " epoch 1991 of 2000\n",
      "5/5 [==============================] - 0s 983us/step\n",
      "11/11 [==============================] - 0s 889us/step\n",
      "(for 1 minibatch) Training loss 0.0101734 | PSNR training 40.7929306\n",
      "\n",
      " epoch 1992 of 2000\n",
      "5/5 [==============================] - 0s 924us/step\n",
      "11/11 [==============================] - 0s 849us/step\n",
      "(for 1 minibatch) Training loss 0.0100886 | PSNR training 41.0754852\n",
      "\n",
      " epoch 1993 of 2000\n",
      "5/5 [==============================] - 0s 928us/step\n",
      "11/11 [==============================] - 0s 887us/step\n",
      "(for 1 minibatch) Training loss 0.0097191 | PSNR training 41.1627007\n",
      "\n",
      " epoch 1994 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 907us/step\n",
      "(for 1 minibatch) Training loss 0.0101182 | PSNR training 41.0783920\n",
      "\n",
      " epoch 1995 of 2000\n",
      "5/5 [==============================] - 0s 953us/step\n",
      "11/11 [==============================] - 0s 864us/step\n",
      "(for 1 minibatch) Training loss 0.0101123 | PSNR training 40.8464813\n",
      "\n",
      " epoch 1996 of 2000\n",
      "5/5 [==============================] - 0s 971us/step\n",
      "11/11 [==============================] - 0s 859us/step\n",
      "(for 1 minibatch) Training loss 0.0100448 | PSNR training 41.1313667\n",
      "\n",
      " epoch 1997 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 845us/step\n",
      "(for 1 minibatch) Training loss 0.0103517 | PSNR training 40.9133530\n",
      "\n",
      " epoch 1998 of 2000\n",
      "5/5 [==============================] - 0s 972us/step\n",
      "11/11 [==============================] - 0s 847us/step\n",
      "(for 1 minibatch) Training loss 0.0102649 | PSNR training 40.9885979\n",
      "\n",
      " epoch 1999 of 2000\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 816us/step\n",
      "(for 1 minibatch) Training loss 0.0098799 | PSNR training 41.1509476\n",
      "\n",
      " epoch 2000 of 2000\n",
      "5/5 [==============================] - 0s 982us/step\n",
      "11/11 [==============================] - 0s 878us/step\n",
      "(for 1 minibatch) Training loss 0.0102056 | PSNR training 41.0075684\n"
     ]
    }
   ],
   "source": [
    "initial_learning_rate = 1e-3\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate,\n",
    "    decay_steps=450,\n",
    "    decay_rate=0.6,\n",
    "    staircase=True)\n",
    "\n",
    "optimizer = keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "loss_fn = tf.keras.losses.MeanAbsoluteError()\n",
    "\n",
    "def loss_s(x,y):\n",
    "    return loss_fn(y, x) + 1 - tf.reduce_mean(tf.image.ssim(y, x, 1.0)) \n",
    "\n",
    "epochs = 2000\n",
    "loss_train = []\n",
    "psnr_train = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(\"\\n epoch %d of %d\" % (epoch+1,epochs))\n",
    "    sigma = 2#np.random.choice(list(np.arange(1, 2)),1)[0]\n",
    "    #note that is not strictly necesary to use different sigmas always\n",
    "    #noise = np.random.normal(0, sigma, size=(100, 128))\n",
    "    generated_image = generator.predict(np.random.normal(0, sigma, size=(500, 128)))  \n",
    "    \n",
    "    \n",
    "    \n",
    "    imgsGAN = (generated_image - np.min(generated_image))/(np.max(generated_image) - np.min(generated_image))\n",
    "    \n",
    "    \n",
    "    \n",
    "    inn = subsampling(imgsGAN,cr=[0.17,0.25,0.35])\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "      y_batch_pred = modelcombinado(inn[0])\n",
    "      #loss_value =  loss_fn(y_batch_pred, inn[1]) + 1 - tf.reduce_mean(tf.image.ssim(y_batch_pred, inn[1], 1.0)) \n",
    "      loss_value =  loss_s(inn[1],y_batch_pred)\n",
    "      loss_train.append(loss_value)\n",
    "      psnrt = PSNR(inn[1], y_batch_pred)\n",
    "      psnr_train.append(psnrt)\n",
    "\n",
    "    grads = tape.gradient(loss_value, modelcombinado.trainable_weights)\n",
    "    optimizer.apply_gradients(zip(grads, modelcombinado.trainable_weights))\n",
    "\n",
    "    print(\n",
    "        \"(for 1 minibatch) Training loss %.7f | PSNR training %.7f\"\n",
    "        % (float(loss_value),  float(psnrt) )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "id": "45feLClLhZ4w"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'PSNR (dB)')"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 800x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "modelcombinado.save('models/doubleGAN_model.h5')\n",
    "unet.save('models/unet_doubleGAN.h5')\n",
    "resnet.save('models/resnet_doubleGAN.h5')\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.subplot(121)\n",
    "plt.semilogy(loss_train[1:])\n",
    "plt.ylabel('Loss')\n",
    "plt.subplot(122)\n",
    "plt.plot(psnr_train[1:],'r')\n",
    "plt.ylabel('PSNR (dB)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "id": "bsRq4GcOZgPr"
   },
   "outputs": [],
   "source": [
    "from numpy import save\n",
    "save('metrics_train/LOSSdoubleGANrecon2000.npy', loss_train)\n",
    "save('metrics_train/PSNRdoubleGANrecon2000.npy', psnr_train)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuClass": "premium",
   "gpuType": "T4",
   "provenance": []
  },
  "gpuClass": "premium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
